{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimid :  Inferring Grammars\n",
    "\n",
    "* Code for subjects [here](#Our-subject-programs)\n",
    "* Evaluation starts [here](#Evaluation)\n",
    "  * The evaluation on specific subjects starts [here](#Subjects)\n",
    "    * [CGIDecode](#CGIDecode)\n",
    "    * [Calculator](#Calculator)\n",
    "    * [MathExpr](#MathExpr)\n",
    "    * [URLParse](#URLParse)\n",
    "    * [Microjson](#Microjson)\n",
    "* Results are [here](#Results)\n",
    "* Recovering parse tree from a recognizer is [here](#Using-a-Recognizer-(not-a-Parser))\n",
    "* Recovering parse tree from parser combinators is [here](#Parsing-with-Parser-Combinators)\n",
    "* Recovering parse tree from PEG parer is [here](#Parsing-with-PEG-Parser)\n",
    "\n",
    "Please note that a complete run can take an hour and a half to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a few Jupyter magics that let us specify examples inline, that can be turned off if needed for faster execution. Switch `TOP to False` if you do not want examples to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.196736Z",
     "start_time": "2019-11-10T17:56:43.190391Z"
    }
   },
   "outputs": [],
   "source": [
    "TOP = __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magics we use are `%%var` and `%top`. The `%%var` lets us specify large strings such as file contents directly without too many escapes. The `%top` helps with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.338218Z",
     "start_time": "2019-11-10T17:56:43.308112Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import  (Magics, magics_class, cell_magic, line_magic, line_cell_magic)\n",
    "class B(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self.__getitem__(name)\n",
    "@magics_class\n",
    "class MyMagics(Magics):\n",
    "    def __init__(self, shell=None,  **kwargs):\n",
    "        super().__init__(shell=shell, **kwargs)\n",
    "        self._vars = B()\n",
    "        shell.user_ns['VARS'] = self._vars\n",
    "\n",
    "    @cell_magic\n",
    "    def var(self, line, cell):\n",
    "        self._vars[line.strip()] = cell.strip()\n",
    " \n",
    "    @line_cell_magic\n",
    "    def top(self, line, cell=None):\n",
    "        if TOP:\n",
    "            if cell is None:\n",
    "                cell = line\n",
    "            ip = get_ipython()\n",
    "            res = ip.run_cell(cell)\n",
    "\n",
    "get_ipython().register_magics(MyMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify System Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.353505Z",
     "start_time": "2019-11-10T17:56:43.348363Z"
    },
    "tags": [
     "#import_sys"
    ]
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of the program, especially the subprocess execution using `do()` requires the new flags in `3.7`. I am not sure if the taints will work on anything above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.366649Z",
     "start_time": "2019-11-10T17:56:43.359988Z"
    },
    "tags": [
     "#verifyversion",
     "=>install_fuzzingbook",
     "=>install_dependencies",
     "=>install_dill",
     "=>import_sys"
    ]
   },
   "outputs": [],
   "source": [
    "%top assert sys.version_info[0:2] == (3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.372687Z",
     "start_time": "2019-11-10T17:56:43.369688Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.378359Z",
     "start_time": "2019-11-10T17:56:43.375110Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep a log of all system commands executed for easier debugging at `./build/do.log` when debug is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.385464Z",
     "start_time": "2019-11-10T17:56:43.380925Z"
    }
   },
   "outputs": [],
   "source": [
    "class O:\n",
    "    def __init__(self, **keys): self.__dict__.update(keys)\n",
    "    def __repr__(self): return str(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMD_TIMEOUT=60*60*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do(command, env=None, shell=False, log=False, inputv=None, timeout=CMD_TIMEOUT, **args):\n",
    "    result = None\n",
    "    if inputv:\n",
    "        result = subprocess.Popen(command,\n",
    "            stdin = subprocess.PIPE,\n",
    "            stdout = subprocess.PIPE,\n",
    "            stderr = subprocess.STDOUT,\n",
    "            shell = shell,\n",
    "            env=dict(os.environ, **({} if env is None else env))\n",
    "        )\n",
    "        result.stdin.write(inputv)\n",
    "        stdout, stderr = result.communicate(timeout=timeout)\n",
    "    else:\n",
    "        result = subprocess.Popen(command,\n",
    "            stdout = subprocess.PIPE,\n",
    "            stderr = subprocess.STDOUT,\n",
    "            shell = shell,\n",
    "            env=dict(os.environ, **({} if env is None else env))\n",
    "        )\n",
    "        stdout, stderr = result.communicate(timeout=timeout)\n",
    "    if log:\n",
    "         with open('build/do.log', 'a+') as f:\n",
    "            print(json.dumps({'cmd':command,\n",
    "                              'env':env,\n",
    "                              'exitcode':result.returncode}), env,\n",
    "                  flush=True, file=f)\n",
    "    stdout = '' if stdout is None else stdout.decode()\n",
    "    stderr = '' if stderr is None else stderr.decode()\n",
    "    result.kill()\n",
    "    return O(returncode=result.returncode, stdout=stdout, stderr=stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.428776Z",
     "start_time": "2019-11-10T17:56:43.423056Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to ensure replicability of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.440514Z",
     "start_time": "2019-11-10T17:56:43.434634Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook was tested on `Ubuntu 18.04.4 LTS`. In particular, I do not know if everything will work on `Windows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.450107Z",
     "start_time": "2019-11-10T17:56:43.444212Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:43.544584Z",
     "start_time": "2019-11-10T17:56:43.453225Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "if shutil.which('lsb_release'):\n",
    "    res = do(['lsb_release', '-d']).stdout\n",
    "elif shutil.which('sw_vers'):\n",
    "    res = do(['sw_vers']).stdout\n",
    "else:\n",
    "    assert False\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:44.509651Z",
     "start_time": "2019-11-10T17:56:43.547780Z"
    }
   },
   "outputs": [],
   "source": [
    "%top print(do(['jupyter', '--version']).stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid reinstalling things on each run. So, we define a variable `INSTALL` that should be made true for installations to take place. Use it only during the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTALL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install(fn):\n",
    "    if INSTALL:\n",
    "        return fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code is based on the utilities provided by the [Fuzzingbook](http://fuzzingbook.org). Note that the measurements on time and precision in paper were based on Fuzzingbook `0.0.7`. During the development, we found a few bugs in Autogram, which we communicated back, which resulted in a new version of Fuzzingbook `0.8.0`.\n",
    "\n",
    "The fixed *Autogram* implementation of the *Fuzzingbook* has better precision rates for *Autogram*, and timing for grammar generation. However, these numbers still fall short of *Mimid* for most grammars. Further, the grammars generated by *Autogram* are still enumerative. That is, rather than producing a context free grammar, it simply appends input strings as alternates to the `<START>` nonterminal. This again results in bad recall numbers as before. Hence, it does not change our main points. During the remainder of this notebook, we use the `0.8.0` version of the Fuzzingbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define `pip_install()`, a helper to silently install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:44.519751Z",
     "start_time": "2019-11-10T17:56:44.513118Z"
    }
   },
   "outputs": [],
   "source": [
    "def pip_install(v):\n",
    "    return do(['pip', 'install', '-qqq', *v.split(' ')]).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:45.402809Z",
     "start_time": "2019-11-10T17:56:44.522930Z"
    },
    "tags": [
     "#install_fuzzingbook"
    ]
   },
   "outputs": [],
   "source": [
    "%top install(lambda: pip_install('fuzzingbook==0.8.1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to update the Parser to the current version (not published in pypy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!file Parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook.Parser as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp Parser.py {P.__file__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook.GrammarMiner as G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp GrammarMiner.py {G.__file__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work before we proceed. If not, restart the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = P.IterativeEarleyParser({'<START>': [['start']]}, start_symbol='<START>', canonical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our external dependencies other than `fuzzingbook` are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:46.162958Z",
     "start_time": "2019-11-10T17:56:45.406415Z"
    },
    "tags": [
     "#install_dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "%top install(lambda: pip_install('astor graphviz scipy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Restart the jupyter server after installation of dependencies and extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend the following jupyter notebook extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:47.031087Z",
     "start_time": "2019-11-10T17:56:46.166282Z"
    }
   },
   "outputs": [],
   "source": [
    "%top install(lambda: pip_install('jupyter_contrib_nbextensions jupyter_nbextensions_configurator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:48.674155Z",
     "start_time": "2019-11-10T17:56:47.034337Z"
    }
   },
   "outputs": [],
   "source": [
    "%top install(lambda: do(['jupyter','contrib','nbextension','install', '--sys-prefix']).returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:48.686123Z",
     "start_time": "2019-11-10T17:56:48.681777Z"
    }
   },
   "outputs": [],
   "source": [
    "def nb_enable(v): return do(['jupyter','nbextension','enable',v]).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:49.148987Z",
     "start_time": "2019-11-10T17:56:48.691076Z"
    }
   },
   "outputs": [],
   "source": [
    "%top install(lambda: do(['jupyter','nbextensions_configurator','enable']).returncode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents\n",
    "\n",
    "Please install this extension. The navigation in the notebook is rather hard without this installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:49.419567Z",
     "start_time": "2019-11-10T17:56:49.152161Z"
    }
   },
   "outputs": [],
   "source": [
    "%top install(lambda: nb_enable('toc2/main'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collapsible headings\n",
    "\n",
    "Again, do install this extension. This will let you fold away those sections that you do not have an immediate interest in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:49.694288Z",
     "start_time": "2019-11-10T17:56:49.423125Z"
    }
   },
   "outputs": [],
   "source": [
    "%top install(lambda: nb_enable('collapsible_headings/main'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code folding\n",
    "Very helpful for hiding away source contents of libraries that are not for grammar recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.282941Z",
     "start_time": "2019-11-10T17:56:49.974436Z"
    }
   },
   "outputs": [],
   "source": [
    "%top install(lambda: nb_enable('codefolding/main'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make runs faster, we cache quite a lot of things. Remove `build` if you change code or samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.324144Z",
     "start_time": "2019-11-10T17:56:50.286107Z"
    }
   },
   "outputs": [],
   "source": [
    "%top do(['rm', '-rf','build']).returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic for cell contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before `%%var` defines a multi line embedded string that is accessible from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.337297Z",
     "start_time": "2019-11-10T17:56:50.327664Z"
    }
   },
   "outputs": [],
   "source": [
    "%%var Mimid\n",
    "# [(\n",
    "Testing Mimid\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.348388Z",
     "start_time": "2019-11-10T17:56:50.339703Z"
    }
   },
   "outputs": [],
   "source": [
    "%top VARS['Mimid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the fuzer to generate inputs when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.886000Z",
     "start_time": "2019-11-10T17:57:46.882892Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "ASCII_MAP = {\n",
    "        '[__WHITESPACE__]': string.whitespace,\n",
    "        '[__DIGIT__]': string.digits,\n",
    "        '[__ASCII_LOWER__]': string.ascii_lowercase,\n",
    "        '[__ASCII_UPPER__]': string.ascii_uppercase,\n",
    "        '[__ASCII_PUNCT__]': string.punctuation,\n",
    "        '[__ASCII_LETTER__]': string.ascii_letters,\n",
    "        '[__ASCII_ALPHANUM__]': string.ascii_letters + string.digits,\n",
    "        '[__ASCII_PRINTABLE__]': string.printable\n",
    "        }\n",
    "\n",
    "class Fuzzer:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    def fuzz(self, key='<start>', max_num=None, max_depth=None):\n",
    "        raise NotImplemented()\n",
    "\n",
    "FUZZRANGE = 10\n",
    "\n",
    "class LimitFuzzer(Fuzzer):\n",
    "    def symbol_cost(self, grammar, symbol, seen):\n",
    "        if symbol in self.key_cost: return self.key_cost[symbol]\n",
    "        if symbol in seen:\n",
    "            self.key_cost[symbol] = float('inf')\n",
    "            return float('inf')\n",
    "        v = min((self.expansion_cost(grammar, rule, seen | {symbol})\n",
    "                    for rule in grammar.get(symbol, [])), default=0)\n",
    "        self.key_cost[symbol] = v\n",
    "        return v\n",
    "\n",
    "    def expansion_cost(self, grammar, tokens, seen):\n",
    "        return max((self.symbol_cost(grammar, token, seen)\n",
    "                    for token in tokens if token in grammar), default=0) + 1\n",
    "\n",
    "    def nonterminals(self, rule):\n",
    "        return [t for t in rule if is_nt(t)]\n",
    "\n",
    "    def iter_gen_key(self, key, max_depth):\n",
    "        def get_def(t):\n",
    "            if t in ASCII_MAP:\n",
    "                return [random.choice(ASCII_MAP[t]), []]\n",
    "            elif t and t[-1] == '+' and t[0:-1] in ASCII_MAP:\n",
    "                num = random.randrange(FUZZRANGE) + 1\n",
    "                val = [random.choice(ASCII_MAP[t[0:-1]]) for i in range(num)]\n",
    "                return [''.join(val), []]\n",
    "            elif is_nt(t):\n",
    "                return [t, None]\n",
    "            else:\n",
    "                return [t, []]\n",
    "\n",
    "        cheap_grammar = {}\n",
    "        for k in self.cost:\n",
    "            # should we minimize it here? We simply avoid infinities\n",
    "            rules = self.grammar[k]\n",
    "            min_cost = min([self.cost[k][str(r)] for r in rules])\n",
    "            #grammar[k] = [r for r in grammar[k] if self.cost[k][str(r)] == float('inf')]\n",
    "            cheap_grammar[k] = [r for r in self.grammar[k] if self.cost[k][str(r)] == min_cost]\n",
    "\n",
    "        root = [key, None]\n",
    "        queue = [(0, root)]\n",
    "        while queue:\n",
    "            # get one item to expand from the queue\n",
    "            (depth, item), *queue = queue\n",
    "            key = item[0]\n",
    "            if item[1] is not None: continue\n",
    "            grammar = self.grammar if depth < max_depth else cheap_grammar\n",
    "            chosen_rule = random.choice(grammar[key])\n",
    "            expansion = [get_def(t) for t in chosen_rule]\n",
    "            item[1] = expansion\n",
    "            for t in expansion: queue.append((depth+1, t))\n",
    "            #print(\"Fuzz: %s\" % key, len(queue), file=sys.stderr)\n",
    "        #print(file=sys.stderr)\n",
    "        return root\n",
    "    \n",
    "    def gen_key(self, key, depth, max_depth):\n",
    "        if key in ASCII_MAP:\n",
    "            return (random.choice(ASCII_MAP[key]), [])\n",
    "        if key and key[-1] == '+' and key[0:-1] in ASCII_MAP:\n",
    "            m = random.randrange(FUZZRANGE) + 1\n",
    "            return (''.join([random.choice(ASCII_MAP[key[0:-1]]) for i in range(m)]), [])\n",
    "        if key not in self.grammar: return (key, [])\n",
    "        if depth > max_depth:\n",
    "            #return self.gen_key_cheap_iter(key)\n",
    "            clst = sorted([(self.cost[key][str(rule)], rule) for rule in self.grammar[key]])\n",
    "            rules = [r for c,r in clst if c == clst[0][0]]\n",
    "        else:\n",
    "            rules = self.grammar[key]\n",
    "        return (key, self.gen_rule(random.choice(rules), depth+1, max_depth))\n",
    "\n",
    "    def gen_rule(self, rule, depth, max_depth):\n",
    "        return [self.gen_key(token, depth, max_depth) for token in rule]\n",
    "\n",
    "    def fuzz(self, key='<start>', max_depth=10):\n",
    "        return tree_to_str(self.iter_gen_key(key=key, max_depth=max_depth))\n",
    "\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        self.key_cost = {}\n",
    "        self.cost = self.compute_cost(grammar)\n",
    "\n",
    "    def compute_cost(self, grammar):\n",
    "        cost = {}\n",
    "        for k in grammar:\n",
    "            cost[k] = {}\n",
    "            for rule in grammar[k]:\n",
    "                cost[k][str(rule)] = self.expansion_cost(grammar, rule, set())\n",
    "            if len(grammar[k]):\n",
    "                assert len([v for v in cost[k] if v != float('inf')]) > 0\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Our subject programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our taint tracking implementation is incomplete in that only some of the functions in Python are proxied to preserve taints. Hence, we modify source slightly where necessary to use the proxied functions without affecting the evaluation of the grammar inferencing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator.py\n",
    "\n",
    "This is a really simple calculator written in text book recursive descent style. Note that I have used `list()` in a few places to help out with taint tracking. This is due to the limitations of my taint tracking prototype. It can be fixed if required by simple AST walkers or better taint trackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.364273Z",
     "start_time": "2019-11-10T17:56:50.353126Z"
    },
    "code_folding": [],
    "tags": [
     "#calc_src"
    ]
   },
   "outputs": [],
   "source": [
    "%%var calc_src\n",
    "# [(\n",
    "import string\n",
    "\n",
    "def is_digit(i):\n",
    "    return i in string.digits\n",
    "    \n",
    "def parse_num(s,i):\n",
    "    n = ''\n",
    "    while s[i:] and is_digit(s[i]):\n",
    "        n += s[i]\n",
    "        i = i +1\n",
    "    return i,n\n",
    "\n",
    "def parse_paren(s, i):\n",
    "    assert s[i] == '('\n",
    "    i, v = parse_expr(s, i+1)\n",
    "    if s[i:] == '':\n",
    "        raise Exception(s, i)\n",
    "    assert s[i] == ')'\n",
    "    return i+1, v\n",
    "\n",
    "def parse_expr(s, i = 0):\n",
    "    expr = []\n",
    "    is_op = True\n",
    "    while s[i:]:\n",
    "        c = s[i]\n",
    "        if c in string.digits:\n",
    "            if not is_op: raise Exception(s,i)\n",
    "            i,num = parse_num(s,i)\n",
    "            expr.append(num)\n",
    "            is_op = False\n",
    "        elif c in ['+', '-', '*', '/']:\n",
    "            if is_op: raise Exception(s,i)\n",
    "            expr.append(c)\n",
    "            is_op = True\n",
    "            i = i + 1\n",
    "        elif c == '(':\n",
    "            if not is_op: raise Exception(s,i)\n",
    "            i, cexpr = parse_paren(s, i)\n",
    "            expr.append(cexpr)\n",
    "            is_op = False\n",
    "        elif c == ')':\n",
    "            break\n",
    "        else:\n",
    "            raise Exception(s,i)\n",
    "    if is_op:\n",
    "        raise Exception(s,i)\n",
    "    return i, expr\n",
    "\n",
    "def main(arg):\n",
    "    return parse_expr(arg)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathexpr.py\n",
    "\n",
    "Originally from [here]( https://github.com/louisfisch/mathematical-expressions-parser). The mathexpr is much more complicated than our `calculator` and supports advanced functionalities such as predefined functions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.376293Z",
     "start_time": "2019-11-10T17:56:50.367465Z"
    },
    "code_folding": [],
    "tags": [
     "#mathexpr_src"
    ]
   },
   "outputs": [],
   "source": [
    "%%var mathexpr_src\n",
    "# [(\n",
    "import math\n",
    "\n",
    "_CONSTANTS = {\n",
    "    'pi' : math.pi,\n",
    "    'e' : math.e,\n",
    "    'phi': (1 + 5 ** .5) / 2\n",
    "}\n",
    "\n",
    "_FUNCTIONS = {\n",
    "    'abs': abs,\n",
    "    'acos': math.acos,\n",
    "    'asin': math.asin,\n",
    "    'atan': math.atan,\n",
    "    'atan2': math.atan2,\n",
    "    'ceil': math.ceil,\n",
    "    'cos': math.cos,\n",
    "    'cosh': math.cosh,\n",
    "    'degrees': math.degrees,\n",
    "    'exp': math.exp,\n",
    "    'fabs': math.fabs,\n",
    "    'floor': math.floor,\n",
    "    'fmod': math.fmod,\n",
    "    'frexp': math.frexp,\n",
    "    'hypot': math.hypot,\n",
    "    'ldexp': math.ldexp,\n",
    "    'log': math.log,\n",
    "    'log10': math.log10,\n",
    "    'modf': math.modf,\n",
    "    'pow': math.pow,\n",
    "    'radians': math.radians,\n",
    "    'sin': math.sin,\n",
    "    'sinh': math.sinh,\n",
    "    'sqrt': math.sqrt,\n",
    "    'tan': math.tan,\n",
    "    'tanh': math.tanh\n",
    "}\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, string, vars = None):\n",
    "        self.string = string\n",
    "        self.index = 0\n",
    "        self.vars = {} if vars == None else vars.copy()\n",
    "        for constant in _CONSTANTS.keys():\n",
    "            if self.vars.get(constant) != None:\n",
    "                raise Exception(\"Cannot redefine the value of \" + constant)\n",
    "\n",
    "    def getValue(self):\n",
    "        value = self.parseExpression()\n",
    "        self.skipWhitespace()\n",
    "        \n",
    "        if self.hasNext():\n",
    "            raise Exception(\n",
    "                \"Unexpected character found: '\" + self.peek() + \"' at index \" + str(self.index)\n",
    "            )\n",
    "        return value\n",
    "\n",
    "    def peek(self):\n",
    "        return self.string[self.index:self.index + 1]\n",
    "\n",
    "    def hasNext(self):\n",
    "        return self.index < len(self.string)\n",
    "\n",
    "    def isNext(self, value):\n",
    "        return self.string[self.index:self.index+len(value)] == value\n",
    "\n",
    "    def popIfNext(self, value):\n",
    "        if self.isNext(value):\n",
    "            self.index += len(value)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def popExpected(self, value):\n",
    "        if not self.popIfNext(value):\n",
    "            raise Exception(\"Expected '\" + value + \"' at index \" + str(self.index))\n",
    "\n",
    "\n",
    "    def skipWhitespace(self):\n",
    "        while self.hasNext():\n",
    "            if self.peek() in ' \\t\\n\\r':\n",
    "                self.index += 1\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    def parseExpression(self):\n",
    "        return self.parseAddition()\n",
    "    \n",
    "    def parseAddition(self):\n",
    "        values = [self.parseMultiplication()]\n",
    "        \n",
    "        while True:\n",
    "            self.skipWhitespace()\n",
    "            char = self.peek()\n",
    "            \n",
    "            if char == '+':\n",
    "                self.index += 1\n",
    "                values.append(self.parseMultiplication())\n",
    "            elif char == '-':\n",
    "                self.index += 1\n",
    "                values.append(-1 * self.parseMultiplication())\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return sum(values)\n",
    "\n",
    "    def parseMultiplication(self):\n",
    "        values = [self.parseParenthesis()]\n",
    "            \n",
    "        while True:\n",
    "            self.skipWhitespace()\n",
    "            char = self.peek()\n",
    "                \n",
    "            if char == '*':\n",
    "                self.index += 1\n",
    "                values.append(self.parseParenthesis())\n",
    "            elif char == '/':\n",
    "                div_index = self.index\n",
    "                self.index += 1\n",
    "                denominator = self.parseParenthesis()\n",
    "                     \n",
    "                if denominator == 0:\n",
    "                    raise Exception(\n",
    "                        \"Division by 0 kills baby whales (occured at index \" + str(div_index) + \")\"\n",
    "                    )\n",
    "                values.append(1.0 / denominator)\n",
    "            else:\n",
    "                break\n",
    "                     \n",
    "        value = 1.0\n",
    "        \n",
    "        for factor in values:\n",
    "            value *= factor\n",
    "        return value\n",
    "\n",
    "    def parseParenthesis(self):\n",
    "        self.skipWhitespace()\n",
    "        char = self.peek()\n",
    "        \n",
    "        if char == '(':\n",
    "            self.index += 1\n",
    "            value = self.parseExpression()\n",
    "            self.skipWhitespace()\n",
    "            \n",
    "            if self.peek() != ')':\n",
    "                raise Exception(\n",
    "                    \"No closing parenthesis found at character \" + str(self.index)\n",
    "                )\n",
    "            self.index += 1\n",
    "            return value\n",
    "        else:\n",
    "            return self.parseNegative()\n",
    "\n",
    "    def parseArguments(self):\n",
    "        args = []\n",
    "        self.skipWhitespace()\n",
    "        self.popExpected('(')\n",
    "        while not self.popIfNext(')'):\n",
    "            self.skipWhitespace()\n",
    "            if len(args) > 0:\n",
    "                self.popExpected(',')\n",
    "                self.skipWhitespace()\n",
    "            args.append(self.parseExpression())\n",
    "            self.skipWhitespace()\n",
    "        return args\n",
    "\n",
    "    def parseNegative(self):\n",
    "        self.skipWhitespace()\n",
    "        char = self.peek()\n",
    "        \n",
    "        if char == '-':\n",
    "            self.index += 1\n",
    "            return -1 * self.parseParenthesis()\n",
    "        else:\n",
    "            return self.parseValue()\n",
    "\n",
    "    def parseValue(self):\n",
    "        self.skipWhitespace()\n",
    "        char = self.peek()\n",
    "        \n",
    "        if char in '0123456789.':\n",
    "            return self.parseNumber()\n",
    "        else:\n",
    "            return self.parseVariable()\n",
    " \n",
    "    def parseVariable(self):\n",
    "        self.skipWhitespace()\n",
    "        var = []\n",
    "        while self.hasNext():\n",
    "            char = self.peek()\n",
    "            \n",
    "            if char.lower() in '_abcdefghijklmnopqrstuvwxyz0123456789':\n",
    "                var.append(char)\n",
    "                self.index += 1\n",
    "            else:\n",
    "                break\n",
    "        #s = ''\n",
    "        #for a in var:\n",
    "        #    s += a # CHANGE from ORIGINAL to preserve taints. We need to taints.w__ join() calls.\n",
    "        var = ''.join(var)\n",
    "        \n",
    "        function = _FUNCTIONS.get(var.lower())\n",
    "        if function != None:\n",
    "            args = self.parseArguments()\n",
    "            return float(function(*args))\n",
    "        \n",
    "        constant = _CONSTANTS.get(var.lower())\n",
    "        if constant != None:\n",
    "            return constant\n",
    "\n",
    "        value = self.vars.get(var, None)\n",
    "        if value != None:\n",
    "            return float(value)\n",
    "            \n",
    "        raise Exception(\"Unrecognized variable: '\" + var + \"'\")\n",
    "\n",
    "    def parseNumber(self):\n",
    "        self.skipWhitespace()\n",
    "        strValue = ''\n",
    "        decimal_found = False\n",
    "        char = ''\n",
    "\n",
    "        while self.hasNext():\n",
    "            char = self.peek()            \n",
    "            \n",
    "            if char == '.':\n",
    "                if decimal_found:\n",
    "                    raise Exception(\n",
    "                        \"Found an extra period in a number at character \" + str(self.index) + \". Are you European?\"\n",
    "                    )\n",
    "                decimal_found = True\n",
    "                strValue += '.'\n",
    "            elif char in '0123456789':\n",
    "                strValue += char\n",
    "            else:\n",
    "                break\n",
    "            self.index += 1\n",
    "\n",
    "        if len(strValue) == 0:\n",
    "            if char == '':\n",
    "                raise Exception(\"Unexpected end found\")\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"I was expecting to find a number at character \" + str(self.index) + \" but instead I found a '\" + char + \"'. What's up with that?\")\n",
    "\n",
    "        return float(strValue)\n",
    "import string\n",
    "def main(arg):\n",
    "    p = Parser(arg, {a:ord(a) for a in string.ascii_lowercase if a != 'e'})\n",
    "    p.getValue()\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microjson.py\n",
    "The microjson is a complete pure python implementation of JSON parser, that was obtained from from [here](https://github.com/phensley/microjson). Note that we use `myio` which is an instrumented version of the original `io` to preserve taints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.388735Z",
     "start_time": "2019-11-10T17:56:50.380183Z"
    },
    "code_folding": [],
    "tags": [
     "#microjson_src"
    ]
   },
   "outputs": [],
   "source": [
    "%%var microjson_src\n",
    "# [(\n",
    "# microjson - Minimal JSON parser/emitter for use in standalone scripts.\n",
    "# No warranty. Free to use/modify as you see fit. Trades speed for compactness.\n",
    "# Send ideas, bugs, simplifications to http://github.com/phensley\n",
    "# Copyright (c) 2010 Patrick Hensley <spaceboy@indirect.com>\n",
    "\n",
    "# std\n",
    "import math\n",
    "import myio as io\n",
    "import types\n",
    "\n",
    "\n",
    "# the '_from_json_number' function returns either float or long.\n",
    "__pychecker__ = 'no-returnvalues'\n",
    "\n",
    "# character classes\n",
    "WS = ' ' # ''.join([' ','\\t','\\r','\\n','\\b','\\f'])\n",
    "DIGITS = ''.join([str(i) for i in range(0, 10)])\n",
    "NUMSTART = DIGITS + ''.join(['.','-','+'])\n",
    "NUMCHARS = NUMSTART + ''.join(['e','E'])\n",
    "ESC_MAP = {'n':'\\n','t':'\\t','r':'\\r','b':'\\b','f':'\\f'}\n",
    "REV_ESC_MAP = dict([(_v,_k) for _k,_v in list(ESC_MAP.items())] + [('\"','\"')])\n",
    "\n",
    "# error messages\n",
    "E_BYTES = 'input string must be type str containing ASCII or UTF-8 bytes'\n",
    "E_MALF = 'malformed JSON data'\n",
    "E_TRUNC = 'truncated JSON data'\n",
    "E_BOOL = 'expected boolean'\n",
    "E_NULL = 'expected null'\n",
    "E_LITEM = 'expected list item'\n",
    "E_DKEY = 'expected key'\n",
    "E_COMMA = 'missing comma between elements'\n",
    "E_COLON = 'missing colon after key'\n",
    "E_EMPTY = 'found empty string, not valid JSON data'\n",
    "E_BADESC = 'bad escape character found'\n",
    "E_UNSUPP = 'unsupported type \"%s\" cannot be JSON-encoded'\n",
    "E_BADFLOAT = 'cannot emit floating point value \"%s\"'\n",
    "\n",
    "NEG_INF = float('-inf')\n",
    "POS_INF = float('inf')\n",
    "\n",
    "\n",
    "class JSONError(Exception):\n",
    "    def __init__(self, msg, stm=None, pos=0):\n",
    "        if stm:\n",
    "            msg += ' at position %d, \"%s\"' % (pos, repr(stm.substr(pos, 32)))\n",
    "        Exception.__init__(self, msg)\n",
    "\n",
    "\n",
    "class JSONStream:\n",
    "\n",
    "    # no longer inherit directly from StringIO, since we only want to\n",
    "    # expose the methods below and not allow direct access to the\n",
    "    # underlying stream.\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self._stm = io.StringIO(data)\n",
    "\n",
    "    @property\n",
    "    def pos(self):\n",
    "        return self._stm.tell()\n",
    "\n",
    "    @property\n",
    "    def len(self):\n",
    "        return len(self._stm.getvalue())\n",
    "\n",
    "    def getvalue(self):\n",
    "        return self._stm.getvalue()\n",
    "\n",
    "    def skipspaces(self):\n",
    "        \"post-cond: read pointer will be over first non-WS char\"\n",
    "        self._skip(lambda c: not c in WS)\n",
    "\n",
    "    def _skip(self, stopcond):\n",
    "        while True:\n",
    "            c = self.peek()\n",
    "            if stopcond(c) or c == '':\n",
    "                break\n",
    "            self.next()\n",
    "\n",
    "    def next(self, size=1):\n",
    "        return self._stm.read(size)\n",
    "\n",
    "    def next_ord(self):\n",
    "        return ord(next(self))\n",
    "\n",
    "    def peek(self):\n",
    "        if self.pos == self.len:\n",
    "            return self.getvalue()[self.pos:]\n",
    "        return self.getvalue()[self.pos]\n",
    "\n",
    "    def substr(self, pos, length):\n",
    "        return self.getvalue()[pos:pos+length]\n",
    "\n",
    "\n",
    "def _decode_utf8(c0, stm):\n",
    "    c0 = ord(c0)\n",
    "    r = 0xFFFD      # unicode replacement character\n",
    "    nc = stm.next_ord\n",
    "\n",
    "    # 110yyyyy 10zzzzzz\n",
    "    if (c0 & 0xE0) == 0xC0:\n",
    "        r = ((c0 & 0x1F) << 6) + (nc() & 0x3F)\n",
    "\n",
    "    # 1110xxxx 10yyyyyy 10zzzzzz\n",
    "    elif (c0 & 0xF0) == 0xE0:\n",
    "        r = ((c0 & 0x0F) << 12) + ((nc() & 0x3F) << 6) + (nc() & 0x3F)\n",
    "\n",
    "    # 11110www 10xxxxxx 10yyyyyy 10zzzzzz\n",
    "    elif (c0 & 0xF8) == 0xF0:\n",
    "        r = ((c0 & 0x07) << 18) + ((nc() & 0x3F) << 12) + \\\n",
    "            ((nc() & 0x3F) << 6) + (nc() & 0x3F)\n",
    "    return chr(r)\n",
    "\n",
    "\n",
    "def decode_escape(c, stm):\n",
    "    # whitespace\n",
    "    v = ESC_MAP.get(c, None)\n",
    "    if v is not None:\n",
    "        return v\n",
    "\n",
    "    # plain character\n",
    "    elif c != 'u':\n",
    "        return c\n",
    "\n",
    "    # decode unicode escape \\u1234\n",
    "    sv = 12\n",
    "    r = 0\n",
    "    for _ in range(0, 4):\n",
    "        r |= int(stm.next(), 16) << sv\n",
    "        sv -= 4\n",
    "    return chr(r)\n",
    "\n",
    "\n",
    "def _from_json_string(stm):\n",
    "    # skip over '\"'\n",
    "    stm.next()\n",
    "    r = ''\n",
    "    while True:\n",
    "        c = stm.next()\n",
    "        if c == '':\n",
    "            raise JSONError(E_TRUNC, stm, stm.pos - 1)\n",
    "        elif c == '\\\\':\n",
    "            c = stm.next()\n",
    "            r += decode_escape(c, stm)\n",
    "        elif c == '\"':\n",
    "            return r\n",
    "        elif c in [str(i) for i in range(127, 256)]:\n",
    "            r += _decode_utf8(c, stm)\n",
    "        else:\n",
    "            r += c\n",
    "\n",
    "\n",
    "def _from_json_fixed(stm, expected, value, errmsg):\n",
    "    off = len(expected)\n",
    "    pos = stm.pos\n",
    "    res = stm.substr(pos, off)\n",
    "    if res == expected:\n",
    "        stm.next(off)\n",
    "        return res\n",
    "    raise JSONError(errmsg, stm, pos)\n",
    "\n",
    "\n",
    "def _from_json_number(stm):\n",
    "    # Per rfc 4627 section 2.4 '0' and '0.1' are valid, but '01' and\n",
    "    # '01.1' are not, presumably since this would be confused with an\n",
    "    # octal number.  This rule is not enforced.\n",
    "    is_float = 0\n",
    "    saw_exp = 0\n",
    "    pos = stm.pos\n",
    "    while True:\n",
    "        c = stm.peek()\n",
    "        if not c: break\n",
    "\n",
    "        if not c in NUMCHARS:\n",
    "            break\n",
    "        elif c == '-' and not saw_exp:\n",
    "            pass\n",
    "        elif c in '.eE':\n",
    "            is_float = 1\n",
    "            if c in 'eE':\n",
    "                saw_exp = 1\n",
    "\n",
    "        stm.next()\n",
    "\n",
    "    s = stm.substr(pos, stm.pos - pos)\n",
    "    if is_float:\n",
    "        return s\n",
    "    return s\n",
    "\n",
    "\n",
    "def _from_json_list(stm):\n",
    "    # skip over '['\n",
    "    stm.next()\n",
    "    result = []\n",
    "    pos = stm.pos\n",
    "    comma = False\n",
    "    while True:\n",
    "        stm.skipspaces()\n",
    "        c = stm.peek()\n",
    "        if c == '':\n",
    "            raise JSONError(E_TRUNC, stm, pos)\n",
    "\n",
    "        elif c == ']':\n",
    "            stm.next()\n",
    "            return result\n",
    "\n",
    "        elif c == ',':\n",
    "            if not result:\n",
    "                raise JSONError(E_TRUNC, stm, pos)\n",
    "            if comma:\n",
    "                raise JSONError(E_TRUNC, stm, pos)\n",
    "            comma = True\n",
    "            stm.next()\n",
    "            result.append(_from_json_raw(stm))\n",
    "            comma = False\n",
    "            continue\n",
    "\n",
    "        elif not result:\n",
    "            # first item\n",
    "            result.append(_from_json_raw(stm))\n",
    "            comma = False\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            raise JSONError(E_MALF, stm, stm.pos)\n",
    "\n",
    "\n",
    "def _from_json_dict(stm):\n",
    "    # skip over '{'\n",
    "    stm.next()\n",
    "    result = {}\n",
    "    expect_key = 1\n",
    "    pos = stm.pos\n",
    "    comma = False\n",
    "    while True:\n",
    "        stm.skipspaces()\n",
    "        c = stm.peek()\n",
    "        if c == '':\n",
    "            raise JSONError(E_TRUNC, stm, pos)\n",
    "\n",
    "        # end of dictionary, or next item\n",
    "        elif c == '}':\n",
    "            if expect_key == 2:\n",
    "                raise JSONError(E_TRUNC, stm, pos)\n",
    "            stm.next()\n",
    "            return result\n",
    "\n",
    "        elif c == ',':\n",
    "            if not result:\n",
    "                raise JSONError(E_TRUNC, stm, pos)\n",
    "            if comma:\n",
    "                raise JSONError(E_TRUNC, stm, pos)\n",
    "            comma = True\n",
    "            stm.next()\n",
    "            expect_key = 2\n",
    "            continue\n",
    "\n",
    "        # parse out a key/value pair\n",
    "        elif c == '\"':\n",
    "            if not expect_key:\n",
    "                raise JSONError(E_COMMA, stm, stm.pos)\n",
    "            key = _from_json_string(stm)\n",
    "            stm.skipspaces()\n",
    "            c = stm.next()\n",
    "            if c != ':':\n",
    "                raise JSONError(E_COLON, stm, stm.pos)\n",
    "\n",
    "            stm.skipspaces()\n",
    "            val = _from_json_raw(stm)\n",
    "            result[key] = val\n",
    "            expect_key = 0\n",
    "            comma = False\n",
    "            continue\n",
    "\n",
    "        # unexpected character in middle of dict\n",
    "        raise JSONError(E_MALF, stm, stm.pos)\n",
    "\n",
    "\n",
    "def _from_json_raw(stm):\n",
    "    while True:\n",
    "        stm.skipspaces()\n",
    "        c = stm.peek()\n",
    "        if c == '\"': \n",
    "            return _from_json_string(stm)\n",
    "        elif c == '{': \n",
    "            return _from_json_dict(stm)\n",
    "        elif c == '[': \n",
    "            return _from_json_list(stm)\n",
    "        elif c == 't':\n",
    "            return _from_json_fixed(stm, 'true', True, E_BOOL)\n",
    "        elif c == 'f':\n",
    "            return _from_json_fixed(stm, 'false', False, E_BOOL)\n",
    "        elif c == 'n': \n",
    "            return _from_json_fixed(stm, 'null', None, E_NULL)\n",
    "        elif c in NUMSTART:\n",
    "            return _from_json_number(stm)\n",
    "\n",
    "        raise JSONError(E_MALF, stm, stm.pos)\n",
    "\n",
    "\n",
    "def from_json(data):\n",
    "    \"\"\"\n",
    "    Converts 'data' which is UTF-8 (or the 7-bit pure ASCII subset) into\n",
    "    a Python representation.  You must pass bytes to this in a str type,\n",
    "    not unicode.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, str):\n",
    "        raise JSONError(E_BYTES)\n",
    "    if not data:\n",
    "        return None\n",
    "    stm = JSONStream(data)\n",
    "    v = _from_json_raw(stm)\n",
    "    c = stm.peek()\n",
    "    if c:\n",
    "        raise JSONError(E_BYTES)\n",
    "    return v\n",
    "\n",
    "\n",
    "# JSON emitter\n",
    "\n",
    "def _to_json_list(stm, lst):\n",
    "    seen = 0\n",
    "    stm.write('[')\n",
    "    for elem in lst:\n",
    "        if seen:\n",
    "            stm.write(',')\n",
    "        seen = 1\n",
    "        _to_json_object(stm, elem)\n",
    "    stm.write(']')\n",
    "\n",
    "\n",
    "def _to_json_string(stm, buf):\n",
    "    stm.write('\"')\n",
    "    for c in buf:\n",
    "        nc = REV_ESC_MAP.get(c, None)\n",
    "        if nc:\n",
    "            stm.write('\\\\' + nc)\n",
    "        elif ord(c) <= 0x7F:\n",
    "            # force ascii\n",
    "            stm.write(str(c))\n",
    "        else:\n",
    "            stm.write('\\\\u%04x' % ord(c))\n",
    "    stm.write('\"')\n",
    "\n",
    "\n",
    "def _to_json_dict(stm, dct):\n",
    "    seen = 0\n",
    "    stm.write('{')\n",
    "    for key in list(dct.keys()):\n",
    "        if seen:\n",
    "            stm.write(',')\n",
    "        seen = 1\n",
    "        val = dct[key]\n",
    "        if not type(key) in (bytes, str):\n",
    "            key = str(key)\n",
    "        _to_json_string(stm, key)\n",
    "        stm.write(':')\n",
    "        _to_json_object(stm, val)\n",
    "    stm.write('}')\n",
    "\n",
    "\n",
    "def _to_json_object(stm, obj):\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        _to_json_list(stm, obj)\n",
    "    elif isinstance(obj, bool):\n",
    "        if obj:\n",
    "            stm.write('true')\n",
    "        else:\n",
    "            stm.write('false')\n",
    "    elif isinstance(obj, float):\n",
    "        # this raises an error for NaN, -inf and inf values\n",
    "        if not (NEG_INF < obj < POS_INF):\n",
    "            raise JSONError(E_BADFLOAT % obj)\n",
    "        stm.write(\"%s\" % obj)\n",
    "    elif isinstance(obj, int):\n",
    "        stm.write(\"%d\" % obj)\n",
    "    elif isinstance(obj, type(None)):\n",
    "        stm.write('null')\n",
    "    elif isinstance(obj, (bytes, str)):\n",
    "        _to_json_string(stm, obj)\n",
    "    elif hasattr(obj, 'keys') and hasattr(obj, '__getitem__'):\n",
    "        _to_json_dict(stm, obj)\n",
    "    # fall back to implicit string conversion.\n",
    "    elif hasattr(obj, '__unicode__'):\n",
    "        _to_json_string(stm, obj.__unicode__())\n",
    "    elif hasattr(obj, '__str__'):\n",
    "        _to_json_string(stm, obj.__str__())\n",
    "    else:\n",
    "        raise JSONError(E_UNSUPP % type(obj))\n",
    "\n",
    "\n",
    "def to_json(obj):\n",
    "    \"\"\"\n",
    "    Converts 'obj' to an ASCII JSON string representation.\n",
    "    \"\"\"\n",
    "    stm = io.StringIO('')\n",
    "    _to_json_object(stm, obj)\n",
    "    return stm.getvalue()\n",
    "\n",
    "\n",
    "decode = from_json\n",
    "encode = to_json\n",
    "\n",
    "def main(arg):\n",
    "    return from_json(arg)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLParse.py\n",
    "\n",
    "This is the URL parser that is part of the Python distribution. The source was obtained from [here](https://github.com/python/cpython/blob/3.6/Lib/urllib/parse.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.407031Z",
     "start_time": "2019-11-10T17:56:50.392052Z"
    },
    "code_folding": [],
    "tags": [
     "#urlparse_src"
    ]
   },
   "outputs": [],
   "source": [
    "%%var urlparse_src\n",
    "# [(\n",
    "\"\"\"Parse (absolute and relative) URLs.\n",
    "\n",
    "urlparse module is based upon the following RFC specifications.\n",
    "\n",
    "RFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\n",
    "and L.  Masinter, January 2005.\n",
    "\n",
    "RFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\n",
    "and L.Masinter, December 1999.\n",
    "\n",
    "RFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\n",
    "Berners-Lee, R. Fielding, and L. Masinter, August 1998.\n",
    "\n",
    "RFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zawinski, July 1998.\n",
    "\n",
    "RFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n",
    "1995.\n",
    "\n",
    "RFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\n",
    "McCahill, December 1994\n",
    "\n",
    "RFC 3986 is considered the current standard and any future changes to\n",
    "urlparse module should conform with it.  The urlparse module is\n",
    "currently not entirely compliant with this RFC due to defacto\n",
    "scenarios for parsing, and for backward compatibility purposes, some\n",
    "parsing quirks from older RFCs are retained. The testcases in\n",
    "test_urlparse.py provides a good indicator of parsing behavior.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n",
    "           \"urlsplit\", \"urlunsplit\", \"urlencode\", \"parse_qs\",\n",
    "           \"parse_qsl\", \"quote\", \"quote_plus\", \"quote_from_bytes\",\n",
    "           \"unquote\", \"unquote_plus\", \"unquote_to_bytes\",\n",
    "           \"DefragResult\", \"ParseResult\", \"SplitResult\",\n",
    "           \"DefragResultBytes\", \"ParseResultBytes\", \"SplitResultBytes\"]\n",
    "\n",
    "# A classification of schemes.\n",
    "# The empty string classifies URLs with no scheme specified,\n",
    "# being the default value returned by \"urlsplit\" and \"urlparse\".\n",
    "\n",
    "uses_relative = ['', 'ftp', 'http', 'gopher', 'nntp', 'imap',\n",
    "                 'wais', 'file', 'https', 'shttp', 'mms',\n",
    "                 'prospero', 'rtsp', 'rtspu', 'sftp',\n",
    "                 'svn', 'svn+ssh', 'ws', 'wss']\n",
    "\n",
    "uses_netloc = ['', 'ftp', 'http', 'gopher', 'nntp', 'telnet',\n",
    "               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n",
    "               'snews', 'prospero', 'rtsp', 'rtspu', 'rsync',\n",
    "               'svn', 'svn+ssh', 'sftp', 'nfs', 'git', 'git+ssh',\n",
    "               'ws', 'wss']\n",
    "\n",
    "uses_params = ['', 'ftp', 'hdl', 'prospero', 'http', 'imap',\n",
    "               'https', 'shttp', 'rtsp', 'rtspu', 'sip', 'sips',\n",
    "               'mms', 'sftp', 'tel']\n",
    "\n",
    "# These are not actually used anymore, but should stay for backwards\n",
    "# compatibility.  (They are undocumented, but have a public-looking name.)\n",
    "\n",
    "non_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n",
    "                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\n",
    "\n",
    "uses_query = ['', 'http', 'wais', 'imap', 'https', 'shttp', 'mms',\n",
    "              'gopher', 'rtsp', 'rtspu', 'sip', 'sips']\n",
    "\n",
    "uses_fragment = ['', 'ftp', 'hdl', 'http', 'gopher', 'news',\n",
    "                 'nntp', 'wais', 'https', 'shttp', 'snews',\n",
    "                 'file', 'prospero']\n",
    "\n",
    "# Characters valid in scheme names\n",
    "scheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n",
    "                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "                '0123456789'\n",
    "                '+-.')\n",
    "\n",
    "# XXX: Consider replacing with functools.lru_cache\n",
    "MAX_CACHE_SIZE = 20\n",
    "_parse_cache = {}\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear the parse cache and the quoters cache.\"\"\"\n",
    "    _parse_cache.clear()\n",
    "    _safe_quoters.clear()\n",
    "\n",
    "\n",
    "# Helpers for bytes handling\n",
    "# For 3.2, we deliberately require applications that\n",
    "# handle improperly quoted URLs to do their own\n",
    "# decoding and encoding. If valid use cases are\n",
    "# presented, we may relax this by using latin-1\n",
    "# decoding internally for 3.3\n",
    "_implicit_encoding = 'ascii'\n",
    "_implicit_errors = 'strict'\n",
    "\n",
    "def _noop(obj):\n",
    "    return obj\n",
    "\n",
    "def _encode_result(obj, encoding=_implicit_encoding,\n",
    "                        errors=_implicit_errors):\n",
    "    return obj.encode(encoding, errors)\n",
    "\n",
    "def _decode_args(args, encoding=_implicit_encoding,\n",
    "                       errors=_implicit_errors):\n",
    "    return tuple(x.decode(encoding, errors) if x else '' for x in args)\n",
    "\n",
    "def _coerce_args(*args):\n",
    "    # Invokes decode if necessary to create str args\n",
    "    # and returns the coerced inputs along with\n",
    "    # an appropriate result coercion function\n",
    "    #   - noop for str inputs\n",
    "    #   - encoding function otherwise\n",
    "    str_input = isinstance(args[0], str)\n",
    "    for arg in args[1:]:\n",
    "        # We special-case the empty string to support the\n",
    "        # \"scheme=''\" default argument to some functions\n",
    "        if arg and isinstance(arg, str) != str_input:\n",
    "            raise TypeError(\"Cannot mix str and non-str arguments\")\n",
    "    if str_input:\n",
    "        return args + (_noop,)\n",
    "    return _decode_args(args) + (_encode_result,)\n",
    "\n",
    "# Result objects are more helpful than simple tuples\n",
    "class _ResultMixinStr(object):\n",
    "    \"\"\"Standard approach to encoding parsed results from str to bytes\"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def encode(self, encoding='ascii', errors='strict'):\n",
    "        return self._encoded_counterpart(*(x.encode(encoding, errors) for x in self))\n",
    "\n",
    "\n",
    "class _ResultMixinBytes(object):\n",
    "    \"\"\"Standard approach to decoding parsed results from bytes to str\"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    def decode(self, encoding='ascii', errors='strict'):\n",
    "        return self._decoded_counterpart(*(x.decode(encoding, errors) for x in self))\n",
    "\n",
    "\n",
    "class _NetlocResultMixinBase(object):\n",
    "    \"\"\"Shared methods for the parsed result objects containing a netloc element\"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    @property\n",
    "    def username(self):\n",
    "        return self._userinfo[0]\n",
    "\n",
    "    @property\n",
    "    def password(self):\n",
    "        return self._userinfo[1]\n",
    "\n",
    "    @property\n",
    "    def hostname(self):\n",
    "        hostname = self._hostinfo[0]\n",
    "        if not hostname:\n",
    "            return None\n",
    "        # Scoped IPv6 address may have zone info, which must not be lowercased\n",
    "        # like http://[fe80::822a:a8ff:fe49:470c%tESt]:1234/keys\n",
    "        separator = '%' if isinstance(hostname, str) else b'%'\n",
    "        hostname, percent, zone = hostname.partition(separator)\n",
    "        return hostname.lower() + percent + zone\n",
    "\n",
    "    @property\n",
    "    def port(self):\n",
    "        port = self._hostinfo[1]\n",
    "        if port is not None:\n",
    "            port = int(port, 10)\n",
    "            if not ( 0 <= port <= 65535):\n",
    "                raise ValueError(\"Port out of range 0-65535\")\n",
    "        return port\n",
    "\n",
    "\n",
    "class _NetlocResultMixinStr(_NetlocResultMixinBase, _ResultMixinStr):\n",
    "    __slots__ = ()\n",
    "\n",
    "    @property\n",
    "    def _userinfo(self):\n",
    "        netloc = self.netloc\n",
    "        userinfo, have_info, hostinfo = netloc.rpartition('@')\n",
    "        if have_info:\n",
    "            username, have_password, password = userinfo.partition(':')\n",
    "            if not have_password:\n",
    "                password = None\n",
    "        else:\n",
    "            username = password = None\n",
    "        return username, password\n",
    "\n",
    "    @property\n",
    "    def _hostinfo(self):\n",
    "        netloc = self.netloc\n",
    "        _, _, hostinfo = netloc.rpartition('@')\n",
    "        _, have_open_br, bracketed = hostinfo.partition('[')\n",
    "        if have_open_br:\n",
    "            hostname, _, port = bracketed.partition(']')\n",
    "            _, _, port = port.partition(':')\n",
    "        else:\n",
    "            hostname, _, port = hostinfo.partition(':')\n",
    "        if not port:\n",
    "            port = None\n",
    "        return hostname, port\n",
    "\n",
    "\n",
    "class _NetlocResultMixinBytes(_NetlocResultMixinBase, _ResultMixinBytes):\n",
    "    __slots__ = ()\n",
    "\n",
    "    @property\n",
    "    def _userinfo(self):\n",
    "        netloc = self.netloc\n",
    "        userinfo, have_info, hostinfo = netloc.rpartition(b'@')\n",
    "        if have_info:\n",
    "            username, have_password, password = userinfo.partition(b':')\n",
    "            if not have_password:\n",
    "                password = None\n",
    "        else:\n",
    "            username = password = None\n",
    "        return username, password\n",
    "\n",
    "    @property\n",
    "    def _hostinfo(self):\n",
    "        netloc = self.netloc\n",
    "        _, _, hostinfo = netloc.rpartition(b'@')\n",
    "        _, have_open_br, bracketed = hostinfo.partition(b'[')\n",
    "        if have_open_br:\n",
    "            hostname, _, port = bracketed.partition(b']')\n",
    "            _, _, port = port.partition(b':')\n",
    "        else:\n",
    "            hostname, _, port = hostinfo.partition(b':')\n",
    "        if not port:\n",
    "            port = None\n",
    "        return hostname, port\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "_DefragResultBase = namedtuple('DefragResult', 'url fragment')\n",
    "_SplitResultBase = namedtuple(\n",
    "    'SplitResult', 'scheme netloc path query fragment')\n",
    "_ParseResultBase = namedtuple(\n",
    "    'ParseResult', 'scheme netloc path params query fragment')\n",
    "\n",
    "_DefragResultBase.__doc__ = \"\"\"\n",
    "DefragResult(url, fragment)\n",
    "\n",
    "A 2-tuple that contains the url without fragment identifier and the fragment\n",
    "identifier as a separate argument.\n",
    "\"\"\"\n",
    "\n",
    "_DefragResultBase.url.__doc__ = \"\"\"The URL with no fragment identifier.\"\"\"\n",
    "\n",
    "_DefragResultBase.fragment.__doc__ = \"\"\"\n",
    "Fragment identifier separated from URL, that allows indirect identification of a\n",
    "secondary resource by reference to a primary resource and additional identifying\n",
    "information.\n",
    "\"\"\"\n",
    "\n",
    "_SplitResultBase.__doc__ = \"\"\"\n",
    "SplitResult(scheme, netloc, path, query, fragment)\n",
    "\n",
    "A 5-tuple that contains the different components of a URL. Similar to\n",
    "ParseResult, but does not split params.\n",
    "\"\"\"\n",
    "\n",
    "_SplitResultBase.scheme.__doc__ = \"\"\"Specifies URL scheme for the request.\"\"\"\n",
    "\n",
    "_SplitResultBase.netloc.__doc__ = \"\"\"\n",
    "Network location where the request is made to.\n",
    "\"\"\"\n",
    "\n",
    "_SplitResultBase.path.__doc__ = \"\"\"\n",
    "The hierarchical path, such as the path to a file to download.\n",
    "\"\"\"\n",
    "\n",
    "_SplitResultBase.query.__doc__ = \"\"\"\n",
    "The query component, that contains non-hierarchical data, that along with data\n",
    "in path component, identifies a resource in the scope of URI's scheme and\n",
    "network location.\n",
    "\"\"\"\n",
    "\n",
    "_SplitResultBase.fragment.__doc__ = \"\"\"\n",
    "Fragment identifier, that allows indirect identification of a secondary resource\n",
    "by reference to a primary resource and additional identifying information.\n",
    "\"\"\"\n",
    "\n",
    "_ParseResultBase.__doc__ = \"\"\"\n",
    "ParseResult(scheme, netloc, path, params,  query, fragment)\n",
    "\n",
    "A 6-tuple that contains components of a parsed URL.\n",
    "\"\"\"\n",
    "\n",
    "_ParseResultBase.scheme.__doc__ = _SplitResultBase.scheme.__doc__\n",
    "_ParseResultBase.netloc.__doc__ = _SplitResultBase.netloc.__doc__\n",
    "_ParseResultBase.path.__doc__ = _SplitResultBase.path.__doc__\n",
    "_ParseResultBase.params.__doc__ = \"\"\"\n",
    "Parameters for last path element used to dereference the URI in order to provide\n",
    "access to perform some operation on the resource.\n",
    "\"\"\"\n",
    "\n",
    "_ParseResultBase.query.__doc__ = _SplitResultBase.query.__doc__\n",
    "_ParseResultBase.fragment.__doc__ = _SplitResultBase.fragment.__doc__\n",
    "\n",
    "\n",
    "# For backwards compatibility, alias _NetlocResultMixinStr\n",
    "# ResultBase is no longer part of the documented API, but it is\n",
    "# retained since deprecating it isn't worth the hassle\n",
    "ResultBase = _NetlocResultMixinStr\n",
    "\n",
    "# Structured result objects for string data\n",
    "class DefragResult(_DefragResultBase, _ResultMixinStr):\n",
    "    __slots__ = ()\n",
    "    def geturl(self):\n",
    "        if self.fragment:\n",
    "            return self.url + '#' + self.fragment\n",
    "        else:\n",
    "            return self.url\n",
    "\n",
    "class SplitResult(_SplitResultBase, _NetlocResultMixinStr):\n",
    "    __slots__ = ()\n",
    "    def geturl(self):\n",
    "        return urlunsplit(self)\n",
    "\n",
    "class ParseResult(_ParseResultBase, _NetlocResultMixinStr):\n",
    "    __slots__ = ()\n",
    "    def geturl(self):\n",
    "        return urlunparse(self)\n",
    "\n",
    "# Structured result objects for bytes data\n",
    "class DefragResultBytes(_DefragResultBase, _ResultMixinBytes):\n",
    "    __slots__ = ()\n",
    "    def geturl(self):\n",
    "        if self.fragment:\n",
    "            return self.url + b'#' + self.fragment\n",
    "        else:\n",
    "            return self.url\n",
    "\n",
    "class SplitResultBytes(_SplitResultBase, _NetlocResultMixinBytes):\n",
    "    __slots__ = ()\n",
    "    def geturl(self):\n",
    "        return urlunsplit(self)\n",
    "\n",
    "class ParseResultBytes(_ParseResultBase, _NetlocResultMixinBytes):\n",
    "    __slots__ = ()\n",
    "    def geturl(self):\n",
    "        return urlunparse(self)\n",
    "\n",
    "# Set up the encode/decode result pairs\n",
    "def _fix_result_transcoding():\n",
    "    _result_pairs = (\n",
    "        (DefragResult, DefragResultBytes),\n",
    "        (SplitResult, SplitResultBytes),\n",
    "        (ParseResult, ParseResultBytes),\n",
    "    )\n",
    "    for _decoded, _encoded in _result_pairs:\n",
    "        _decoded._encoded_counterpart = _encoded\n",
    "        _encoded._decoded_counterpart = _decoded\n",
    "\n",
    "_fix_result_transcoding()\n",
    "del _fix_result_transcoding\n",
    "\n",
    "def urlparse(url, scheme='', allow_fragments=True):\n",
    "    \"\"\"Parse a URL into 6 components:\n",
    "    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n",
    "    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n",
    "    Note that we don't break the components up in smaller bits\n",
    "    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n",
    "    url, scheme, _coerce_result = _coerce_args(url, scheme)\n",
    "    splitresult = urlsplit(url, scheme, allow_fragments)\n",
    "    scheme, netloc, url, query, fragment = splitresult\n",
    "    if scheme in uses_params and ';' in url:\n",
    "        url, params = _splitparams(url)\n",
    "    else:\n",
    "        params = ''\n",
    "    result = ParseResult(scheme, netloc, url, params, query, fragment)\n",
    "    return _coerce_result(result)\n",
    "\n",
    "def _splitparams(url):\n",
    "    if '/'  in url:\n",
    "        i = url.find(';', url.rfind('/'))\n",
    "        if i < 0:\n",
    "            return url, ''\n",
    "    else:\n",
    "        i = url.find(';')\n",
    "    return url[:i], url[i+1:]\n",
    "\n",
    "def _splitnetloc(url, start=0):\n",
    "    delim = len(url)   # position of end of domain part of url, default is end\n",
    "    for c in '/?#':    # look for delimiters; the order is NOT important\n",
    "        wdelim = -1    # FIXME: changed for tracking taints.\n",
    "        for i, c_ in enumerate(url[start:]):\n",
    "            if c_ == c:\n",
    "                wdelim = start + i\n",
    "                break\n",
    "        #wdelim = url.find(c, start)        # find first of this delim\n",
    "        if wdelim >= 0:                    # if found\n",
    "            delim = min(delim, wdelim)     # use earliest delim position\n",
    "    return url[start:delim], url[delim:]   # return (domain, rest)\n",
    "\n",
    "def _checknetloc(netloc):\n",
    "    if not netloc or not any(ord(c) > 127 for c in netloc):\n",
    "        return\n",
    "    # looking for characters like \\u2100 that expand to 'a/c'\n",
    "    # IDNA uses NFKC equivalence, so normalize for this check\n",
    "    import unicodedata\n",
    "    n = netloc.rpartition('@')[2] # ignore anything to the left of '@'\n",
    "    n = n.replace(':', '')        # ignore characters already included\n",
    "    n = n.replace('#', '')        # but not the surrounding text\n",
    "    n = n.replace('?', '')\n",
    "    netloc2 = unicodedata.normalize('NFKC', n)\n",
    "    if n == netloc2:\n",
    "        return\n",
    "    for c in '/?#@:':\n",
    "        if c in netloc2:\n",
    "            raise ValueError(\"netloc '\" + netloc + \"' contains invalid \" +\n",
    "                             \"characters under NFKC normalization\")\n",
    "\n",
    "def urlsplit(url, scheme='', allow_fragments=True):\n",
    "    \"\"\"Parse a URL into 5 components:\n",
    "    <scheme>://<netloc>/<path>?<query>#<fragment>\n",
    "    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n",
    "    Note that we don't break the components up in smaller bits\n",
    "    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n",
    "    url, scheme, _coerce_result = _coerce_args(url, scheme)\n",
    "    allow_fragments = bool(allow_fragments)\n",
    "    key = url, scheme, allow_fragments, type(url), type(scheme)\n",
    "    cached = _parse_cache.get(key, None)\n",
    "    if cached:\n",
    "        return _coerce_result(cached)\n",
    "    if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth\n",
    "        clear_cache()\n",
    "    netloc = query = fragment = ''\n",
    "    # FIXME: changed for tracking\n",
    "    i = -1\n",
    "    for j, c_ in enumerate(url):\n",
    "        if ':' == c_:\n",
    "            i = j\n",
    "            break\n",
    "    #i = url.find(':')\n",
    "    if i > 0:\n",
    "        if url[:i] == 'http': # optimize the common case\n",
    "            scheme = url[:i].lower()\n",
    "            url = url[i+1:]\n",
    "            if url[:2] == '//':\n",
    "                netloc, url = _splitnetloc(url, 2)\n",
    "                if (('[' in netloc and not ']' in netloc) or\n",
    "                        (']' in netloc and not '[' in netloc)):\n",
    "                    raise ValueError(\"Invalid IPv6 URL\")\n",
    "            if allow_fragments and '#' in url:\n",
    "                url, fragment = url.split('#', 1)\n",
    "            if '?' in url:\n",
    "                url, query = url.split('?', 1)\n",
    "            _checknetloc(netloc)\n",
    "            v = SplitResult(scheme, netloc, url, query, fragment)\n",
    "            _parse_cache[key] = v\n",
    "            return _coerce_result(v)\n",
    "        valid_scheme = True\n",
    "        for c in url[:i]:\n",
    "            if not c in scheme_chars:\n",
    "                valid_scheme = False\n",
    "                break\n",
    "        if valid_scheme:\n",
    "            # make sure \"url\" is not actually a port number (in which case\n",
    "            # \"scheme\" is really part of the path)\n",
    "            rest = url[i+1:]\n",
    "            if not rest or any(not c in '0123456789' for c in rest):\n",
    "                # not a port number\n",
    "                scheme, url = url[:i].lower(), rest\n",
    "\n",
    "    if url[:2] == '//':\n",
    "        netloc, url = _splitnetloc(url, 2)\n",
    "        if (('[' in netloc and not ']' in netloc) or\n",
    "                (']' in netloc and not '[' in netloc)):\n",
    "            raise ValueError(\"Invalid IPv6 URL\")\n",
    "    if allow_fragments and '#' in url:\n",
    "        url, fragment = url.split('#', 1)\n",
    "    if '?' in url:\n",
    "        url, query = url.split('?', 1)\n",
    "    _checknetloc(netloc)\n",
    "    v = SplitResult(scheme, netloc, url, query, fragment)\n",
    "    _parse_cache[key] = v\n",
    "    return _coerce_result(v)\n",
    "\n",
    "def urlunparse(components):\n",
    "    \"\"\"Put a parsed URL back together again.  This may result in a\n",
    "    slightly different, but equivalent URL, if the URL that was parsed\n",
    "    originally had redundant delimiters, e.g. a ? with an empty query\n",
    "    (the draft states that these are equivalent).\"\"\"\n",
    "    scheme, netloc, url, params, query, fragment, _coerce_result = (\n",
    "                                                  _coerce_args(*components))\n",
    "    if params:\n",
    "        url = \"%s;%s\" % (url, params)\n",
    "    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))\n",
    "\n",
    "def urlunsplit(components):\n",
    "    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n",
    "    complete URL as a string. The data argument can be any five-item iterable.\n",
    "    This may result in a slightly different, but equivalent URL, if the URL that\n",
    "    was parsed originally had unnecessary delimiters (for example, a ? with an\n",
    "    empty query; the RFC states that these are equivalent).\"\"\"\n",
    "    scheme, netloc, url, query, fragment, _coerce_result = (\n",
    "                                          _coerce_args(*components))\n",
    "    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n",
    "        if url and url[:1] != '/': url = '/' + url\n",
    "        url = '//' + (netloc or '') + url\n",
    "    if scheme:\n",
    "        url = scheme + ':' + url\n",
    "    if query:\n",
    "        url = url + '?' + query\n",
    "    if fragment:\n",
    "        url = url + '#' + fragment\n",
    "    return _coerce_result(url)\n",
    "\n",
    "def urljoin(base, url, allow_fragments=True):\n",
    "    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n",
    "    interpretation of the latter.\"\"\"\n",
    "    if not base:\n",
    "        return url\n",
    "    if not url:\n",
    "        return base\n",
    "\n",
    "    base, url, _coerce_result = _coerce_args(base, url)\n",
    "    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n",
    "            urlparse(base, '', allow_fragments)\n",
    "    scheme, netloc, path, params, query, fragment = \\\n",
    "            urlparse(url, bscheme, allow_fragments)\n",
    "\n",
    "    if scheme != bscheme or not scheme in uses_relative:\n",
    "        return _coerce_result(url)\n",
    "    if scheme in uses_netloc:\n",
    "        if netloc:\n",
    "            return _coerce_result(urlunparse((scheme, netloc, path,\n",
    "                                              params, query, fragment)))\n",
    "        netloc = bnetloc\n",
    "\n",
    "    if not path and not params:\n",
    "        path = bpath\n",
    "        params = bparams\n",
    "        if not query:\n",
    "            query = bquery\n",
    "        return _coerce_result(urlunparse((scheme, netloc, path,\n",
    "                                          params, query, fragment)))\n",
    "\n",
    "    base_parts = bpath.split('/')\n",
    "    if base_parts[-1] != '':\n",
    "        # the last item is not a directory, so will not be taken into account\n",
    "        # in resolving the relative path\n",
    "        del base_parts[-1]\n",
    "\n",
    "    # for rfc3986, ignore all base path should the first character be root.\n",
    "    if path[:1] == '/':\n",
    "        segments = path.split('/')\n",
    "    else:\n",
    "        segments = base_parts + path.split('/')\n",
    "        # filter out elements that would cause redundant slashes on re-joining\n",
    "        # the resolved_path\n",
    "        segments[1:-1] = filter(None, segments[1:-1])\n",
    "\n",
    "    resolved_path = []\n",
    "\n",
    "    for seg in segments:\n",
    "        if seg == '..':\n",
    "            try:\n",
    "                resolved_path.pop()\n",
    "            except IndexError:\n",
    "                # ignore any .. segments that would otherwise cause an IndexError\n",
    "                # when popped from resolved_path if resolving for rfc3986\n",
    "                pass\n",
    "        elif seg == '.':\n",
    "            continue\n",
    "        else:\n",
    "            resolved_path.append(seg)\n",
    "\n",
    "    if segments[-1] in ('.', '..'):\n",
    "        # do some post-processing here. if the last segment was a relative dir,\n",
    "        # then we need to append the trailing '/'\n",
    "        resolved_path.append('')\n",
    "\n",
    "    return _coerce_result(urlunparse((scheme, netloc, '/'.join(\n",
    "        resolved_path) or '/', params, query, fragment)))\n",
    "\n",
    "\n",
    "def urldefrag(url):\n",
    "    \"\"\"Removes any existing fragment from URL.\n",
    "\n",
    "    Returns a tuple of the defragmented URL and the fragment.  If\n",
    "    the URL contained no fragments, the second element is the\n",
    "    empty string.\n",
    "    \"\"\"\n",
    "    url, _coerce_result = _coerce_args(url)\n",
    "    if '#' in url:\n",
    "        s, n, p, a, q, frag = urlparse(url)\n",
    "        defrag = urlunparse((s, n, p, a, q, ''))\n",
    "    else:\n",
    "        frag = ''\n",
    "        defrag = url\n",
    "    return _coerce_result(DefragResult(defrag, frag))\n",
    "\n",
    "_hexdig = '0123456789ABCDEFabcdef'\n",
    "_hextobyte = None\n",
    "\n",
    "def unquote_to_bytes(string):\n",
    "    \"\"\"unquote_to_bytes('abc%20def') -> b'abc def'.\"\"\"\n",
    "    # Note: strings are encoded as UTF-8. This is only an issue if it contains\n",
    "    # unescaped non-ASCII characters, which URIs should not.\n",
    "    if not string:\n",
    "        # Is it a string-like object?\n",
    "        string.split\n",
    "        return b''\n",
    "    if isinstance(string, str):\n",
    "        string = string.encode('utf-8')\n",
    "    bits = string.split(b'%')\n",
    "    if len(bits) == 1:\n",
    "        return string\n",
    "    res = [bits[0]]\n",
    "    append = res.append\n",
    "    # Delay the initialization of the table to not waste memory\n",
    "    # if the function is never called\n",
    "    global _hextobyte\n",
    "    if _hextobyte is None:\n",
    "        _hextobyte = {(a + b).encode(): bytes([int(a + b, 16)])\n",
    "                      for a in _hexdig for b in _hexdig}\n",
    "    for item in bits[1:]:\n",
    "        try:\n",
    "            append(_hextobyte[item[:2]])\n",
    "            append(item[2:])\n",
    "        except KeyError:\n",
    "            append(b'%')\n",
    "            append(item)\n",
    "    return b''.join(res)\n",
    "\n",
    "_asciire = re.compile('([\\x00-\\x7f]+)')\n",
    "\n",
    "def unquote(string, encoding='utf-8', errors='replace'):\n",
    "    \"\"\"Replace %xx escapes by their single-character equivalent. The optional\n",
    "    encoding and errors parameters specify how to decode percent-encoded\n",
    "    sequences into Unicode characters, as accepted by the bytes.decode()\n",
    "    method.\n",
    "    By default, percent-encoded sequences are decoded with UTF-8, and invalid\n",
    "    sequences are replaced by a placeholder character.\n",
    "\n",
    "    unquote('abc%20def') -> 'abc def'.\n",
    "    \"\"\"\n",
    "    if not '%' in string:\n",
    "        string.split\n",
    "        return string\n",
    "    if encoding is None:\n",
    "        encoding = 'utf-8'\n",
    "    if errors is None:\n",
    "        errors = 'replace'\n",
    "    bits = _asciire.split(string)\n",
    "    res = [bits[0]]\n",
    "    append = res.append\n",
    "    for i in range(1, len(bits), 2):\n",
    "        append(unquote_to_bytes(bits[i]).decode(encoding, errors))\n",
    "        append(bits[i + 1])\n",
    "    return ''.join(res)\n",
    "\n",
    "\n",
    "def parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n",
    "             encoding='utf-8', errors='replace', max_num_fields=None):\n",
    "    \"\"\"Parse a query given as a string argument.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        qs: percent-encoded query string to be parsed\n",
    "\n",
    "        keep_blank_values: flag indicating whether blank values in\n",
    "            percent-encoded queries should be treated as blank strings.\n",
    "            A true value indicates that blanks should be retained as\n",
    "            blank strings.  The default false value indicates that\n",
    "            blank values are to be ignored and treated as if they were\n",
    "            not included.\n",
    "\n",
    "        strict_parsing: flag indicating what to do with parsing errors.\n",
    "            If false (the default), errors are silently ignored.\n",
    "            If true, errors raise a ValueError exception.\n",
    "\n",
    "        encoding and errors: specify how to decode percent-encoded sequences\n",
    "            into Unicode characters, as accepted by the bytes.decode() method.\n",
    "\n",
    "        max_num_fields: int. If set, then throws a ValueError if there\n",
    "            are more than n fields read by parse_qsl().\n",
    "\n",
    "        Returns a dictionary.\n",
    "    \"\"\"\n",
    "    parsed_result = {}\n",
    "    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,\n",
    "                      encoding=encoding, errors=errors,\n",
    "                      max_num_fields=max_num_fields)\n",
    "    for name, value in pairs:\n",
    "        if name in parsed_result:\n",
    "            parsed_result[name].append(value)\n",
    "        else:\n",
    "            parsed_result[name] = [value]\n",
    "    return parsed_result\n",
    "\n",
    "\n",
    "def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n",
    "              encoding='utf-8', errors='replace', max_num_fields=None):\n",
    "    \"\"\"Parse a query given as a string argument.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        qs: percent-encoded query string to be parsed\n",
    "\n",
    "        keep_blank_values: flag indicating whether blank values in\n",
    "            percent-encoded queries should be treated as blank strings.\n",
    "            A true value indicates that blanks should be retained as blank\n",
    "            strings.  The default false value indicates that blank values\n",
    "            are to be ignored and treated as if they were  not included.\n",
    "\n",
    "        strict_parsing: flag indicating what to do with parsing errors. If\n",
    "            false (the default), errors are silently ignored. If true,\n",
    "            errors raise a ValueError exception.\n",
    "\n",
    "        encoding and errors: specify how to decode percent-encoded sequences\n",
    "            into Unicode characters, as accepted by the bytes.decode() method.\n",
    "\n",
    "        max_num_fields: int. If set, then throws a ValueError\n",
    "            if there are more than n fields read by parse_qsl().\n",
    "\n",
    "        Returns a list, as G-d intended.\n",
    "    \"\"\"\n",
    "    qs, _coerce_result = _coerce_args(qs)\n",
    "\n",
    "    # If max_num_fields is defined then check that the number of fields\n",
    "    # is less than max_num_fields. This prevents a memory exhaustion DOS\n",
    "    # attack via post bodies with many fields.\n",
    "    if max_num_fields is not None:\n",
    "        num_fields = 1 + qs.count('&') + qs.count(';')\n",
    "        if max_num_fields < num_fields:\n",
    "            raise ValueError('Max number of fields exceeded')\n",
    "\n",
    "    pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n",
    "    r = []\n",
    "    for name_value in pairs:\n",
    "        if not name_value and not strict_parsing:\n",
    "            continue\n",
    "        nv = name_value.split('=', 1)\n",
    "        if len(nv) != 2:\n",
    "            if strict_parsing:\n",
    "                raise ValueError(\"bad query field: %r\" % (name_value,))\n",
    "            # Handle case of a control-name with no equal sign\n",
    "            if keep_blank_values:\n",
    "                nv.append('')\n",
    "            else:\n",
    "                continue\n",
    "        if len(nv[1]) or keep_blank_values:\n",
    "            name = nv[0].replace('+', ' ')\n",
    "            name = unquote(name, encoding=encoding, errors=errors)\n",
    "            name = _coerce_result(name)\n",
    "            value = nv[1].replace('+', ' ')\n",
    "            value = unquote(value, encoding=encoding, errors=errors)\n",
    "            value = _coerce_result(value)\n",
    "            r.append((name, value))\n",
    "    return r\n",
    "\n",
    "def unquote_plus(string, encoding='utf-8', errors='replace'):\n",
    "    \"\"\"Like unquote(), but also replace plus signs by spaces, as required for\n",
    "    unquoting HTML form values.\n",
    "\n",
    "    unquote_plus('%7e/abc+def') -> '~/abc def'\n",
    "    \"\"\"\n",
    "    string = string.replace('+', ' ')\n",
    "    return unquote(string, encoding, errors)\n",
    "\n",
    "_ALWAYS_SAFE = frozenset(b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "                         b'abcdefghijklmnopqrstuvwxyz'\n",
    "                         b'0123456789'\n",
    "                         b'_.-')\n",
    "_ALWAYS_SAFE_BYTES = bytes(_ALWAYS_SAFE)\n",
    "_safe_quoters = {}\n",
    "\n",
    "class Quoter(collections.defaultdict):\n",
    "    \"\"\"A mapping from bytes (in range(0,256)) to strings.\n",
    "\n",
    "    String values are percent-encoded byte values, unless the key < 128, and\n",
    "    in the \"safe\" set (either the specified safe set, or default set).\n",
    "    \"\"\"\n",
    "    # Keeps a cache internally, using defaultdict, for efficiency (lookups\n",
    "    # of cached keys don't call Python code at all).\n",
    "    def __init__(self, safe):\n",
    "        \"\"\"safe: bytes object.\"\"\"\n",
    "        self.safe = _ALWAYS_SAFE.union(safe)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Without this, will just display as a defaultdict\n",
    "        return \"<%s %r>\" % (self.__class__.__name__, dict(self))\n",
    "\n",
    "    def __missing__(self, b):\n",
    "        # Handle a cache miss. Store quoted string in cache and return.\n",
    "        res = chr(b) if b in self.safe else '%{:02X}'.format(b)\n",
    "        self[b] = res\n",
    "        return res\n",
    "\n",
    "def quote(string, safe='/', encoding=None, errors=None):\n",
    "    \"\"\"quote('abc def') -> 'abc%20def'\n",
    "\n",
    "    Each part of a URL, e.g. the path info, the query, etc., has a\n",
    "    different set of reserved characters that must be quoted.\n",
    "\n",
    "    RFC 2396 Uniform Resource Identifiers (URI): Generic Syntax lists\n",
    "    the following reserved characters.\n",
    "\n",
    "    reserved    = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\" | \"+\" |\n",
    "                  \"$\" | \",\"\n",
    "\n",
    "    Each of these characters is reserved in some component of a URL,\n",
    "    but not necessarily in all of them.\n",
    "\n",
    "    By default, the quote function is intended for quoting the path\n",
    "    section of a URL.  Thus, it will not encode '/'.  This character\n",
    "    is reserved, but in typical usage the quote function is being\n",
    "    called on a path where the existing slash characters are used as\n",
    "    reserved characters.\n",
    "\n",
    "    string and safe may be either str or bytes objects. encoding and errors\n",
    "    must not be specified if string is a bytes object.\n",
    "\n",
    "    The optional encoding and errors parameters specify how to deal with\n",
    "    non-ASCII characters, as accepted by the str.encode method.\n",
    "    By default, encoding='utf-8' (characters are encoded with UTF-8), and\n",
    "    errors='strict' (unsupported characters raise a UnicodeEncodeError).\n",
    "    \"\"\"\n",
    "    if isinstance(string, str):\n",
    "        if not string:\n",
    "            return string\n",
    "        if encoding is None:\n",
    "            encoding = 'utf-8'\n",
    "        if errors is None:\n",
    "            errors = 'strict'\n",
    "        string = string.encode(encoding, errors)\n",
    "    else:\n",
    "        if encoding is not None:\n",
    "            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\n",
    "        if errors is not None:\n",
    "            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\n",
    "    return quote_from_bytes(string, safe)\n",
    "\n",
    "def quote_plus(string, safe='', encoding=None, errors=None):\n",
    "    \"\"\"Like quote(), but also replace ' ' with '+', as required for quoting\n",
    "    HTML form values. Plus signs in the original string are escaped unless\n",
    "    they are included in safe. It also does not have safe default to '/'.\n",
    "    \"\"\"\n",
    "    # Check if ' ' in string, where string may either be a str or bytes.  If\n",
    "    # there are no spaces, the regular quote will produce the right answer.\n",
    "    if ((isinstance(string, str) and not ' ' in string) or\n",
    "        (isinstance(string, bytes) and not b' ' in string)):\n",
    "        return quote(string, safe, encoding, errors)\n",
    "    if isinstance(safe, str):\n",
    "        space = ' '\n",
    "    else:\n",
    "        space = b' '\n",
    "    string = quote(string, safe + space, encoding, errors)\n",
    "    return string.replace(' ', '+')\n",
    "\n",
    "def quote_from_bytes(bs, safe='/'):\n",
    "    \"\"\"Like quote(), but accepts a bytes object rather than a str, and does\n",
    "    not perform string-to-bytes encoding.  It always returns an ASCII string.\n",
    "    quote_from_bytes(b'abc def\\x3f') -> 'abc%20def%3f'\n",
    "    \"\"\"\n",
    "    if not isinstance(bs, (bytes, bytearray)):\n",
    "        raise TypeError(\"quote_from_bytes() expected bytes\")\n",
    "    if not bs:\n",
    "        return ''\n",
    "    if isinstance(safe, str):\n",
    "        # Normalize 'safe' by converting to bytes and removing non-ASCII chars\n",
    "        safe = safe.encode('ascii', 'ignore')\n",
    "    else:\n",
    "        safe = bytes([c for c in safe if c < 128])\n",
    "    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\n",
    "        return bs.decode()\n",
    "    try:\n",
    "        quoter = _safe_quoters[safe]\n",
    "    except KeyError:\n",
    "        _safe_quoters[safe] = quoter = Quoter(safe).__getitem__\n",
    "    return ''.join([quoter(char) for char in bs])\n",
    "\n",
    "def urlencode(query, doseq=False, safe='', encoding=None, errors=None,\n",
    "              quote_via=quote_plus):\n",
    "    \"\"\"Encode a dict or sequence of two-element tuples into a URL query string.\n",
    "\n",
    "    If any values in the query arg are sequences and doseq is true, each\n",
    "    sequence element is converted to a separate parameter.\n",
    "\n",
    "    If the query arg is a sequence of two-element tuples, the order of the\n",
    "    parameters in the output will match the order of parameters in the\n",
    "    input.\n",
    "\n",
    "    The components of a query arg may each be either a string or a bytes type.\n",
    "\n",
    "    The safe, encoding, and errors parameters are passed down to the function\n",
    "    specified by quote_via (encoding and errors only if a component is a str).\n",
    "    \"\"\"\n",
    "\n",
    "    if hasattr(query, \"items\"):\n",
    "        query = query.items()\n",
    "    else:\n",
    "        # It's a bother at times that strings and string-like objects are\n",
    "        # sequences.\n",
    "        try:\n",
    "            # non-sequence items should not work with len()\n",
    "            # non-empty strings will fail this\n",
    "            if len(query) and not isinstance(query[0], tuple):\n",
    "                raise TypeError\n",
    "            # Zero-length sequences of all types will get here and succeed,\n",
    "            # but that's a minor nit.  Since the original implementation\n",
    "            # allowed empty dicts that type of behavior probably should be\n",
    "            # preserved for consistency\n",
    "        except TypeError:\n",
    "            ty, va, tb = sys.exc_info()\n",
    "            raise TypeError(\"not a valid non-string sequence \"\n",
    "                            \"or mapping object\").with_traceback(tb)\n",
    "\n",
    "    l = []\n",
    "    if not doseq:\n",
    "        for k, v in query:\n",
    "            if isinstance(k, bytes):\n",
    "                k = quote_via(k, safe)\n",
    "            else:\n",
    "                k = quote_via(str(k), safe, encoding, errors)\n",
    "\n",
    "            if isinstance(v, bytes):\n",
    "                v = quote_via(v, safe)\n",
    "            else:\n",
    "                v = quote_via(str(v), safe, encoding, errors)\n",
    "            l.append(k + '=' + v)\n",
    "    else:\n",
    "        for k, v in query:\n",
    "            if isinstance(k, bytes):\n",
    "                k = quote_via(k, safe)\n",
    "            else:\n",
    "                k = quote_via(str(k), safe, encoding, errors)\n",
    "\n",
    "            if isinstance(v, bytes):\n",
    "                v = quote_via(v, safe)\n",
    "                l.append(k + '=' + v)\n",
    "            elif isinstance(v, str):\n",
    "                v = quote_via(v, safe, encoding, errors)\n",
    "                l.append(k + '=' + v)\n",
    "            else:\n",
    "                try:\n",
    "                    # Is this a sufficient test for sequence-ness?\n",
    "                    x = len(v)\n",
    "                except TypeError:\n",
    "                    # not a sequence\n",
    "                    v = quote_via(str(v), safe, encoding, errors)\n",
    "                    l.append(k + '=' + v)\n",
    "                else:\n",
    "                    # loop over the sequence\n",
    "                    for elt in v:\n",
    "                        if isinstance(elt, bytes):\n",
    "                            elt = quote_via(elt, safe)\n",
    "                        else:\n",
    "                            elt = quote_via(str(elt), safe, encoding, errors)\n",
    "                        l.append(k + '=' + elt)\n",
    "    return '&'.join(l)\n",
    "\n",
    "def to_bytes(url):\n",
    "    \"\"\"to_bytes(u\"URL\") --> 'URL'.\"\"\"\n",
    "    # Most URL schemes require ASCII. If that changes, the conversion\n",
    "    # can be relaxed.\n",
    "    # XXX get rid of to_bytes()\n",
    "    if isinstance(url, str):\n",
    "        try:\n",
    "            url = url.encode(\"ASCII\").decode()\n",
    "        except UnicodeError:\n",
    "            raise UnicodeError(\"URL \" + repr(url) +\n",
    "                               \" contains non-ASCII characters\")\n",
    "    return url\n",
    "\n",
    "def unwrap(url):\n",
    "    \"\"\"unwrap('<URL:type://host/path>') --> 'type://host/path'.\"\"\"\n",
    "    url = str(url).strip()\n",
    "    if url[:1] == '<' and url[-1:] == '>':\n",
    "        url = url[1:-1].strip()\n",
    "    if url[:4] == 'URL:': url = url[4:].strip()\n",
    "    return url\n",
    "\n",
    "_typeprog = None\n",
    "def splittype(url):\n",
    "    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n",
    "    global _typeprog\n",
    "    if _typeprog is None:\n",
    "        _typeprog = re.compile('([^/:]+):(.*)', re.DOTALL)\n",
    "\n",
    "    match = _typeprog.match(url)\n",
    "    if match:\n",
    "        scheme, data = match.groups()\n",
    "        return scheme.lower(), data\n",
    "    return None, url\n",
    "\n",
    "_hostprog = None\n",
    "def splithost(url):\n",
    "    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n",
    "    global _hostprog\n",
    "    if _hostprog is None:\n",
    "        _hostprog = re.compile('//([^/#?]*)(.*)', re.DOTALL)\n",
    "\n",
    "    match = _hostprog.match(url)\n",
    "    if match:\n",
    "        host_port, path = match.groups()\n",
    "        if path and path[0] != '/':\n",
    "            path = '/' + path\n",
    "        return host_port, path\n",
    "    return None, url\n",
    "\n",
    "def splituser(host):\n",
    "    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n",
    "    user, delim, host = host.rpartition('@')\n",
    "    return (user if delim else None), host\n",
    "\n",
    "def splitpasswd(user):\n",
    "    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n",
    "    user, delim, passwd = user.partition(':')\n",
    "    return user, (passwd if delim else None)\n",
    "\n",
    "# splittag('/path#tag') --> '/path', 'tag'\n",
    "_portprog = None\n",
    "def splitport(host):\n",
    "    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n",
    "    global _portprog\n",
    "    if _portprog is None:\n",
    "        _portprog = re.compile('(.*):([0-9]*)$', re.DOTALL)\n",
    "\n",
    "    match = _portprog.match(host)\n",
    "    if match:\n",
    "        host, port = match.groups()\n",
    "        if port:\n",
    "            return host, port\n",
    "    return host, None\n",
    "\n",
    "def splitnport(host, defport=-1):\n",
    "    \"\"\"Split host and port, returning numeric port.\n",
    "    Return given default port if no ':' found; defaults to -1.\n",
    "    Return numerical port if a valid number are found after ':'.\n",
    "    Return None if ':' but not a valid number.\"\"\"\n",
    "    host, delim, port = host.rpartition(':')\n",
    "    if not delim:\n",
    "        host = port\n",
    "    elif port:\n",
    "        try:\n",
    "            nport = int(port)\n",
    "        except ValueError:\n",
    "            nport = None\n",
    "        return host, nport\n",
    "    return host, defport\n",
    "\n",
    "def splitquery(url):\n",
    "    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n",
    "    path, delim, query = url.rpartition('?')\n",
    "    if delim:\n",
    "        return path, query\n",
    "    return url, None\n",
    "\n",
    "def splittag(url):\n",
    "    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n",
    "    path, delim, tag = url.rpartition('#')\n",
    "    if delim:\n",
    "        return path, tag\n",
    "    return url, None\n",
    "\n",
    "def splitattr(url):\n",
    "    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n",
    "        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n",
    "    words = url.split(';')\n",
    "    return words[0], words[1:]\n",
    "\n",
    "def splitvalue(attr):\n",
    "    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n",
    "    attr, delim, value = attr.partition('=')\n",
    "    return attr, (value if delim else None)\n",
    "\n",
    "def main(arg):\n",
    "    clear_cache()\n",
    "    scheme, netloc, path, params, query, fragment = urlparse(arg)\n",
    "    parse_qs(query)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGIDecode.py\n",
    "\n",
    "The CGIDecode is a program to decode a URL encoded string. The source for this program was obtained from [here](https://www.fuzzingbook.org/html/Coverage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.427507Z",
     "start_time": "2019-11-10T17:56:50.420275Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%var cgidecode_src\n",
    "# [(\n",
    "def cgi_decode(s):\n",
    "    \"\"\"Decode the CGI-encoded string `s`:\n",
    "       * replace \"+\" by \" \"\n",
    "       * replace \"%xx\" by the character with hex number xx.\n",
    "       Return the decoded string.  Raise `ValueError` for invalid inputs.\"\"\"\n",
    "\n",
    "    # Mapping of hex digits to their integer values\n",
    "    hex_values = {\n",
    "        '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
    "        '5': 5, '6': 6, '7': 7, '8': 8, '9': 9,\n",
    "        'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15,\n",
    "        'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15,\n",
    "    }\n",
    "\n",
    "    t = \"\"\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        c = s[i]\n",
    "        if c == '+':\n",
    "            t += ' '\n",
    "        elif c == '%':\n",
    "            digit_high, digit_low = s[i + 1], s[i + 2]\n",
    "            i += 2\n",
    "            if digit_high in hex_values and digit_low in hex_values:\n",
    "                v = hex_values[digit_high] * 16 + hex_values[digit_low]\n",
    "                t += chr(v)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid encoding\")\n",
    "        else:\n",
    "            t += c\n",
    "        i += 1\n",
    "    return t\n",
    "def main(arg):\n",
    "    cgi_decode(arg)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Registry\n",
    "\n",
    "We store all our subject programs under `program_src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.437004Z",
     "start_time": "2019-11-10T17:56:50.430878Z"
    },
    "code_folding": [],
    "tags": [
     "#subject_registry",
     "=>calc_src",
     "=>mathexpr_src",
     "=>microjson_src",
     "=>urlparse_src",
     "=>netrc_src",
     "=>verifyversion"
    ]
   },
   "outputs": [],
   "source": [
    "# [(\n",
    "program_src = {\n",
    "    'calculator.py': VARS['calc_src'],\n",
    "    'mathexpr.py': VARS['mathexpr_src'],\n",
    "    'urlparse.py': VARS['urlparse_src'],\n",
    "    'cgidecode.py': VARS['cgidecode_src'],\n",
    "    'microjson.py': VARS['microjson_src']\n",
    "}\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting the source to track control flow and taints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite the source so that `asring in value` gets converted to `taint_wrap__(astring).in_(value)`. Note that what we are tracking is not really taints, but rather _character accesses_ to the origin string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also rewrite the methods so that method bodies are enclosed in a `method__` context manager, any `if`conditions and `while` loops (only `while` for now) are enclosed in an outer `stack__` and inner `scope__` context manager. This lets us track when the corresponding scopes are entered into and left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.451999Z",
     "start_time": "2019-11-10T17:56:50.439943Z"
    },
    "tags": [
     "#import_ast_astor"
    ]
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import astor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InRewriter\n",
    "The `InRewriter` class handles transforming `in` statements so that taints can be tracked. It has two methods. The `wrap()` method transforms any `a in lst` calls to `taint_wrap__(a) in lst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.462509Z",
     "start_time": "2019-11-10T17:56:50.454304Z"
    },
    "tags": [
     "=>import_ast_astor",
     "#InRewriter"
    ]
   },
   "outputs": [],
   "source": [
    "class InRewriter(ast.NodeTransformer):\n",
    "    def wrap(self, node):\n",
    "        return ast.Call(func=ast.Name(id='taint_wrap__', ctx=ast.Load()), args=[node], keywords=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wrap()` method is internally used by `visit_Compare()` method to transform `a in lst` to `taint_wrap__(a).in_(lst)`. We need to do this because Python ties the overriding of `in` operator to the `__contains__()` method in the class of `lst`. In our case, however, very often `a` is the element tainted and hence proxied. Hence we need a method invoked on the `a` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.481613Z",
     "start_time": "2019-11-10T17:56:50.469163Z"
    },
    "tags": [
     "=>InRewriter",
     "#InRewriter_visit_Compare"
    ]
   },
   "outputs": [],
   "source": [
    "class InRewriter(InRewriter):\n",
    "    def visit_Compare(self, tree_node):\n",
    "        left = tree_node.left\n",
    "        if not tree_node.ops or not isinstance(tree_node.ops[0], ast.In):\n",
    "            return tree_node\n",
    "        mod_val = ast.Call(\n",
    "            func=ast.Attribute(\n",
    "                value=self.wrap(left),\n",
    "                attr='in_'),\n",
    "            args=tree_node.comparators,\n",
    "            keywords=[])\n",
    "        return mod_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.493326Z",
     "start_time": "2019-11-10T17:56:50.483668Z"
    },
    "tags": [
     "=>InRewriter_visit_Compare",
     "#rewrite_in"
    ]
   },
   "outputs": [],
   "source": [
    "def rewrite_in(src):\n",
    "    v = ast.fix_missing_locations(InRewriter().visit(ast.parse(src)))\n",
    "    source = astor.to_source(v)\n",
    "    return \"%s\" % source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Using It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.645502Z",
     "start_time": "2019-11-10T17:56:50.495302Z"
    },
    "tags": [
     "#import_print_content"
    ]
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.fuzzingbook_utils import print_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.977961Z",
     "start_time": "2019-11-10T17:56:50.659278Z"
    },
    "tags": [
     "=>rewrite_in"
    ]
   },
   "outputs": [],
   "source": [
    "%top print_content(rewrite_in('s in [\"a\", \"b\", \"c\"]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriter\n",
    "\n",
    "The `Rewriter` class handles inserting tracing probes into methods and control structures. Essentially, we insert a `with` scope for the method body, and a `with` scope outside both `while` and `if` scopes. Finally, we insert a `with` scope inside the `while` and `if` scopes. IMPORTANT: We only implement the `while` context. Similar should be implemented for the `for` context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The method context wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few counters to provide unique identifiers for context managers. Essentially, we number each if and while that we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.991377Z",
     "start_time": "2019-11-10T17:56:50.985591Z"
    },
    "tags": [
     "#main_globals"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(InRewriter):\n",
    "    def init_counters(self):\n",
    "        self.if_counter = 0\n",
    "        self.while_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `methods[]` array is used to keep track of the current method stack during execution. Epsilon and NoEpsilon are simply constants that I use to indicate whether an IF or a Loop is nullable or not. If it is nullable, I mark it with Epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:50.998530Z",
     "start_time": "2019-11-10T17:56:50.993783Z"
    },
    "tags": [
     "#main_globals"
    ]
   },
   "outputs": [],
   "source": [
    "methods = []\n",
    "Epsilon = '-'\n",
    "NoEpsilon = '='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wrap_in_method()` generates a wrapper for method definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.018672Z",
     "start_time": "2019-11-10T17:56:51.003103Z"
    },
    "tags": [
     "#Rewriter",
     "=>main_globals",
     "=>InRewriter_visit_Compare"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def wrap_in_method(self, body, args):\n",
    "        method_name_expr = ast.Str(methods[-1])\n",
    "        my_args = ast.List(args.args, ast.Load())\n",
    "        args = [method_name_expr, my_args]\n",
    "        scope_expr = ast.Call(func=ast.Name(id='method__', ctx=ast.Load()), args=args, keywords=[])\n",
    "        return [ast.With(items=[ast.withitem(scope_expr, ast.Name(id='_method__'))], body=body)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `visit_FunctionDef()` is the method rewriter that actually does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.028969Z",
     "start_time": "2019-11-10T17:56:51.021974Z"
    },
    "tags": [
     "#Rewriter_visit_FunctionDef",
     "=>Rewriter"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def visit_FunctionDef(self, tree_node):\n",
    "        self.init_counters()\n",
    "        methods.append(tree_node.name)\n",
    "        self.generic_visit(tree_node)\n",
    "        tree_node.body = self.wrap_in_method(tree_node.body, tree_node.args)\n",
    "        return tree_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rewrite_def()` method wraps the function definitions in scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.038559Z",
     "start_time": "2019-11-10T17:56:51.032572Z"
    },
    "tags": [
     "#rewrite_def",
     "=>Rewriter_visit_FunctionDef"
    ]
   },
   "outputs": [],
   "source": [
    "def rewrite_def(src):\n",
    "    v = ast.fix_missing_locations(Rewriter().visit(ast.parse(src)))\n",
    "    return astor.to_source(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.129885Z",
     "start_time": "2019-11-10T17:56:51.041453Z"
    }
   },
   "outputs": [],
   "source": [
    "%top print_content(rewrite_def('\\n'.join(program_src['calculator.py'].split('\\n')[12:19])), 'calculator.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The stack wrapper\n",
    "\n",
    "The method `wrap_in_outer()` adds a `with ..stack..()` context _outside_ the control structures. The stack is used to keep track of the current control structure stack for any character comparison made. Notice the `can_empty` parameter. This indicates that the particular structure is _nullable_. For `if` we can make the condition right away. For `while` we postpone the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.185126Z",
     "start_time": "2019-11-10T17:56:51.140381Z"
    },
    "tags": [
     "#Rewriter_wrap_in_outer",
     "=>Rewriter_visit_FunctionDef"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def wrap_in_outer(self, name, can_empty, counter, node):\n",
    "        name_expr = ast.Str(name)\n",
    "        can_empty_expr = ast.Str(can_empty)\n",
    "        counter_expr = ast.Num(counter)\n",
    "        method_id = ast.Name(id='_method__')\n",
    "        args = [name_expr, counter_expr, method_id, can_empty_expr]\n",
    "        scope_expr = ast.Call(func=ast.Name(id='stack__', ctx=ast.Load()),\n",
    "                args=args, keywords=[])\n",
    "        return ast.With(\n",
    "            items=[ast.withitem(scope_expr, ast.Name(id='%s_%d_stack__' % (name, counter)))], \n",
    "            body=[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The scope wrapper\n",
    "The method `wrap_in_inner()` adds a `with ...scope..()` context immediately inside the control structure. For `while`, this means simply adding one `with ...scope..()` just before the first line. For `if`, this means adding one `with ...scope...()` each to each branch of the `if` condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.197500Z",
     "start_time": "2019-11-10T17:56:51.188359Z"
    },
    "tags": [
     "=>Rewriter_wrap_in_outer",
     "#Rewriter_wrap_in_inner"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def wrap_in_inner(self, name, counter, val, body):\n",
    "        val_expr = ast.Num(val)\n",
    "        stack_iter = ast.Name(id='%s_%d_stack__' % (name, counter))\n",
    "        method_id = ast.Name(id='_method__')\n",
    "        args = [val_expr, stack_iter, method_id]\n",
    "        scope_expr = ast.Call(func=ast.Name(id='scope__', ctx=ast.Load()),\n",
    "                args=args, keywords=[])\n",
    "        return [ast.With(\n",
    "            items=[ast.withitem(scope_expr, ast.Name(id='%s_%d_%d_scope__' % (name, counter, val)))], \n",
    "            body=body)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewriting `If` conditions\n",
    "\n",
    "While rewriting if conditions, we have to take care of the cascading if conditions (`elsif`), which is represented as nested if conditions in AST. They do not require separate `stack` context, but only separate `scope` contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.212331Z",
     "start_time": "2019-11-10T17:56:51.201293Z"
    },
    "tags": [
     "#Rewriter_process_if",
     "=>Rewriter_wrap_in_inner"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def process_if(self, tree_node, counter, val=None):\n",
    "        if val is None: val = 0\n",
    "        else: val += 1\n",
    "        if_body = []\n",
    "        self.generic_visit(tree_node.test)\n",
    "        for node in tree_node.body:\n",
    "            if_body.append(self.generic_visit(ast.Module(node)).body)\n",
    "        tree_node.body = self.wrap_in_inner('if', counter, val, if_body)\n",
    "\n",
    "        # else part.\n",
    "        if len(tree_node.orelse) == 1 and isinstance(tree_node.orelse[0], ast.If):\n",
    "            self.process_if(tree_node.orelse[0], counter, val)\n",
    "        else:\n",
    "            if tree_node.orelse:\n",
    "                val += 1\n",
    "                for node in tree_node.orelse: self.generic_visit(node)\n",
    "                tree_node.orelse = self.wrap_in_inner('if', counter, val, tree_node.orelse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While rewriting `if` conditions, we have to take care of the cascading `if` conditions, which is represented as nested `if` conditions in AST. We need to identify whether the cascading `if` conditions (`elsif`) have an empty `orelse` clause or not. If it has an empty `orelse`, then the entire set of `if` conditions may be excised, and still produce a valid value. Hence, it should be marked as optional. The `visit_If()` checks if the cascading `ifs` have an `orelse` or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.226044Z",
     "start_time": "2019-11-10T17:56:51.215393Z"
    },
    "tags": [
     "=>Rewriter_process_if",
     "#Rewriter_visit_If"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def visit_If(self, tree_node):\n",
    "        self.if_counter += 1\n",
    "        counter = self.if_counter\n",
    "        #is it empty\n",
    "        start = tree_node\n",
    "        while start:\n",
    "            if isinstance(start, ast.If):\n",
    "                if not start.orelse:\n",
    "                    start = None\n",
    "                elif len(start.orelse) == 1:\n",
    "                    start = start.orelse[0]\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        self.process_if(tree_node, counter=self.if_counter)\n",
    "        can_empty = NoEpsilon if start else Epsilon  # NoEpsilon for + and Epsilon for *\n",
    "        return self.wrap_in_outer('if', can_empty, counter, tree_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewriting `while` loops\n",
    "\n",
    "Rewriting while loops are simple. We wrap them in `stack` and `scope` contexts. We do not implement the `orelse` feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.239457Z",
     "start_time": "2019-11-10T17:56:51.229274Z"
    },
    "tags": [
     "=>Rewriter_visit_If",
     "#Rewriter_visit_While"
    ]
   },
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def visit_While(self, tree_node):\n",
    "        self.generic_visit(tree_node)\n",
    "        self.while_counter += 1\n",
    "        counter = self.while_counter\n",
    "        test = tree_node.test\n",
    "        body = tree_node.body\n",
    "        assert not tree_node.orelse\n",
    "        tree_node.body = self.wrap_in_inner('while', counter, 0, body)\n",
    "        return self.wrap_in_outer('while', '?', counter, tree_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.264467Z",
     "start_time": "2019-11-10T17:56:51.258478Z"
    },
    "tags": [
     "#rewrite_cf",
     "=>Rewriter_visit_While"
    ]
   },
   "outputs": [],
   "source": [
    "def rewrite_cf(src, original):\n",
    "    v = ast.fix_missing_locations(Rewriter().visit(ast.parse(src)))\n",
    "    return astor.to_source(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example with `if` conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.283611Z",
     "start_time": "2019-11-10T17:56:51.267385Z"
    },
    "tags": [
     "=>rewrite_cf",
     "=>import_print_content"
    ]
   },
   "outputs": [],
   "source": [
    "%top print_content('\\n'.join(program_src['calculator.py'].split('\\n')[12:19]), 'calculator.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.307672Z",
     "start_time": "2019-11-10T17:56:51.287863Z"
    },
    "scrolled": true,
    "tags": [
     "=>rewrite_cf",
     "=>import_print_content"
    ]
   },
   "outputs": [],
   "source": [
    "%top print_content(rewrite_cf('\\n'.join(program_src['calculator.py'].split('\\n')[12:19]), 'calculator.py').strip(), filename='calculator.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example with `while` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.329633Z",
     "start_time": "2019-11-10T17:56:51.313380Z"
    },
    "tags": [
     "=>import_print_content"
    ]
   },
   "outputs": [],
   "source": [
    "%top print_content('\\n'.join(program_src['calculator.py'].split('\\n')[5:11]), 'calculator.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.347105Z",
     "start_time": "2019-11-10T17:56:51.333542Z"
    },
    "tags": [
     "=>import_print_content"
    ]
   },
   "outputs": [],
   "source": [
    "%top print_content(rewrite_cf('\\n'.join(program_src['calculator.py'].split('\\n')[5:11]), 'calculator.py'), filename='calculator.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the complete instrumented source\n",
    "\n",
    "For the complete instrumented source, we need to first make sure that all necessary imports are satisfied. Next, we also need to invoke the parser with the necessary tainted input and output the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.360559Z",
     "start_time": "2019-11-10T17:56:51.350916Z"
    },
    "code_folding": [],
    "tags": [
     "#rewrite",
     "=>Rewriter_visit_While"
    ]
   },
   "outputs": [],
   "source": [
    "def rewrite(src, original):\n",
    "    src = ast.fix_missing_locations(InRewriter().visit(ast.parse(src)))\n",
    "    v = ast.fix_missing_locations(Rewriter().visit(ast.parse(src)))\n",
    "    header = \"\"\"\n",
    "from mimid_context import scope__, stack__, method__\n",
    "import json\n",
    "import sys\n",
    "import taints\n",
    "from taints import taint_wrap__\n",
    "    \"\"\"\n",
    "    source = astor.to_source(v)\n",
    "    footer = \"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    js = []\n",
    "    for arg in sys.argv[1:]:\n",
    "        with open(arg) as f:\n",
    "            mystring = f.read().strip().replace('\\\\n', ' ')\n",
    "        taints.trace_init()\n",
    "        tainted_input = taints.wrap_input(mystring)\n",
    "        main(tainted_input)\n",
    "        assert tainted_input.comparisons\n",
    "        j = {\n",
    "        'comparisons_fmt': 'idx, char, method_call_id',\n",
    "        'comparisons':taints.convert_comparisons(tainted_input.comparisons, mystring),\n",
    "        'method_map_fmt': 'method_call_id, method_name, children',\n",
    "        'method_map': taints.convert_method_map(taints.METHOD_MAP),\n",
    "        'inputstr': mystring,\n",
    "        'original': %s,\n",
    "        'arg': arg}\n",
    "        js.append(j)\n",
    "    print(json.dumps(js))\n",
    "\"\"\"\n",
    "    footer = footer % repr(original)\n",
    "    return \"%s\\n%s\\n%s\" % (header, source, footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.379153Z",
     "start_time": "2019-11-10T17:56:51.363718Z"
    },
    "tags": [
     "=>rewrite",
     "=>subject_registry"
    ]
   },
   "outputs": [],
   "source": [
    "%top calc_parse_rewritten = rewrite(program_src['calculator.py'], original='calculator.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.401107Z",
     "start_time": "2019-11-10T17:56:51.382770Z"
    },
    "tags": [
     "=>rewrite",
     "=>subject_registry"
    ]
   },
   "outputs": [],
   "source": [
    "%top print_content(calc_parse_rewritten, filename='calculator.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Transformed Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write the transformed sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.454580Z",
     "start_time": "2019-11-10T17:56:51.404951Z"
    },
    "tags": [
     "#builddirs"
    ]
   },
   "outputs": [],
   "source": [
    "do(['mkdir','-p','build','subjects','samples']).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.707554Z",
     "start_time": "2019-11-10T17:56:51.479744Z"
    },
    "tags": [
     "#gen_src",
     "=>rewrite",
     "=>subject_registry",
     "=>builddirs"
    ]
   },
   "outputs": [],
   "source": [
    "# [(\n",
    "for file_name in program_src:\n",
    "    print(file_name)\n",
    "    with open(\"subjects/%s\" % file_name, 'wb+') as f:\n",
    "        f.write(program_src[file_name].encode('utf-8'))\n",
    "    with open(\"build/%s\" % file_name, 'w+') as f:\n",
    "        f.write(rewrite(program_src[file_name], file_name))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Mangers\n",
    "\n",
    "The context managers are probes inserted into the source code so that we know when execution enters and exits specific control flow structures such as conditionals and loops. Note that source code for these probes are not really a requirement. They can be inserted directly on binaries too, or even dynamically inserted using tools such as `dtrace`. For now, we make our life simple using AST editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var config_src\n",
    "# [(\n",
    "import urllib.parse\n",
    "# This is useful for parsers such as PEG where the argument\n",
    "# is important as the name of the non-terminal. For others such\n",
    "# as parser combinators, we need to find how to incorporate the\n",
    "# variable name info in the method name.\n",
    "ENCODE_ARGS = False\n",
    "def encode_method_name(name, my_args):\n",
    "    if not ENCODE_ARGS: return name\n",
    "    if not my_args: return name\n",
    "    # trick to convert args that are not of type str for later.\n",
    "    if hasattr(my_args[0], 'tag'):\n",
    "        name = \"%s:%s\" % (my_args[0].tag, name)\n",
    "    else:\n",
    "        return \"%s(%s)\" % (name, urllib.parse.quote('_'.join([str(i) for i in my_args])))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method context\n",
    "The `method__` context handles the assignment of method name, as well as storing the method stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.716369Z",
     "start_time": "2019-11-10T17:56:51.711902Z"
    },
    "code_folding": [],
    "tags": [
     "#mimid_method_context"
    ]
   },
   "outputs": [],
   "source": [
    "%%var mimid_method_context\n",
    "# [(\n",
    "import taints\n",
    "import config\n",
    "def to_key(method, name, num):\n",
    "    return '%s:%s_%s' % (method, name, num)\n",
    "\n",
    "class method__:\n",
    "    def __init__(self, name, args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.name = config.encode_method_name(name, args)\n",
    "        self.method_name = self.name\n",
    "        taints.trace_call(self.name)\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self._old_name =  taints.trace_set_method(self.name)\n",
    "        self.stack = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        taints.trace_return()\n",
    "        taints.trace_set_method(self._old_name)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stack context stores the current prefix and handles updating the stack that is stored at the method context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.731399Z",
     "start_time": "2019-11-10T17:56:51.725069Z"
    },
    "code_folding": [],
    "tags": [
     "#mimid_stack_context"
    ]
   },
   "outputs": [],
   "source": [
    "%%var mimid_stack_context\n",
    "# [(\n",
    "class stack__:\n",
    "    def __init__(self, name, num, method_i, can_empty):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.stack = method_i.stack\n",
    "        self.method_name = method_i.method_name\n",
    "        self.can_empty = can_empty # * means yes. + means no, ? means to be determined\n",
    "        self.name, self.num = name, num\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        if self.name in {'while'}:\n",
    "            self.stack.append('loop_%d' % self.num)\n",
    "        elif self.name in {'if'}:\n",
    "            self.stack.append('if_%d' % self.num)\n",
    "        else:\n",
    "            assert False\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.stack.pop()\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope context\n",
    "The scope context correctly identifies when the control structure is entered into, and exited (in case of loops) and the alternative entered into (in case of if conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.742063Z",
     "start_time": "2019-11-10T17:56:51.735503Z"
    },
    "code_folding": [],
    "tags": [
     "#mimid_scope_context"
    ]
   },
   "outputs": [],
   "source": [
    "%%var mimid_scope_context\n",
    "# [(\n",
    "import json\n",
    "\n",
    "def encode_name(method, ctrl, ctrl_id, alt_num, can_empty, stack):\n",
    "    assert ctrl in {'while', 'if'}\n",
    "    return '%s:%s_%s,%s %s#%s' % (method, ctrl, ctrl_id, alt_num, can_empty, json.dumps(stack))\n",
    "\n",
    "class scope__:\n",
    "    def __init__(self, alt, stack_i, method_i):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.name, self.num, self.method, self.alt = stack_i.name, stack_i.num, stack_i.method_name, alt\n",
    "        self.stack = stack_i.stack\n",
    "        self.method_name = stack_i.method_name\n",
    "        self.can_empty = stack_i.can_empty\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        if self.name in {'while'}:\n",
    "            # we do not have to keep track of iteration id. We\n",
    "            # instead recognize different parts of loops by compatible patterns\n",
    "            pass\n",
    "            #self.stack[-1] += 1\n",
    "        elif self.name in {'if'}:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, self.name\n",
    "        encoded_name = encode_name(self.method, self.name, self.num, self.alt, self.can_empty, self.stack)\n",
    "        taints.trace_call(encoded_name)\n",
    "        self._old_name = taints.trace_set_method(self.name)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        taints.trace_return()\n",
    "        taints.trace_set_method(self._old_name)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taint Tracker\n",
    "\n",
    "The taint tracker is essentially a reimplementation of the information flow taints from the Fuzzingbook. It incorporates tracing of character accesses. IMPORTANT: Not all methods are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.772986Z",
     "start_time": "2019-11-10T17:56:51.755702Z"
    },
    "code_folding": [],
    "tags": [
     "#mimid_taints_src"
    ]
   },
   "outputs": [],
   "source": [
    "%%var taints_src\n",
    "# [(\n",
    "import inspect\n",
    "import enum\n",
    "\n",
    "class tstr_(str):\n",
    "    def __new__(cls, value, *args, **kw):\n",
    "        return super(tstr_, cls).__new__(cls, value)\n",
    "\n",
    "class tstr(tstr_):\n",
    "    def __init__(self, value, taint=None, parent=None, **kwargs):\n",
    "        self.parent = parent\n",
    "        l = len(self)\n",
    "        if taint is None:\n",
    "            taint = 0\n",
    "        self.taint = list(range(taint, taint + l)) if isinstance(\n",
    "            taint, int) else taint\n",
    "        assert len(self.taint) == l\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return str.__str__(self)\n",
    "\n",
    "class tstr(tstr):\n",
    "    def untaint(self):\n",
    "        self.taint = [None] * len(self)\n",
    "        return self\n",
    "\n",
    "    def has_taint(self):\n",
    "        return any(True for i in self.taint if i is not None)\n",
    "\n",
    "    def taint_in(self, gsentence):\n",
    "        return set(self.taint) <= set(gsentence.taint)\n",
    "\n",
    "\n",
    "\n",
    "class tstr(tstr):\n",
    "    def create(self, res, taint):\n",
    "        return tstr(res, taint, self)\n",
    "\n",
    "\n",
    "\n",
    "class tstr(tstr):\n",
    "    def __getitem__(self, key):\n",
    "        res = super().__getitem__(key)\n",
    "        if isinstance(key, int):\n",
    "            key = len(self) + key if key < 0 else key\n",
    "            return self.create(res, [self.taint[key]])\n",
    "        elif isinstance(key, slice):\n",
    "            return self.create(res, self.taint[key])\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "class tstr(tstr):\n",
    "    def __iter__(self):\n",
    "        return tstr_iterator(self)\n",
    "\n",
    "class tstr_iterator():\n",
    "    def __init__(self, tstr):\n",
    "        self._tstr = tstr\n",
    "        self._str_idx = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._str_idx == len(self._tstr):\n",
    "            raise StopIteration\n",
    "        # calls tstr getitem should be tstr\n",
    "        c = self._tstr[self._str_idx]\n",
    "        assert isinstance(c, tstr)\n",
    "        self._str_idx += 1\n",
    "        return c\n",
    "\n",
    "class tstr(tstr):\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, tstr):\n",
    "            return self.create(str.__add__(self, other),\n",
    "                               (self.taint + other.taint))\n",
    "        else:\n",
    "            return self.create(str.__add__(self, other),\n",
    "                               (self.taint + [-1 for i in other]))\n",
    "\n",
    "class tstr(tstr):\n",
    "    def __radd__(self, other):\n",
    "        if other:\n",
    "            taint = other.taint if isinstance(other, tstr) else [\n",
    "                None for i in other]\n",
    "        else:\n",
    "            taint = []\n",
    "        return self.create(str.__add__(other, self), (taint + self.taint))\n",
    "\n",
    "class tstr(tstr):\n",
    "    class TaintException(Exception):\n",
    "        pass\n",
    "\n",
    "    def x(self, i=0):\n",
    "        if not self.taint:\n",
    "            raise tstr.TaintException('Invalid request idx')\n",
    "        if isinstance(i, int):\n",
    "            return [self[p]\n",
    "                    for p in [k for k, j in enumerate(self.taint) if j == i]]\n",
    "        elif isinstance(i, slice):\n",
    "            r = range(i.start or 0, i.stop or len(self), i.step or 1)\n",
    "            return [self[p]\n",
    "                    for p in [k for k, j in enumerate(self.taint) if j in r]]\n",
    "\n",
    "class tstr(tstr):\n",
    "    def replace(self, a, b, n=None):\n",
    "        old_taint = self.taint\n",
    "        b_taint = b.taint if isinstance(b, tstr) else [None] * len(b)\n",
    "        mystr = str(self)\n",
    "        i = 0\n",
    "        while True:\n",
    "            if n and i >= n:\n",
    "                break\n",
    "            idx = mystr.find(a)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            last = idx + len(a)\n",
    "            mystr = mystr.replace(a, b, 1)\n",
    "            partA, partB = old_taint[0:idx], old_taint[last:]\n",
    "            old_taint = partA + b_taint + partB\n",
    "            i += 1\n",
    "        return self.create(mystr, old_taint)\n",
    "\n",
    "class tstr(tstr):\n",
    "    def _split_helper(self, sep, splitted):\n",
    "        result_list = []\n",
    "        last_idx = 0\n",
    "        first_idx = 0\n",
    "        sep_len = len(sep)\n",
    "\n",
    "        for s in splitted:\n",
    "            last_idx = first_idx + len(s)\n",
    "            item = self[first_idx:last_idx]\n",
    "            result_list.append(item)\n",
    "            first_idx = last_idx + sep_len\n",
    "        return result_list\n",
    "\n",
    "    def _split_space(self, splitted):\n",
    "        result_list = []\n",
    "        last_idx = 0\n",
    "        first_idx = 0\n",
    "        sep_len = 0\n",
    "        for s in splitted:\n",
    "            last_idx = first_idx + len(s)\n",
    "            item = self[first_idx:last_idx]\n",
    "            result_list.append(item)\n",
    "            v = str(self[last_idx:])\n",
    "            sep_len = len(v) - len(v.lstrip(' '))\n",
    "            first_idx = last_idx + sep_len\n",
    "        return result_list\n",
    "\n",
    "    def rsplit(self, sep=None, maxsplit=-1):\n",
    "        splitted = super().rsplit(sep, maxsplit)\n",
    "        if not sep:\n",
    "            return self._split_space(splitted)\n",
    "        return self._split_helper(sep, splitted)\n",
    "\n",
    "    def split(self, sep=None, maxsplit=-1):\n",
    "        splitted = super().split(sep, maxsplit)\n",
    "        if not sep:\n",
    "            return self._split_space(splitted)\n",
    "        return self._split_helper(sep, splitted)\n",
    "\n",
    "class tstr(tstr):\n",
    "    def strip(self, cl=None):\n",
    "        return self.lstrip(cl).rstrip(cl)\n",
    "\n",
    "    def lstrip(self, cl=None):\n",
    "        res = super().lstrip(cl)\n",
    "        i = self.find(res)\n",
    "        return self[i:]\n",
    "\n",
    "    def rstrip(self, cl=None):\n",
    "        res = super().rstrip(cl)\n",
    "        return self[0:len(res)]\n",
    "\n",
    "\n",
    "class tstr(tstr):\n",
    "    def expandtabs(self, n=8):\n",
    "        parts = self.split('\\t')\n",
    "        res = super().expandtabs(n)\n",
    "        all_parts = []\n",
    "        for i, p in enumerate(parts):\n",
    "            all_parts.extend(p.taint)\n",
    "            if i < len(parts) - 1:\n",
    "                l = len(all_parts) % n\n",
    "                all_parts.extend([p.taint[-1]] * l)\n",
    "        return self.create(res, all_parts)\n",
    "\n",
    "class tstr(tstr):\n",
    "    def join(self, iterable):\n",
    "        mystr = ''\n",
    "        mytaint = []\n",
    "        sep_taint = self.taint\n",
    "        lst = list(iterable)\n",
    "        for i, s in enumerate(lst):\n",
    "            staint = s.taint if isinstance(s, tstr) else [None] * len(s)\n",
    "            mytaint.extend(staint)\n",
    "            mystr += str(s)\n",
    "            if i < len(lst) - 1:\n",
    "                mytaint.extend(sep_taint)\n",
    "                mystr += str(self)\n",
    "        res = super().join(iterable)\n",
    "        assert len(res) == len(mystr)\n",
    "        return self.create(res, mytaint)\n",
    "\n",
    "class tstr(tstr):\n",
    "    def partition(self, sep):\n",
    "        partA, sep, partB = super().partition(sep)\n",
    "        return (self.create(partA, self.taint[0:len(partA)]),\n",
    "                self.create(sep, self.taint[len(partA):len(partA) + len(sep)]),\n",
    "                self.create(partB, self.taint[len(partA) + len(sep):]))\n",
    "\n",
    "    def rpartition(self, sep):\n",
    "        partA, sep, partB = super().rpartition(sep)\n",
    "        return (self.create(partA, self.taint[0:len(partA)]),\n",
    "                self.create(sep, self.taint[len(partA):len(partA) + len(sep)]),\n",
    "                self.create(partB, self.taint[len(partA) + len(sep):]))\n",
    "\n",
    "class tstr(tstr):\n",
    "    def ljust(self, width, fillchar=' '):\n",
    "        res = super().ljust(width, fillchar)\n",
    "        initial = len(res) - len(self)\n",
    "        if isinstance(fillchar, tstr):\n",
    "            t = fillchar.x()\n",
    "        else:\n",
    "            t = -1\n",
    "        return self.create(res, [t] * initial + self.taint)\n",
    "\n",
    "    def rjust(self, width, fillchar=' '):\n",
    "        res = super().rjust(width, fillchar)\n",
    "        final = len(res) - len(self)\n",
    "        if isinstance(fillchar, tstr):\n",
    "            t = fillchar.x()\n",
    "        else:\n",
    "            t = -1\n",
    "        return self.create(res, self.taint + [t] * final)\n",
    "\n",
    "class tstr(tstr):\n",
    "    def swapcase(self):\n",
    "        return self.create(str(self).swapcase(), self.taint)\n",
    "\n",
    "    def upper(self):\n",
    "        return self.create(str(self).upper(), self.taint)\n",
    "\n",
    "    def lower(self):\n",
    "        return self.create(str(self).lower(), self.taint)\n",
    "\n",
    "    def capitalize(self):\n",
    "        return self.create(str(self).capitalize(), self.taint)\n",
    "\n",
    "    def title(self):\n",
    "        return self.create(str(self).title(), self.taint)\n",
    "\n",
    "def taint_include(gword, gsentence):\n",
    "    return set(gword.taint) <= set(gsentence.taint)\n",
    "\n",
    "\n",
    "def make_str_wrapper(fun):\n",
    "    def proxy(*args, **kwargs):\n",
    "        res = fun(*args, **kwargs)\n",
    "        return res\n",
    "    return proxy\n",
    "\n",
    "import types\n",
    "tstr_members = [name for name, fn in inspect.getmembers(tstr, callable)\n",
    "                if isinstance(fn, types.FunctionType) and fn.__qualname__.startswith('tstr')]\n",
    "\n",
    "for name, fn in inspect.getmembers(str, callable):\n",
    "    if name not in set(['__class__', '__new__', '__str__', '__init__',\n",
    "                        '__repr__', '__getattribute__']) | set(tstr_members):\n",
    "        setattr(tstr, name, make_str_wrapper(fn))\n",
    "\n",
    "\n",
    "def make_str_abort_wrapper(fun):\n",
    "    def proxy(*args, **kwargs):\n",
    "        raise tstr.TaintException('%s Not implemented in TSTR' % fun.__name__)\n",
    "    return proxy\n",
    "\n",
    "\n",
    "\n",
    "class eoftstr(tstr):\n",
    "    def create(self, res, taint):\n",
    "        return eoftstr(res, taint, self)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        def get_interval(key):\n",
    "            return ((0 if key.start is None else key.start),\n",
    "                    (len(res) if key.stop is None else key.stop))\n",
    "\n",
    "        res = super().__getitem__(key)\n",
    "        if isinstance(key, int):\n",
    "            key = len(self) + key if key < 0 else key\n",
    "            return self.create(res, [self.taint[key]])\n",
    "        elif isinstance(key, slice):\n",
    "            if res:\n",
    "                return self.create(res, self.taint[key])\n",
    "            # Result is an empty string\n",
    "            t = self.create(res, self.taint[key])\n",
    "            key_start, key_stop = get_interval(key)\n",
    "            cursor = 0\n",
    "            if key_start < len(self):\n",
    "                assert key_stop < len(self)\n",
    "                #cursor = self.taint[key_stop]\n",
    "            else:\n",
    "                if len(self) == 0:\n",
    "                    # if the original string was empty, we assume that any\n",
    "                    # empty string produced from it should carry the same\n",
    "                    # taint.\n",
    "                    #cursor = self.x()\n",
    "                #else:\n",
    "                    # Key start was not in the string. We can reply only\n",
    "                    # if the key start was just outside the string, in\n",
    "                    # which case, we guess.\n",
    "                    if key_start != len(self):\n",
    "                        raise tstr.TaintException('Can\\'t guess the taint')\n",
    "                    #cursor = self.taint[len(self) - 1] + 1\n",
    "            # _tcursor gets created only for empty strings.\n",
    "            t._tcursor = cursor\n",
    "            return t\n",
    "\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "class eoftstr(eoftstr):\n",
    "    def t(self, i=0):\n",
    "        if self.taint:\n",
    "            return self.taint[i]\n",
    "        else:\n",
    "            if i != 0:\n",
    "                raise tstr.TaintException('Invalid request idx')\n",
    "            # self._tcursor gets created only for empty strings.\n",
    "            # use the exception to determine which ones need it.\n",
    "            return self._tcursor\n",
    "\n",
    "class Op(enum.Enum):\n",
    "    LT = 0\n",
    "    LE = enum.auto()\n",
    "    EQ = enum.auto()\n",
    "    NE = enum.auto()\n",
    "    GT = enum.auto()\n",
    "    GE = enum.auto()\n",
    "    IN = enum.auto()\n",
    "    NOT_IN = enum.auto()\n",
    "    IS = enum.auto()\n",
    "    IS_NOT = enum.auto()\n",
    "    FIND_STR = enum.auto()\n",
    "\n",
    "COMPARE_OPERATORS = {\n",
    "    Op.EQ: lambda x, y: x == y,\n",
    "    Op.NE: lambda x, y: x != y,\n",
    "    Op.IN: lambda x, y: x in y,\n",
    "    Op.NOT_IN: lambda x, y: x not in y,\n",
    "    Op.FIND_STR: lambda x, y: x.find(y)\n",
    "}\n",
    "\n",
    "Comparisons = []\n",
    "\n",
    "# ### Instructions\n",
    "\n",
    "class Instr:\n",
    "    def __init__(self, o, a, b):\n",
    "        self.opA = a\n",
    "        self.opB = b\n",
    "        self.op = o\n",
    "\n",
    "    def o(self):\n",
    "        if self.op == Op.EQ:\n",
    "            return 'eq'\n",
    "        elif self.op == Op.NE:\n",
    "            return 'ne'\n",
    "        else:\n",
    "            return '?'\n",
    "\n",
    "    def opS(self):\n",
    "        if not self.opA.has_taint() and isinstance(self.opB, tstr):\n",
    "            return (self.opB, self.opA)\n",
    "        else:\n",
    "            return (self.opA, self.opB)\n",
    "\n",
    "    @property\n",
    "    def op_A(self):\n",
    "        return self.opS()[0]\n",
    "\n",
    "    @property\n",
    "    def op_B(self):\n",
    "        return self.opS()[1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s,%s,%s\" % (self.o(), repr(self.opA), repr(self.opB))\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.op == Op.EQ:\n",
    "            if str(self.opA) == str(self.opB):\n",
    "                return \"%s = %s\" % (repr(self.opA), repr(self.opB))\n",
    "            else:\n",
    "                return \"%s != %s\" % (repr(self.opA), repr(self.opB))\n",
    "        elif self.op == Op.NE:\n",
    "            if str(self.opA) == str(self.opB):\n",
    "                return \"%s = %s\" % (repr(self.opA), repr(self.opB))\n",
    "            else:\n",
    "                return \"%s != %s\" % (repr(self.opA), repr(self.opB))\n",
    "        elif self.op == Op.IN:\n",
    "            if str(self.opA) in str(self.opB):\n",
    "                return \"%s in %s\" % (repr(self.opA), repr(self.opB))\n",
    "            else:\n",
    "                return \"%s not in %s\" % (repr(self.opA), repr(self.opB))\n",
    "        elif self.op == Op.NOT_IN:\n",
    "            if str(self.opA) in str(self.opB):\n",
    "                return \"%s in %s\" % (repr(self.opA), repr(self.opB))\n",
    "            else:\n",
    "                return \"%s not in %s\" % (repr(self.opA), repr(self.opB))\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "\n",
    "class ctstr(eoftstr):\n",
    "    def create(self, res, taint):\n",
    "        o = ctstr(res, taint, self)\n",
    "        o.comparisons = self.comparisons\n",
    "        return o\n",
    "\n",
    "    def add_instr(self, op, c_a, c_b):\n",
    "        self.comparisons.append(Instr(op, c_a, c_b))\n",
    "\n",
    "    def with_comparisons(self, comparisons):\n",
    "        self.comparisons = comparisons\n",
    "        return self\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def __eq__(self, other):\n",
    "        if len(self) == 0 and len(other) == 0:\n",
    "            self.add_instr(Op.EQ, self, other)\n",
    "            return True\n",
    "        elif len(self) == 0:\n",
    "            self.add_instr(Op.EQ, self, other[0])\n",
    "            return False\n",
    "        elif len(other) == 0:\n",
    "            self.add_instr(Op.EQ, self[0], other)\n",
    "            return False\n",
    "        elif len(self) == 1 and len(other) == 1:\n",
    "            self.add_instr(Op.EQ, self, other)\n",
    "            return super().__eq__(other)\n",
    "        else:\n",
    "            if not self[0] == other[0]:\n",
    "                return False\n",
    "            return self[1:] == other[1:]\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def __contains__(self, other):\n",
    "        self.add_instr(Op.IN, self, other)\n",
    "        return super().__contains__(other)\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def find(self, sub, start=None, end=None):\n",
    "        if start is None:\n",
    "            start_val = 0\n",
    "        else:\n",
    "            start_val = start\n",
    "        if end is None:\n",
    "            end_val = len(self)\n",
    "        else:\n",
    "            end_val = end\n",
    "        self.add_instr(Op.IN, self[start_val:end_val], sub)\n",
    "        return super().find(sub, start, end)\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def rfind(self, sub, start=None, end=None):\n",
    "        if start is None:\n",
    "            start_val = 0\n",
    "        else:\n",
    "            start_val = start\n",
    "        if end is None:\n",
    "            end_val = len(self)\n",
    "        else:\n",
    "            end_val = end\n",
    "        self.add_instr(Op.IN, self[start_val:end_val], sub)\n",
    "        return super().find(sub, start, end)\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def startswith(self, s, beg =0,end=None):\n",
    "        if end == None:\n",
    "            end = len(self)\n",
    "        self == s[beg:end]\n",
    "        return super().startswith(s, beg, end)\n",
    "\n",
    "\n",
    "def substrings(s, l):\n",
    "    for i in range(len(s) - (l - 1)):\n",
    "        yield s[i:i + l]\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def in_(self, s):\n",
    "        if isinstance(s, str):\n",
    "            # c in '0123456789'\n",
    "            # to\n",
    "            # __fn(c).in_('0123456789')\n",
    "            # ensure that all characters are compared\n",
    "            result = [self == c for c in substrings(s, len(self))]\n",
    "            return any(result)\n",
    "        else:\n",
    "            for item in s:\n",
    "                if self == item:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "class ctstr(ctstr):\n",
    "    def split(self, sep=None, maxsplit=-1):\n",
    "        self.add_instr(Op.IN, self, sep)\n",
    "        return super().split(sep, maxsplit)\n",
    "\n",
    "\n",
    "class xtstr(ctstr):\n",
    "    def _find(self, substr, sub, m):\n",
    "        v_ = str(substr)\n",
    "        if not v_: return []\n",
    "        v = v_.find(str(sub))\n",
    "        start = substr.taint[0]\n",
    "        if v == -1:\n",
    "            return [(i, m) for i in range(start, start + len(substr))]\n",
    "        else:\n",
    "            return [(i, m) for i in range(start, start + v + len(sub))]\n",
    "\n",
    "    def add_instr(self, op, c_a, c_b):\n",
    "        ct = None\n",
    "        m = get_current_method()\n",
    "        if len(c_a) == 1 and isinstance(c_a, xtstr):\n",
    "            ct = c_a.taint[0]\n",
    "            self.comparisons.append((ct, m))\n",
    "        elif len(c_b) == 1 and isinstance(c_b, xtstr):\n",
    "            ct = c_b.taint[0]\n",
    "            self.comparisons.append((ct, m))\n",
    "        elif op == Op.IN:\n",
    "            self.comparisons.extend(self._find(c_a, c_b, m))\n",
    "        elif len(c_a) == 0 or len(c_b) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, \"op:%s A:%s B:%s\" % (op, c_a, c_b)\n",
    "        # print(repr(m))\n",
    "    def replace(self, old, new, count=None):\n",
    "        m = get_current_method()\n",
    "        if count is not None:\n",
    "            # TODO\n",
    "            self.comparisons.extend([(t, m) for t in self.taint])\n",
    "            return super().replace(old, new, count)\n",
    "        else:\n",
    "            self.comparisons.extend([(t, m) for t in self.taint])\n",
    "            return super().replace(old, new)\n",
    "\n",
    "    def create(self, res, taint):\n",
    "        o = xtstr(res, taint, self)\n",
    "        o.comparisons = self.comparisons\n",
    "        return o\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "import inspect\n",
    "def make_str_abort_wrapper(fun):\n",
    "    def proxy(*args, **kwargs):\n",
    "        raise tstr.TaintException(\n",
    "            '%s Not implemented in `xtstr`' %\n",
    "            fun.__name__)\n",
    "    return proxy\n",
    "\n",
    "defined_xtstr = {}\n",
    "for name, fn in inspect.getmembers(xtstr, callable):\n",
    "    clz = fn.__qualname__.split('.')[0]\n",
    "    if clz in {'ctstr', 'xtstr'}:\n",
    "        defined_xtstr[name] = clz\n",
    "\n",
    "for name, fn in inspect.getmembers(str, callable):\n",
    "    if name not in defined_xtstr and name not in {\n",
    "            '__init__', '__str__', '__eq__', '__ne__', '__class__', '__new__',\n",
    "            '__setattr__', '__len__', '__getattribute__', '__le__', 'lower',\n",
    "            'strip', 'lstrip', 'rstrip', '__iter__', '__getitem__', '__add__'}:\n",
    "        setattr(xtstr, name, make_str_abort_wrapper(fn))\n",
    "        \n",
    "\n",
    "CURRENT_METHOD = None\n",
    "METHOD_NUM_STACK = []\n",
    "METHOD_MAP = {}\n",
    "METHOD_NUM = 0\n",
    "\n",
    "def get_current_method():\n",
    "    return CURRENT_METHOD\n",
    "\n",
    "def set_current_method(method, stack_depth, mid):\n",
    "    global CURRENT_METHOD\n",
    "    CURRENT_METHOD = (method, stack_depth, mid)\n",
    "    return CURRENT_METHOD\n",
    "\n",
    "def trace_init():\n",
    "    global CURRENT_METHOD\n",
    "    global METHOD_NUM_STACK\n",
    "    global METHOD_MAP\n",
    "    global METHOD_NUM\n",
    "    CURRENT_METHOD = None\n",
    "    METHOD_NUM_STACK.clear()\n",
    "    METHOD_MAP.clear()\n",
    "    METHOD_NUM = 0\n",
    "\n",
    "    start = (METHOD_NUM, None, [])\n",
    "    METHOD_NUM_STACK.append(start)\n",
    "    METHOD_MAP[METHOD_NUM] = start\n",
    "\n",
    "def trace_call(method):\n",
    "    global CURRENT_METHOD\n",
    "    global METHOD_NUM_STACK\n",
    "    global METHOD_MAP\n",
    "    global METHOD_NUM\n",
    "    METHOD_NUM += 1\n",
    "\n",
    "    # create our method invocation\n",
    "    # method_num, method_name, children\n",
    "    n = (METHOD_NUM, method, [])\n",
    "    METHOD_MAP[METHOD_NUM] = n\n",
    "    # add ourselves as one of the children to the previous method invocation\n",
    "    METHOD_NUM_STACK[-1][2].append(n)\n",
    "    # and set us as the current method.\n",
    "    METHOD_NUM_STACK.append(n)\n",
    "\n",
    "def trace_return():\n",
    "    METHOD_NUM_STACK.pop()\n",
    "\n",
    "def trace_set_method(method):\n",
    "    set_current_method(method, len(METHOD_NUM_STACK), METHOD_NUM_STACK[-1][0])\n",
    "    \n",
    "class in_wrap:\n",
    "    def __init__(self, s):\n",
    "        self.s = s\n",
    "\n",
    "    def in_(self, s):\n",
    "        m = get_current_method()\n",
    "        if isinstance(s, xtstr):\n",
    "            cmps = s._find(s, self.s, m)\n",
    "            s.comparisons.extend(cmps)\n",
    "        if isinstance(self.s, xtstr):\n",
    "            cmps = [(t,m) for t in self.s.taint]\n",
    "            self.s.comparisons.extend(cmps)\n",
    "        return self.s in s\n",
    "\n",
    "def taint_wrap__(st):\n",
    "    if isinstance(st, str):\n",
    "        return in_wrap(st)\n",
    "    else:\n",
    "        return st\n",
    "\n",
    "def wrap_input(inputstr):\n",
    "    return xtstr(inputstr, parent=None).with_comparisons([])\n",
    "\n",
    "def convert_comparisons(comparisons, inputstr):\n",
    "    light_comparisons = []\n",
    "    for idx, (method, stack_depth, mid) in comparisons:\n",
    "        if idx is None: continue\n",
    "        light_comparisons.append((idx, inputstr[idx], mid))\n",
    "    return light_comparisons\n",
    "\n",
    "def convert_method_map(method_map):\n",
    "    light_map = {}\n",
    "    for k in method_map:\n",
    "        method_num, method_name, children = method_map[k]\n",
    "        light_map[k] = (k, method_name, [c[0] for c in children])\n",
    "    return light_map\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write both files to the appropriate locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.787969Z",
     "start_time": "2019-11-10T17:56:51.778371Z"
    },
    "tags": [
     "#gen_helpers",
     "=>mimid_context_src",
     "=>mimid_taints_src",
     "=>mimid_tracer_src"
    ]
   },
   "outputs": [],
   "source": [
    "# [(\n",
    "with open('build/mimid_context.py', 'w+') as f:\n",
    "    print(VARS['mimid_method_context'], file=f)\n",
    "    print(VARS['mimid_stack_context'], file=f)\n",
    "    print(VARS['mimid_scope_context'], file=f)\n",
    "\n",
    "with open('build/taints.py', 'w+') as f:\n",
    "    print(VARS['taints_src'], file=f)\n",
    "with open('build/config.py', 'w+') as f:\n",
    "    print(VARS['config_src'], file=f)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "import taints\n",
    "from taints import taint_wrap__\n",
    "import json\n",
    "import copy\n",
    "from mimid_context import scope__, stack__, method__\n",
    "\n",
    "\n",
    "def example(string):\n",
    "    with method__('example', [string]) as _method__:\n",
    "        if taint_wrap__('?').in_(string):\n",
    "            print('?')\n",
    "        for i in string:\n",
    "            print(i)\n",
    "    \n",
    "def main(arg):\n",
    "    with method__('main', [arg]) as _method__:\n",
    "        example(arg)\n",
    "\n",
    "mystring = 'cat?cats'\n",
    "taints.trace_init()\n",
    "tainted_input = taints.wrap_input(mystring)\n",
    "example(tainted_input)\n",
    "cs = copy.copy(tainted_input.comparisons)\n",
    "for c in cs:\n",
    "    print(c)\n",
    "'''\n",
    "with open('build/example.py' , 'w+') as f:\n",
    "    print(example, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd build; python3 example.py )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one can generate traces for the `calc` program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.837063Z",
     "start_time": "2019-11-10T17:56:51.797850Z"
    },
    "tags": [
     "#eg_calc_dir"
    ]
   },
   "outputs": [],
   "source": [
    "%top do(['mkdir','-p','samples/calc']).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.873015Z",
     "start_time": "2019-11-10T17:56:51.841392Z"
    },
    "tags": [
     "#eg_calc_dir"
    ]
   },
   "outputs": [],
   "source": [
    "%top do(['mkdir','-p','samples/mathexpr']).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.887537Z",
     "start_time": "2019-11-10T17:56:51.876652Z"
    },
    "code_folding": [],
    "tags": [
     "=>gen_src",
     "=>gen_helpers",
     "=>eg_calc_dir",
     "#eg_calc"
    ]
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "with open('samples/calc/0.csv', 'w+') as f:\n",
    "    print('9-(16+72)*3/458', file=f)\n",
    "    \n",
    "with open('samples/calc/1.csv', 'w+') as f:\n",
    "    print('(9)+3/4/58', file=f)\n",
    "    \n",
    "with open('samples/calc/2.csv', 'w+') as f:\n",
    "    print('8*3/40', file=f)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating traces on `mathexpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.899583Z",
     "start_time": "2019-11-10T17:56:51.890702Z"
    },
    "code_folding": [],
    "tags": [
     "=>gen_src",
     "=>gen_helpers",
     "=>eg_calc_dir",
     "#eg_calc"
    ]
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "with open('samples/mathexpr/0.csv', 'w+') as f:\n",
    "    print('100', file=f)\n",
    "    \n",
    "with open('samples/mathexpr/1.csv', 'w+') as f:\n",
    "    print('2 + 3', file=f)\n",
    "    \n",
    "with open('samples/mathexpr/2.csv', 'w+') as f:\n",
    "    print('4 * 5 + cos(30)', file=f)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:51.989293Z",
     "start_time": "2019-11-10T17:56:51.902926Z"
    },
    "tags": [
     "=>eg_calc",
     "#eg_calc_trace"
    ]
   },
   "outputs": [],
   "source": [
    "%top calc_trace_out = do(\"python3 build/calculator.py samples/calc/*.csv\", shell=True).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.082181Z",
     "start_time": "2019-11-10T17:56:51.993713Z"
    },
    "tags": [
     "=>eg_calc",
     "#eg_calc_trace"
    ]
   },
   "outputs": [],
   "source": [
    "%top mathexpr_trace_out = do(\"python3 build/mathexpr.py samples/mathexpr/*.csv\", shell=True).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.092384Z",
     "start_time": "2019-11-10T17:56:52.085711Z"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.128915Z",
     "start_time": "2019-11-10T17:56:52.113099Z"
    },
    "tags": [
     "=>eg_calc_trace"
    ]
   },
   "outputs": [],
   "source": [
    "%top calc_trace = json.loads(calc_trace_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct in calc_trace:\n",
    "    cstr = ct['inputstr']\n",
    "    print(cstr)\n",
    "    seen = set()\n",
    "    for ci, char, m in ct['comparisons']:\n",
    "        assert(char == cstr[ci])\n",
    "        seen.add(ci)\n",
    "        print(ci, cstr[ci])\n",
    "    print(seen, cstr)\n",
    "    assert len(seen) == len(cstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.140299Z",
     "start_time": "2019-11-10T17:56:52.132455Z"
    },
    "tags": [
     "=>eg_calc_trace"
    ]
   },
   "outputs": [],
   "source": [
    "%top mathexpr_trace = json.loads(mathexpr_trace_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining the Traces Generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the Method Tree with Attached Character Comparisons\n",
    "\n",
    "Reconstruct the actual method trace from a trace with the following\n",
    "format\n",
    "```\n",
    "key   : [ mid, method_name, children_ids ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.197498Z",
     "start_time": "2019-11-10T17:56:52.187742Z"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruct_method_tree(method_map):\n",
    "    first_id = None\n",
    "    tree_map = {}\n",
    "    for key in method_map:\n",
    "        m_id, m_name, m_children = method_map[key]\n",
    "        children = []\n",
    "        if m_id in tree_map:\n",
    "            # just update the name and children\n",
    "            assert not tree_map[m_id]\n",
    "            tree_map[m_id]['id'] = m_id\n",
    "            tree_map[m_id]['name'] = m_name\n",
    "            tree_map[m_id]['indexes'] = []\n",
    "            tree_map[m_id]['children'] = children\n",
    "        else:\n",
    "            assert first_id is None\n",
    "            tree_map[m_id] = {'id': m_id, 'name': m_name, 'children': children, 'indexes': []}\n",
    "            first_id = m_id\n",
    "\n",
    "        for c in m_children:\n",
    "            assert c not in tree_map\n",
    "            val = {}\n",
    "            tree_map[c] = val\n",
    "            children.append(val)\n",
    "    return first_id, tree_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it. The first element in the returned tuple is the id of the bottom most method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.227813Z",
     "start_time": "2019-11-10T17:56:52.200868Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.237892Z",
     "start_time": "2019-11-10T17:56:52.230459Z"
    }
   },
   "outputs": [],
   "source": [
    "%top first, calc_method_tree1 = reconstruct_method_tree(calc_trace[0]['method_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.255863Z",
     "start_time": "2019-11-10T17:56:52.247477Z"
    }
   },
   "outputs": [],
   "source": [
    "%top first, mathexpr_method_tree1 = reconstruct_method_tree(mathexpr_trace[0]['method_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.266313Z",
     "start_time": "2019-11-10T17:56:52.259045Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_node(node, id):\n",
    "    symbol = str(node['id'])\n",
    "    children = node['children']\n",
    "    annotation = str(node['name'])\n",
    "    return \"%s:%s\" % (symbol, annotation), children, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.283861Z",
     "start_time": "2019-11-10T17:56:52.268706Z"
    }
   },
   "outputs": [],
   "source": [
    "%top v = display_tree(calc_method_tree1[0], extract_node=extract_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.291289Z",
     "start_time": "2019-11-10T17:56:52.286185Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.301335Z",
     "start_time": "2019-11-10T17:56:52.294758Z"
    }
   },
   "outputs": [],
   "source": [
    "def zoom(v, zoom=True):\n",
    "    # return v directly if you do not want to zoom out.\n",
    "    if zoom:\n",
    "        return Image(v.render(format='png'))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.450129Z",
     "start_time": "2019-11-10T17:56:52.305249Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.586826Z",
     "start_time": "2019-11-10T17:56:52.455953Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(mathexpr_method_tree1[0], extract_node=extract_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.656286Z",
     "start_time": "2019-11-10T17:56:52.593345Z"
    }
   },
   "outputs": [],
   "source": [
    "%top trace = {0: {'id': 0, 'name': None, 'children': [{'id': 1, 'name': '_real_program_main', 'indexes': [], 'children': [{'id': 2, 'name': '_real_program_main:if_16 - 0#[-1]', 'indexes': [], 'children': [{'id': 3, 'name': 'if:if_17 - 0#[-1, -1]', 'indexes': [], 'children': []}]}, {'id': 4, 'name': 'parse_expr', 'indexes': [], 'children': [{'id': 5, 'name': 'parse_expr:while_10 ? [1]', 'indexes': [], 'children': [{'id': 6, 'name': 'while:if_11 - 0#[1, -1]', 'indexes': [], 'children': [{'id': 7, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}, {'id': 12, 'name': 'parse_expr:while_10 ? [2]', 'indexes': [], 'children': [{'id': 13, 'name': 'while:if_11 - 0#[2, -1]', 'indexes': [], 'children': []}]}, {'id': 14, 'name': 'parse_expr:while_10 ? [3]', 'indexes': [], 'children': [{'id': 15, 'name': 'while:if_11 - 0#[3, -1]', 'indexes': [], 'children': [{'id': 16, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}]}]}], 'indexes': []}, 1: {'id': 1, 'name': '_real_program_main', 'indexes': [], 'children': [{'id': 2, 'name': '_real_program_main:if_16 - 0#[-1]', 'indexes': [], 'children': [{'id': 3, 'name': 'if:if_17 - 0#[-1, -1]', 'indexes': [], 'children': []}]}, {'id': 4, 'name': 'parse_expr', 'indexes': [], 'children': [{'id': 5, 'name': 'parse_expr:while_10 ? [1]', 'indexes': [], 'children': [{'id': 6, 'name': 'while:if_11 - 0#[1, -1]', 'indexes': [], 'children': [{'id': 7, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}, {'id': 12, 'name': 'parse_expr:while_10 ? [2]', 'indexes': [], 'children': [{'id': 13, 'name': 'while:if_11 - 0#[2, -1]', 'indexes': [], 'children': []}]}, {'id': 14, 'name': 'parse_expr:while_10 ? [3]', 'indexes': [], 'children': [{'id': 15, 'name': 'while:if_11 - 0#[3, -1]', 'indexes': [], 'children': [{'id': 16, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}]}]}, 2: {'id': 2, 'name': '_real_program_main:if_16 - 0#[-1]', 'indexes': [], 'children': [{'id': 3, 'name': 'if:if_17 - 0#[-1, -1]', 'indexes': [], 'children': []}]}, 4: {'id': 4, 'name': 'parse_expr', 'indexes': [], 'children': [{'id': 5, 'name': 'parse_expr:while_10 ? [1]', 'indexes': [], 'children': [{'id': 6, 'name': 'while:if_11 - 0#[1, -1]', 'indexes': [], 'children': [{'id': 7, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}, {'id': 12, 'name': 'parse_expr:while_10 ? [2]', 'indexes': [], 'children': [{'id': 13, 'name': 'while:if_11 - 0#[2, -1]', 'indexes': [], 'children': []}]}, {'id': 14, 'name': 'parse_expr:while_10 ? [3]', 'indexes': [], 'children': [{'id': 15, 'name': 'while:if_11 - 0#[3, -1]', 'indexes': [], 'children': [{'id': 16, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}]}, 3: {'id': 3, 'name': 'if:if_17 - 0#[-1, -1]', 'indexes': [], 'children': []}, 5: {'id': 5, 'name': 'parse_expr:while_10 ? [1]', 'indexes': [], 'children': [{'id': 6, 'name': 'while:if_11 - 0#[1, -1]', 'indexes': [], 'children': [{'id': 7, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}, 12: {'id': 12, 'name': 'parse_expr:while_10 ? [2]', 'indexes': [], 'children': [{'id': 13, 'name': 'while:if_11 - 0#[2, -1]', 'indexes': [], 'children': []}]}, 14: {'id': 14, 'name': 'parse_expr:while_10 ? [3]', 'indexes': [], 'children': [{'id': 15, 'name': 'while:if_11 - 0#[3, -1]', 'indexes': [], 'children': [{'id': 16, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}]}, 6: {'id': 6, 'name': 'while:if_11 - 0#[1, -1]', 'indexes': [], 'children': [{'id': 7, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}, 7: {'id': 7, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}, 8: {'id': 8, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, 10: {'id': 10, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}, 9: {'id': 9, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}, 11: {'id': 11, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}, 13: {'id': 13, 'name': 'while:if_11 - 0#[2, -1]', 'indexes': [], 'children': []}, 15: {'id': 15, 'name': 'while:if_11 - 0#[3, -1]', 'indexes': [], 'children': [{'id': 16, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}]}, 16: {'id': 16, 'name': 'parse_num', 'indexes': [], 'children': [{'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}]}, 17: {'id': 17, 'name': 'parse_num:for_2 ? 0#[1]', 'indexes': [], 'children': [{'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}]}, 19: {'id': 19, 'name': 'parse_num:for_2 ? 0#[2]', 'indexes': [], 'children': [{'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}]}, 18: {'id': 18, 'name': 'for:if_3 - 1#[1, -1]', 'indexes': [], 'children': []}, 20: {'id': 20, 'name': 'for:if_3 - 0#[2, -1]', 'indexes': [], 'children': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.759898Z",
     "start_time": "2019-11-10T17:56:52.660234Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(trace[0], extract_node=extract_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying last comparisons\n",
    "We need only the last comparisons made on any index. This means that we should care for only the last parse in an ambiguous parse. So, we assign the method that last touched an index to be its consumer.\n",
    "\n",
    "However, to make concessions for real world, we also check if we are overwriting a child (`HEURISTIC`). Essentially, if the heursitic is enabled, then if the current method id (`midP`) is smaller than the `midC` already stored in the last comparison map, then it means that `midP` is a parent that called `midC` previously, and now accessing an index that `midC` touched. This happens when the parent tries to find a substring like `#` in the entirety of the original string. (Note that we have seen this only in `URLParser`). (Note that this heuristic does not restrict reparsing by another function call -- in such a case, `midC` will not smaller than `midP`). So, perhaps, we should let the child keep the ownership. However, there is one more wrinkle. If the character being contested was the last index touched by our `mid`, then it is likely that it was simply a boundary check. In that case, we should let the parent own this character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_COMPARISON_HEURISTIC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.772570Z",
     "start_time": "2019-11-10T17:56:52.763269Z"
    }
   },
   "outputs": [],
   "source": [
    "def last_comparisons(comparisons):\n",
    "    last_cmp_only = {}\n",
    "    last_idx = {}\n",
    "\n",
    "    # get the last indexes compared in methods.\n",
    "    for idx, char, mid in comparisons:\n",
    "        if mid in last_idx:\n",
    "            if idx > last_idx[mid]:\n",
    "                last_idx[mid] = idx\n",
    "        else:\n",
    "            last_idx[mid] = idx\n",
    "\n",
    "    for idx, char, mid in comparisons:\n",
    "        if LAST_COMPARISON_HEURISTIC:\n",
    "            if idx in last_cmp_only:\n",
    "                midC = last_cmp_only[idx]\n",
    "                if midC > mid:\n",
    "                    # midC is a child of mid.\n",
    "                    # do not clobber children unless it was the last character\n",
    "                    # for that child.\n",
    "                    if last_idx[mid] == idx:\n",
    "                        # if it was the last index, may be the child used it\n",
    "                        # as a boundary check.\n",
    "                        pass\n",
    "                    else:\n",
    "                        # do not overwrite the current value of `last_cmp_only[idx]`\n",
    "                        continue\n",
    "        last_cmp_only[idx] = mid\n",
    "    return last_cmp_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.781804Z",
     "start_time": "2019-11-10T17:56:52.776245Z"
    }
   },
   "outputs": [],
   "source": [
    "%top calc_last_comparisons1 = last_comparisons(calc_trace[0]['comparisons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.792863Z",
     "start_time": "2019-11-10T17:56:52.785859Z"
    }
   },
   "outputs": [],
   "source": [
    "%top calc_last_comparisons1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.802750Z",
     "start_time": "2019-11-10T17:56:52.798386Z"
    }
   },
   "outputs": [],
   "source": [
    "%top mathexpr_last_comparisons1 = last_comparisons(mathexpr_trace[0]['comparisons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.815775Z",
     "start_time": "2019-11-10T17:56:52.809321Z"
    }
   },
   "outputs": [],
   "source": [
    "%top mathexpr_last_comparisons1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attaching characters to the tree\n",
    "Add the comparison indexes to the method tree that we constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.823511Z",
     "start_time": "2019-11-10T17:56:52.818689Z"
    }
   },
   "outputs": [],
   "source": [
    "def attach_comparisons(method_tree, comparisons):\n",
    "    for idx in comparisons:\n",
    "        mid = comparisons[idx]\n",
    "        method_tree[mid]['indexes'].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it. Note which method call each input index is associated. For example, the first index is associated with method call id: 6, which corresponds to `is_digit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.832378Z",
     "start_time": "2019-11-10T17:56:52.827218Z"
    }
   },
   "outputs": [],
   "source": [
    "%top attach_comparisons(calc_method_tree1, calc_last_comparisons1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.893773Z",
     "start_time": "2019-11-10T17:56:52.834505Z"
    }
   },
   "outputs": [],
   "source": [
    "%top calc_method_tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:52.904668Z",
     "start_time": "2019-11-10T17:56:52.897444Z"
    }
   },
   "outputs": [],
   "source": [
    "%top attach_comparisons(mathexpr_method_tree1, mathexpr_last_comparisons1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.019675Z",
     "start_time": "2019-11-10T17:56:52.906765Z"
    }
   },
   "outputs": [],
   "source": [
    "%top mathexpr_method_tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.032194Z",
     "start_time": "2019-11-10T17:56:53.023469Z"
    }
   },
   "outputs": [],
   "source": [
    "def wrap_input(istr):\n",
    "    def extract_node(node, id):\n",
    "        symbol = str(node['id'])\n",
    "        children = node['children']\n",
    "        annotation = str(node['name'])\n",
    "        indexes = repr(tuple([istr[i] for i in node['indexes']]))\n",
    "        return \"%s %s\" % (annotation, indexes), children, ''\n",
    "    return extract_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.042232Z",
     "start_time": "2019-11-10T17:56:53.034999Z"
    }
   },
   "outputs": [],
   "source": [
    "%top extract_node1 = wrap_input(calc_trace[0]['inputstr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.232374Z",
     "start_time": "2019-11-10T17:56:53.070454Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(calc_method_tree1[0], extract_node=extract_node1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.251445Z",
     "start_time": "2019-11-10T17:56:53.245027Z"
    }
   },
   "outputs": [],
   "source": [
    "%top extract_node1 = wrap_input(mathexpr_trace[0]['inputstr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.390719Z",
     "start_time": "2019-11-10T17:56:53.255170Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(mathexpr_method_tree1[0], extract_node=extract_node1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `to_node()` a convenience function that, given a list of _contiguous_ indexes and original string, translates it to a leaf node of a tree (that corresponds to the derivation tree syntax in the Fuzzingbook) with a string, empty children, and starting node and ending node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a list of indexes to a corresponding terminal tree node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.403217Z",
     "start_time": "2019-11-10T17:56:53.395039Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_node(idxes, my_str):\n",
    "    assert len(idxes) == idxes[-1] - idxes[0] + 1\n",
    "    assert min(idxes) == idxes[0]\n",
    "    assert max(idxes) == idxes[-1]\n",
    "    return my_str[idxes[0]:idxes[-1] + 1], [], idxes[0], idxes[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in calc_method_tree1.keys():\n",
    "    idxs = calc_method_tree1[k]['indexes']\n",
    "    if idxs:\n",
    "        print(k, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.417053Z",
     "start_time": "2019-11-10T17:56:53.407642Z"
    }
   },
   "outputs": [],
   "source": [
    "%top to_node(calc_method_tree1[9]['indexes'], calc_trace[0]['inputstr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.428310Z",
     "start_time": "2019-11-10T17:56:53.422150Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to identify the terminal (leaf) nodes. For that, we want to group contiguous letters in a node together, and call it a leaf node. So, convert our list of indexes to lists of contiguous indexes first, then convert them to terminal tree nodes. Then, return a set of one level child nodes with contiguous chars from indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.438556Z",
     "start_time": "2019-11-10T17:56:53.431752Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexes_to_children(indexes, my_str):\n",
    "    lst = [\n",
    "        list(map(itemgetter(1), g))\n",
    "        for k, g in it.groupby(enumerate(indexes), lambda x: x[0] - x[1])\n",
    "    ]\n",
    "\n",
    "    return [to_node(n, my_str) for n in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.450699Z",
     "start_time": "2019-11-10T17:56:53.441790Z"
    }
   },
   "outputs": [],
   "source": [
    "%top indexes_to_children(calc_method_tree1[9]['indexes'], calc_trace[0]['inputstr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to remove the overlap from the trees we have so far. The idea is that, given a node, each child node of that node should be uniquely responsible for a specified range of characters, with no overlap allowed between the children. The starting of the first child to ending of the last child will be the range of the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Overlap\n",
    "If overlap is found, the tie is biased to the later child. That is, the later child gets to keep the range, and the former child is recursively traversed to remove overlaps from its children. If a child is completely included in the overlap, the child is excised. A few convenience functions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.460270Z",
     "start_time": "2019-11-10T17:56:53.453950Z"
    }
   },
   "outputs": [],
   "source": [
    "def does_item_overlap(r, r_):\n",
    "    (s, e), (s_, e_) = r, r_\n",
    "    return ((s_ >= s and s_ <= e) or \n",
    "            (e_ >= s and e_ <= e) or \n",
    "            (s_ <= s and e_ >= e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.469385Z",
     "start_time": "2019-11-10T17:56:53.463966Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_second_item_included(r, r_):\n",
    "    (s, e), (s_, e_) = r, r_\n",
    "    return (s_ >= s and e_ <= e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.478818Z",
     "start_time": "2019-11-10T17:56:53.472395Z"
    }
   },
   "outputs": [],
   "source": [
    "def has_overlap(ranges, r_):\n",
    "    return {r for r in ranges if does_item_overlap(r, r_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.499507Z",
     "start_time": "2019-11-10T17:56:53.488325Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_included(ranges, r_):\n",
    "    return {r for r in ranges if is_second_item_included(r, r_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.512889Z",
     "start_time": "2019-11-10T17:56:53.503324Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_overlap_from(original_node, orange):\n",
    "    node, children, start, end = original_node\n",
    "    new_children = []\n",
    "    if not children:\n",
    "        return None\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for child in children:\n",
    "        if does_item_overlap(child[2:4], orange):\n",
    "            new_child = remove_overlap_from(child, orange)\n",
    "            if new_child: # and new_child[1]:\n",
    "                if start == -1: start = new_child[2]\n",
    "                new_children.append(new_child)\n",
    "                end = new_child[3]\n",
    "        else:\n",
    "            new_children.append(child)\n",
    "            if start == -1: start = child[2]\n",
    "            end = child[3]\n",
    "    if not new_children:\n",
    "        return None\n",
    "    assert start != -1\n",
    "    assert end != -1\n",
    "    return (node, new_children, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there is no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.522232Z",
     "start_time": "2019-11-10T17:56:53.515146Z"
    }
   },
   "outputs": [],
   "source": [
    "def no_overlap(arr):\n",
    "    my_ranges = {}\n",
    "    for a in arr:\n",
    "        _, _, s, e = a\n",
    "        r = (s, e)\n",
    "        included = is_included(my_ranges, r)\n",
    "        if included:\n",
    "            continue  # we will fill up the blanks later.\n",
    "        else:\n",
    "            overlaps = has_overlap(my_ranges, r) \n",
    "            if overlaps:\n",
    "                # unlike include which can happen only once in a set of\n",
    "                # non-overlapping ranges, overlaps can happen on multiple parts.\n",
    "                # The rule is, the later child gets the say. So, we recursively\n",
    "                # remove any ranges that overlap with the current one from the\n",
    "                # overlapped range.\n",
    "                assert len(overlaps) == 1\n",
    "                oitem = list(overlaps)[0]\n",
    "                v = remove_overlap_from(my_ranges[oitem], r)\n",
    "                del my_ranges[oitem]\n",
    "                if v:\n",
    "                    my_ranges[v[2:4]] = v\n",
    "                my_ranges[r] = a\n",
    "            else:\n",
    "                my_ranges[r] = a\n",
    "    res = my_ranges.values()\n",
    "    # assert no overlap, and order by starting index\n",
    "    s = sorted(res, key=lambda x: x[2])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate derivation tree\n",
    "\n",
    "Convert a mapped tree to the _fuzzingbook_ style derivation tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.533219Z",
     "start_time": "2019-11-10T17:56:53.524649Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_tree(node, my_str):\n",
    "    method_name = (\"<%s>\" % node['name']) if node['name'] is not None else '<START>'\n",
    "    indexes = node['indexes']\n",
    "    node_children = [to_tree(c, my_str) for c in node.get('children', [])]\n",
    "    idx_children = indexes_to_children(indexes, my_str)\n",
    "    children = no_overlap([c for c in node_children if c is not None] + idx_children)\n",
    "    if not children:\n",
    "        return None\n",
    "    start_idx = children[0][2]\n",
    "    end_idx = children[-1][3]\n",
    "    si = start_idx\n",
    "    my_children = []\n",
    "    # FILL IN chars that we did not compare. This is likely due to an i + n\n",
    "    # instruction.\n",
    "    for c in children:\n",
    "        if c[2] != si:\n",
    "            sbs = my_str[si: c[2]]\n",
    "            my_children.append((sbs, [], si, c[2] - 1))\n",
    "        my_children.append(c)\n",
    "        si = c[3] + 1\n",
    "\n",
    "    m = (method_name, my_children, start_idx, end_idx)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.635209Z",
     "start_time": "2019-11-10T17:56:53.536804Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(to_tree(calc_method_tree1[0], calc_trace[0]['inputstr'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.750690Z",
     "start_time": "2019-11-10T17:56:53.640068Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(to_tree(mathexpr_method_tree1[0], mathexpr_trace[0]['inputstr'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Complete Miner\n",
    "\n",
    "We now put everything together. The `miner()` takes the traces, produces trees out of them, and verifies that the trees actually correspond to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nt(v):\n",
    "    return len(v) > 1 and (v[0], v[-1]) == ('<', '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.780757Z",
     "start_time": "2019-11-10T17:56:53.771769Z"
    }
   },
   "outputs": [],
   "source": [
    "def tree_to_str(tree): # Non recursive\n",
    "    expanded = []\n",
    "    to_expand = [tree]\n",
    "    while to_expand:\n",
    "        (key, children, *rest), *to_expand = to_expand\n",
    "        if is_nt(key):\n",
    "            to_expand = children + to_expand\n",
    "        else:\n",
    "            assert not children\n",
    "            expanded.append(key)\n",
    "    return ''.join(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.793053Z",
     "start_time": "2019-11-10T17:56:53.784267Z"
    }
   },
   "outputs": [],
   "source": [
    "def miner(call_traces):\n",
    "    my_trees = []\n",
    "    for call_trace in call_traces:\n",
    "        with open('last_trace.json', 'w+') as f:\n",
    "            json.dump([call_trace], fp=f)\n",
    "        method_map = call_trace['method_map']\n",
    "\n",
    "        first, method_tree = reconstruct_method_tree(method_map)\n",
    "        comparisons = call_trace['comparisons']\n",
    "        attach_comparisons(method_tree, last_comparisons(comparisons))\n",
    "\n",
    "        my_str = call_trace['inputstr']\n",
    "\n",
    "        tree = to_tree(method_tree[first], my_str)\n",
    "        my_tree = {'tree': tree, 'original': call_trace['original'], 'arg': call_trace['arg']}\n",
    "        assert tree_to_str(tree) == my_str\n",
    "        my_trees.append(my_tree)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `miner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:53.916711Z",
     "start_time": "2019-11-10T17:56:53.796682Z"
    }
   },
   "outputs": [],
   "source": [
    "%top mined_calc_trees = miner(calc_trace)\n",
    "%top calc_tree = mined_calc_trees[0]\n",
    "%top zoom(display_tree(calc_tree['tree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.019406Z",
     "start_time": "2019-11-10T17:56:53.921061Z"
    }
   },
   "outputs": [],
   "source": [
    "%top mined_mathexpr_trees = miner(mathexpr_trace)\n",
    "%top mathexpr_tree = mined_mathexpr_trees[1]\n",
    "%top zoom(display_tree(mathexpr_tree['tree']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalize Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Fix this. We no longer have separate identifiers for while. The generalizer works by sampling different while nodes and comparing replacement patterns. The identifiers have also become alphanumeric.\n",
    "\n",
    "One of the problems that you can notice in the tree generated is that each `while` iterations get a different identifier. e.g. \n",
    "```\n",
    "                ('<parse_expr:while_1 ? [2]>', [('+', [], 5, 5)], 5, 5),\n",
    "                ('<parse_expr:while_1 ? [3]>',\n",
    "                 [('<parse_expr:if_1 + 0#[3, -1]>',\n",
    "                   [('<parse_num>',\n",
    "                     [('<is_digit>', [('7', [], 6, 6)], 6, 6),\n",
    "                      ('<is_digit>', [('2', [], 7, 7)], 7, 7)],\n",
    "                     6,\n",
    "                     7)],\n",
    "                   6,\n",
    "                   7)],\n",
    "\n",
    "```\n",
    "The separate identifiers are intentional because we do not yet know the actual dependencies between different iterations such as closing quotes, or closing braces or parenthesis. However, this creates a problem when we mine grammar because we need to match up the compatible nodes.\n",
    "\n",
    "Generalizer does it through actively doing surgery on the tree to see whether a node is replaceable with another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants to limit our exploration. The `MAX_PROC_SAMPLES` is used to specify how many samples to match for getting a swap patter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.208906Z",
     "start_time": "2019-11-10T17:56:54.204399Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_PROC_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching and book keeping variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.030919Z",
     "start_time": "2019-11-10T17:56:54.023477Z"
    }
   },
   "outputs": [],
   "source": [
    "EXEC_MAP = {}\n",
    "NODE_REGISTER = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:29.302382Z",
     "start_time": "2019-11-10T17:57:29.296951Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_generalizer():\n",
    "    global EXEC_MAP\n",
    "    global NODE_REGISTER\n",
    "    EXEC_MAP.clear()\n",
    "    NODE_REGISTER.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are evaluating Python functions, we need a wrapper to make them executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.041644Z",
     "start_time": "2019-11-10T17:56:54.035467Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%var check_src\n",
    "# [(\n",
    "import sys, imp\n",
    "parse_ = imp.new_module('parse_')\n",
    "\n",
    "def init_module(src):\n",
    "    with open(src) as sf:\n",
    "        exec(sf.read(), parse_.__dict__)\n",
    "\n",
    "def _check(s):\n",
    "    try:\n",
    "        parse_.main(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "import sys\n",
    "def main(args):\n",
    "    init_module(args[0])\n",
    "    if _check(args[1]):\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        sys.exit(1)\n",
    "import sys\n",
    "main(sys.argv[1:])\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.049921Z",
     "start_time": "2019-11-10T17:56:54.043479Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# [(\n",
    "with open('build/check.py', 'w+') as f:\n",
    "    print(VARS['check_src'], file=f)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small library function to convert from tuple to lists so that we can modify a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_modifiable(derivation_tree):\n",
    "    node, children, *rest = derivation_tree\n",
    "    return [node, [to_modifiable(c) for c in children], *rest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is that, sometimes one finds that our central assumption -- that a fragment consumed by a function can be replaced by another fragment consumed by the same function elsewhere -- doesn't hold. This can be seen in functions that take an additional argument to specify what it should match. In such cases, we want to try and find out how to distinguish between these function invocations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`node_include()` is a library function that checks whether the node `j` is within the boundary of `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def node_include(i, j):\n",
    "    name_i, children_i, s_i, e_i = i\n",
    "    name_j, children_j, s_j, e_j = j\n",
    "    return is_second_item_included((s_i, e_i), (s_j, e_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_ref()` takes a `node` datastructure and searches for `node_name`. It returns the first instance found. This allows us to easily swap nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ref(node, node_name):\n",
    "    name, children, *rest = node\n",
    "    if name == node_name:\n",
    "        return node\n",
    "    for child in children:\n",
    "        res = get_ref(child, node_name)\n",
    "        if res is not None: return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `replace_nodes()` function try to replace the contents of the first node with the _contents_ of the second (That is, the tree that has these nodes will automatically be modified), collect the produced string from the tree, and reset any changes. The arguments are tuples with the following format: (node, file_name, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_copy(t): # Python deepcopy is a bit buggy\n",
    "    v = json.dumps(t)\n",
    "    return json.loads(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace the given node in a2 by the node in a1\n",
    "def replace_nodes(a2, a1):\n",
    "    node2, _, t2 = a2\n",
    "    node1, _, t1 = a1\n",
    "    str2_old = tree_to_str(t2)\n",
    "\n",
    "    # first change the name of the node, then copy the tree.\n",
    "    tmpl_name = '___cmimid___'\n",
    "    old_name = node2[0]\n",
    "    node2[0] = tmpl_name\n",
    "    t2_new = deep_copy(t2)\n",
    "    node2[0] = old_name\n",
    "\n",
    "    # now find the reference to tmpl_name in t2_new\n",
    "    node2 = get_ref(t2_new, tmpl_name)\n",
    "    node2.clear()\n",
    "    for n in node1:\n",
    "        node2.append(n)\n",
    "    str2_new = tree_to_str(t2_new)\n",
    "    assert str2_old != str2_new\n",
    "    return t2_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a given node be replaced with another? The idea is, given two nodes (possibly from two trees), can the first node be replaced by the second, and still result in a valid string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_a_replaceable_with_b(a1, a2, module):\n",
    "    n1, f1, t1 = a1\n",
    "    n2, f2, t2 = a2\n",
    "    if tree_to_str(n1) == tree_to_str(n2): return True\n",
    "    t_x = replace_nodes(a1, (('XXXX', []), None, t2))\n",
    "    x = tree_to_str(t_x)\n",
    "    updated_tree = replace_nodes(a1, a2)\n",
    "    updated_string = tree_to_str(updated_tree)\n",
    "    o = tree_to_str(t1)\n",
    "    v = check(o, x, n1[0], updated_tree, module, tree_to_str(a1[0]), tree_to_str(a2[0]))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_compatible(a1, a2, module):\n",
    "    t1 = is_a_replaceable_with_b(a1, a2, module)\n",
    "    if not t1: return False\n",
    "    t2 = is_a_replaceable_with_b(a2, a1, module)\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fundamentally, two kinds of nodes. The first kind of node is a method node, that correspond to a method call. The second is a node that corresponds to a pseudo-method -- that is, a node that represents a loop or a conditional. Below are the predicates that identify such methods, parses, and reconstructs such nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_node_method(node):\n",
    "    node_name = node[0]\n",
    "    if (node_name[0], node_name[-1]) != ('<', '>'): return False\n",
    "    return not is_node_pseudo(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_node_pseudo(node):\n",
    "    node_name = node[0]\n",
    "    if (node_name[0], node_name[-1]) != ('<', '>'): return False\n",
    "    if ':if_' in node_name: return True\n",
    "    if ':while_' in node_name: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_pseudo_name(node_name):\n",
    "    assert (node_name[0], node_name[-1]) == ('<','>')\n",
    "    return decode_name(node_name[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_method_name(mname):\n",
    "    assert (mname[0], mname[-1]) == ('<', '>')\n",
    "    name = mname[1:-1]\n",
    "    if '.' in name:\n",
    "        nname, my_id = name.split('.')\n",
    "        return nname, my_id\n",
    "    else:\n",
    "        return name, '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_name(node_name_stack):\n",
    "    node_name, mstack = node_name_stack.split('#')\n",
    "    method_stack = json.loads(mstack)\n",
    "    method_ctrl_alt_name, can_empty = node_name.split(' ')\n",
    "    method, ctrl_cid_altid = method_ctrl_alt_name.split(':')\n",
    "    ctrl, cid_altid = ctrl_cid_altid.split('_')\n",
    "    assert ctrl in {'while', 'if'}\n",
    "    cid, altid = cid_altid.split(',')\n",
    "\n",
    "    if 'while' == ctrl:\n",
    "        assert altid == '0'\n",
    "    return method, ctrl, int(cid), altid, can_empty, method_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.098540Z",
     "start_time": "2019-11-10T17:56:54.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "def unparse_pseudo_name(method, ctrl, ctrl_id, alt_num, can_empty, cstack):\n",
    "    return \"<%s>\" % encode_name(method, ctrl, ctrl_id, alt_num, can_empty, cstack)\n",
    "\n",
    "def unparse_method_name(mname, my_id):\n",
    "    return '<%s.%s>' % (mname, my_id)\n",
    "\n",
    "def encode_name(method, ctrl, ctrl_id, alt_num, can_empty, stack):\n",
    "    assert ctrl in {'while', 'if'}\n",
    "    return '%s:%s_%s,%s %s#%s' % (method, ctrl, ctrl_id, alt_num, can_empty, json.dumps(stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `check()` function invokes the given subject call with the previously defined `check.py` wrapper, logs and returns the result of the call.\n",
    "\n",
    "**TODO**: What we really want to do, is to generate a new updated tree first after doing the tree surgery. Then, convert this tree to a parenthesized tree by simply doing `tree_to_string` with additional `{}` (or other open/close symbols that does not conflict with the input) attached when joining the nodes. That is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_pstr(tree, op_='', _cl=''):\n",
    "    expanded = []\n",
    "    to_expand = [tree]\n",
    "    while to_expand:\n",
    "        (key, children, *_), *to_expand = to_expand\n",
    "        if is_nt(key):\n",
    "            expanded.append(op_)\n",
    "            to_expand = children + [(_cl, [])] + to_expand\n",
    "        else:\n",
    "            assert not children\n",
    "            expanded.append(key)\n",
    "    return ''.join(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, when using `tree_to_string(my_tree, '{', '}')`, We will get a string that represents how the original string was parsed. For example `1+2+3` may be represented as `{{1+2}+3}`.\n",
    "\n",
    "Next, we need to run the non-parenthesized string resulting from the tree surgery through the program, and collect the resulting tree. Again, convert this tree to the parentesized version, and compare equality.\n",
    "\n",
    "With this, we can ensure that our tree nodes are correctly compatible, and secondly, we can ignore the return code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.126874Z",
     "start_time": "2019-11-10T17:56:54.114611Z"
    }
   },
   "outputs": [],
   "source": [
    "def check(o, x, e, ut, module, sa1, sa2):\n",
    "    s = tree_to_str(ut)\n",
    "    if s in EXEC_MAP: return EXEC_MAP[s]\n",
    "    updated_ps = tree_to_pstr(ut, op_='{', _cl='}')\n",
    "    tn = \"build/_test.csv\"\n",
    "    with open(tn, 'w+') as f: print(s, file=f)\n",
    "\n",
    "    trace_out = do([\"python3\",\"build/%(m)s\" % {'m': module}, tn] ).stdout\n",
    "    val = None\n",
    "    v = False\n",
    "    parsed_ps = None\n",
    "    try:\n",
    "        val = json.loads(trace_out)\n",
    "        parsed_tree = miner(val)[0]['tree']\n",
    "        parsed_ps = tree_to_pstr(parsed_tree, op_='{', _cl='}')\n",
    "        v = (parsed_ps == updated_ps)\n",
    "    except:\n",
    "        parsed_ps = 'ERROR'\n",
    "        v = False\n",
    "  \n",
    "    with open('%s.log' % module, 'a+') as f:\n",
    "        print('------------------', file=f)\n",
    "        print(' '.join([\"python3\", \"build/%s\" % module, s]), file=f)\n",
    "        print('Checking:',e, file=f)\n",
    "        print('original:', repr(o), file=f)\n",
    "        print('tmpl:', repr(x), file=f)\n",
    "        print('updated:', repr(s), file=f)\n",
    "        print('XXXX:', repr(sa1), file=f)\n",
    "        print('REPL:', repr(sa2), file=f)\n",
    "        print('ops:', repr(updated_ps), file=f)\n",
    "        print('pps:', repr(parsed_ps), file=f)\n",
    "        print(\":=\", v, file=f)\n",
    "    #     print(' '.join([module, repr(s)]), file=f)\n",
    "    #     print(\"\\n\", file=f)\n",
    "    # v = (result.returncode == 0)\n",
    "    EXEC_MAP[s] = v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to collect all nodes of a particular kind together. `register_node()` correctly saves specific kinds of nodes separately as copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.138832Z",
     "start_time": "2019-11-10T17:56:54.129170Z"
    }
   },
   "outputs": [],
   "source": [
    "def register_node(node, tree, executable, input_file):\n",
    "    # we want to save a copy of the tree so we can modify it later. \n",
    "    node_name = node[0]\n",
    "    template_name = '__CMIMID__NODE__'\n",
    "    node[0] = template_name\n",
    "    new_tree = deep_copy(tree)\n",
    "    node[0] = node_name\n",
    "    new_node = get_ref(new_tree, template_name)\n",
    "    new_node[0] = node_name\n",
    "    if node_name not in NODE_REGISTER:\n",
    "        NODE_REGISTER[node_name] = []\n",
    "    new_elt = (new_node, new_tree, executable, input_file,\n",
    "            {'inputstr': tree_to_str(new_tree), 'node':node, 'tree':tree})\n",
    "    NODE_REGISTER[node_name].append(new_elt)\n",
    "    return new_elt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect_nodes()` recursively calls `register_node()` on the tree so that all nodes are registered. The wrinkle here is that in some case such as parser combinators and peg parsers, there may be long chains of single child repetitions. i.e: `parse -> curry -> parse -> curry -> ...` etc. For them, if we have seen the first `parse` and `curry`, we do not gain anything by analyzing the remainign in the *same chain*. So, we mark such chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nodes_single(node, tree, executable, inputfile, seen):\n",
    "    node_name, children, si, ei = node\n",
    "    elt = None\n",
    "    if is_node_method(node):\n",
    "        elt = register_node(node, tree, executable, inputfile)\n",
    "        if node_name in seen:\n",
    "            elt[4]['seen'] = seen[node_name]\n",
    "        else:\n",
    "            seen[node_name] = elt\n",
    "    if len(children) == 1:\n",
    "        collect_nodes_single(children[0], tree, executable, inputfile, seen)\n",
    "    else:\n",
    "        # no longer the single inheritance line.\n",
    "        for child in children:\n",
    "            collect_nodes(child, tree, executable, inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.148491Z",
     "start_time": "2019-11-10T17:56:54.140829Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_nodes(node, tree, executable, inputfile):\n",
    "    node_name, children, si, ei = node\n",
    "    elt = None\n",
    "    if is_node_method(node):\n",
    "        elt = register_node(node, tree, executable, inputfile)\n",
    "    if len(children) == 1:\n",
    "        collect_nodes_single(children[0], tree, executable, inputfile, {node_name: elt})\n",
    "    else:\n",
    "        for child in children:\n",
    "            collect_nodes(child, tree, executable, inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking to see which methods are swappable, the idea is to choose a small sample set for a given node name, and check the current node against that sample set (swap both ways, and check the validity). The different validity patterns we get are marked as different kinds of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we identify the buckets based on one to one compatibility. Unfortunately, there is a problem here. Essentially, we assume that if `a` is compatible with `b`, and `b` is compatible with `c`, then `a` is compatible with `c`. However, this may not be true in all cases. See the limitations for instances when this assumption is invalidated. At this point, we have several options. The first is to do an $n \\times n$ comparison of all items in the bucket, in which case, we will have the true compatibility but with high computational cost. The next alternative is to choose a node in one bucket, and do the bucketing procedure again with the items in the particular bucket. This produces one more bit of information, and one can continue this prodcedure for larger and larger number of bits. One may also choose a statistical sample of $k$ items in the bucket, and go for a comparison only between $n \\times k$ items.\n",
    "\n",
    "At this point, we choose the fastest option, which gets us a reasonable accuracy. We use a single level classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_buckets(node_name):\n",
    "    all_elts = NODE_REGISTER[node_name]\n",
    "    # remove the duplicate nodes\n",
    "    elts = [e for e in all_elts if 'seen' not in e[4]]\n",
    "    seen_elts = [e for e in all_elts if 'seen' in e[4]]\n",
    "    first, *rest = elts\n",
    "    first[4]['pattern'] = 0\n",
    "    buckets = [first]\n",
    "    for enode in rest:\n",
    "        node0, tree0, executable0, inputfile0, _info0 = enode\n",
    "        a0 = node0, inputfile0, tree0\n",
    "        compatible = None\n",
    "        for bi, bnode in enumerate(buckets):\n",
    "            node1, tree1, executable1, inputfile1, _info1 = bnode\n",
    "            a1 = node1, inputfile1, tree1\n",
    "            result = is_compatible(a0, a1, executable0)\n",
    "            if result:\n",
    "                compatible = bi\n",
    "                enode[4]['pattern'] = bi\n",
    "                break\n",
    "        if compatible is None:\n",
    "            enode[4]['pattern'] = len(buckets)\n",
    "            buckets.append(enode)\n",
    "            \n",
    "    for e in seen_elts:\n",
    "        e_seen = e[4]['seen']\n",
    "        e_seen_pattern = e_seen[4]['pattern']\n",
    "        e[4]['pattern'] = e_seen_pattern\n",
    "    return {i:i for i,b in enumerate(buckets)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we identify that a node belongs to a particular pattern identifier, we update all the pseudo-methods belonging to that node. These can be found by simply traversiing the tree until the next method is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.181841Z",
     "start_time": "2019-11-10T17:56:54.173078Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_method_stack(node, old_name, new_name):\n",
    "    nname, children, *rest = node\n",
    "    if not (':if_' in nname or ':while_' in nname):\n",
    "        return\n",
    "    method, ctrl, cname, num, can_empty, cstack = parse_pseudo_name(nname)\n",
    "    assert method == old_name, \"%s != %s\" % (method, old_name)\n",
    "    name = unparse_pseudo_name(new_name, ctrl, cname, num, can_empty, cstack)\n",
    "    #assert '?' not in name\n",
    "    node[0] = name\n",
    "    for c in node[1]:\n",
    "        update_method_stack(c, old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.191811Z",
     "start_time": "2019-11-10T17:56:54.184055Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_method_name(k_m, my_id):\n",
    "    # fixup k_m with what is in my_id\n",
    "    original = k_m[0]\n",
    "    method, old_id = parse_method_name(original)\n",
    "    name = unparse_method_name(method, my_id)\n",
    "    k_m[0] = name\n",
    "\n",
    "    for c in k_m[1]:\n",
    "        update_method_stack(c, original[1:-1], name[1:-1])\n",
    "\n",
    "    return name, k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.201138Z",
     "start_time": "2019-11-10T17:56:54.194359Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_original_method_names(node_name):\n",
    "    registered_xnodes = NODE_REGISTER[node_name]\n",
    "    for xnode in registered_xnodes:\n",
    "        # name it according to its pattern\n",
    "        nodeX, treeX, executableX, inputfileX, infoX = xnode\n",
    "        pattern = infoX['pattern']\n",
    "        update_method_name(infoX['node'], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to first collect and register all nodes by their names.\n",
    "Next, we sample N of these, and use the pattern of matches\n",
    "(**TODO**: Do we simply use the pattern of compatibility or the pattern\n",
    "of left to right replaceability -- that is, a is replaceable with b\n",
    "but b is not replaceable with a is 10 while full compatibility would\n",
    "be 11 -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:56:54.222943Z",
     "start_time": "2019-11-10T17:56:54.213355Z"
    }
   },
   "outputs": [],
   "source": [
    "def generalize_method_trees(jtrees, log=False):\n",
    "    my_trees = []\n",
    "    for i,j in enumerate(jtrees):\n",
    "        tree = to_modifiable(j['tree']) # The tree ds.\n",
    "        executable = j['original']\n",
    "        inputfile = j['arg']\n",
    "        # we skip START\n",
    "        node_name, children, *rest = tree\n",
    "        assert node_name == '<START>'\n",
    "        for child in children:\n",
    "            collect_nodes(tree, tree, executable, inputfile)\n",
    "        my_trees.append({'tree':tree, 'original': executable, 'arg': inputfile})\n",
    "\n",
    "    for k in NODE_REGISTER:\n",
    "        identify_buckets(k)\n",
    "\n",
    "    # finally, update the original names.\n",
    "    for k in NODE_REGISTER:\n",
    "        if k == '<START>': continue\n",
    "        update_original_method_names(k)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:07.779970Z",
     "start_time": "2019-11-10T17:56:54.225184Z"
    }
   },
   "outputs": [],
   "source": [
    "%top reset_generalizer()\n",
    "%top mg_calc_trees = generalize_method_trees(mined_calc_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:10.198039Z",
     "start_time": "2019-11-10T17:57:07.782479Z"
    }
   },
   "outputs": [],
   "source": [
    "%top reset_generalizer()\n",
    "%top mg_mathexpr_trees = generalize_method_trees(mined_mathexpr_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:10.319175Z",
     "start_time": "2019-11-10T17:57:10.202645Z"
    }
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(mg_calc_trees[0]['tree']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define how to update a pseudoname to a new id (when we detect a new pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:10.380666Z",
     "start_time": "2019-11-10T17:57:10.341966Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_pseudo_name(k_m, my_id):\n",
    "    # fixup k_m with what is in my_id\n",
    "    original = k_m[0]\n",
    "    method, ctrl, cid, altid, can_empty, method_stack = parse_pseudo_name(original)\n",
    "    if ctrl == 'if':\n",
    "        name = unparse_pseudo_name(method, ctrl, cid, \"%s.%d\" % (altid, my_id), can_empty, method_stack)\n",
    "    elif ctrl == 'while':\n",
    "        assert altid == '0'\n",
    "        name = unparse_pseudo_name(method, ctrl, cid, my_id, can_empty, method_stack)\n",
    "    else:\n",
    "        assert False\n",
    "    k_m[0] = name\n",
    "    return name, k_m\n",
    "\n",
    "def update_original_pseudo_names(node_name):\n",
    "    registered_xnodes = NODE_REGISTER[node_name]\n",
    "    for xnode in registered_xnodes:\n",
    "        # name it according to its pattern\n",
    "        nodeX, treeX, executableX, inputfileX, infoX = xnode\n",
    "        pattern = infoX['pattern']\n",
    "        update_pseudo_name(infoX['node'], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generalizing pseudonodes, we need to collect them just like we did for methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:10.380666Z",
     "start_time": "2019-11-10T17:57:10.341966Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_pseudo_nodes(node, tree, executable, inputfile):\n",
    "    node_name, children, si, ei = node\n",
    "    if is_node_pseudo(node):\n",
    "        register_node(node, tree, executable, inputfile)\n",
    "\n",
    "    for child in children:\n",
    "        collect_pseudo_nodes(child, tree, executable, inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loops, we have a special processing that checks whether it can be deleted. If so, we would place `*` after their name. Else it is `+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:10.380666Z",
     "start_time": "2019-11-10T17:57:10.341966Z"
    }
   },
   "outputs": [],
   "source": [
    "def can_the_loop_be_deleted(pattern, k, executable):\n",
    "    xnodes = [xnode for xnode in NODE_REGISTER[k] if xnode[-1]['pattern'] == pattern]\n",
    "    can_be_deleted = True\n",
    "    for xnode in xnodes:\n",
    "        node0, tree0, executable0, inputfile0, _info = xnode\n",
    "        a = is_a_replaceable_with_b((node0, '', tree0), (['', [], 0, 0], '', tree0), executable)\n",
    "        if not a:\n",
    "            can_be_deleted = False\n",
    "            break\n",
    "    for xnode in xnodes:\n",
    "        node0, tree0, executable0, inputfile0, info = xnode\n",
    "        method1, ctrl1, cname1, num1, can_empty, cstack1 = parse_pseudo_name(node0[0])\n",
    "        name = unparse_pseudo_name(method1, ctrl1, cname1, num1, Epsilon if can_be_deleted else NoEpsilon, cstack1)\n",
    "        info['node'][0] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main `generalize_loop_trees()` generalizes loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:10.380666Z",
     "start_time": "2019-11-10T17:57:10.341966Z"
    }
   },
   "outputs": [],
   "source": [
    "def generalize_loop_trees(jtrees, log=False):\n",
    "    my_trees = []\n",
    "    for j in jtrees:\n",
    "        tree = to_modifiable(j['tree']) # The tree ds.\n",
    "        executable = j['original']\n",
    "        inputfile = j['arg']\n",
    "        # we skip START\n",
    "        node_name, children, *rest = tree\n",
    "        assert node_name == '<START>'\n",
    "        for child in children:\n",
    "            collect_pseudo_nodes(tree, tree, executable, inputfile)\n",
    "        my_trees.append({'tree':tree, 'original': executable, 'arg': inputfile})\n",
    "\n",
    "    for k in NODE_REGISTER:\n",
    "        patterns = identify_buckets(k)\n",
    "        for p in patterns:\n",
    "            can_the_loop_be_deleted(patterns[p], k, executable)\n",
    "\n",
    "    # finally, update the original names.\n",
    "    for k in NODE_REGISTER:\n",
    "        if k == '<START>': continue\n",
    "        update_original_pseudo_names(k)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:29.203008Z",
     "start_time": "2019-11-10T17:57:10.387748Z"
    }
   },
   "outputs": [],
   "source": [
    "%top reset_generalizer()\n",
    "%top lg_calc_trees = generalize_loop_trees(mg_calc_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:29.292490Z",
     "start_time": "2019-11-10T17:57:29.217332Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%top zoom(display_tree(lg_calc_trees[0]['tree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:29.311255Z",
     "start_time": "2019-11-10T17:57:29.305727Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarFuzzer import extract_node as extract_node_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:43.983008Z",
     "start_time": "2019-11-10T17:57:29.314882Z"
    }
   },
   "outputs": [],
   "source": [
    "%top reset_generalizer()\n",
    "%top generalized_calc_trees = generalize_loop_trees(mg_calc_trees)\n",
    "%top zoom(display_tree(generalized_calc_trees[0]['tree'], extract_node=extract_node_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.664338Z",
     "start_time": "2019-11-10T17:57:43.985786Z"
    }
   },
   "outputs": [],
   "source": [
    "%top reset_generalizer()\n",
    "%top generalized_mathexpr_trees = generalize_loop_trees(mg_mathexpr_trees)\n",
    "%top zoom(display_tree(generalized_mathexpr_trees[2]['tree'], extract_node=extract_node_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Grammar\n",
    "\n",
    "Generating a grammar from the generalized derivation trees is pretty simple. Start at the start node, and any node that represents a method or a pseudo method becomes a nonterminal. The children forms alternate expansions for the nonterminal. Since all the keys are compatible, merging the grammar is simply merging the hash map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a pretty printer for grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.684642Z",
     "start_time": "2019-11-10T17:57:46.666503Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "RE_NONTERMINAL = re.compile(r'(<[^<> ]*>)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.708593Z",
     "start_time": "2019-11-10T17:57:46.688403Z"
    }
   },
   "outputs": [],
   "source": [
    "def recurse_grammar(grammar, key, order, canonical):\n",
    "    rules = sorted(grammar[key])\n",
    "    old_len = len(order)\n",
    "    for rule in rules:\n",
    "        if not canonical:\n",
    "            res =  re.findall(RE_NONTERMINAL, rule)\n",
    "        else:\n",
    "            res = rule\n",
    "        for token in res:\n",
    "            if token.startswith('<') and token.endswith('>'):\n",
    "                if token not in order:\n",
    "                    order.append(token)\n",
    "    new = order[old_len:]\n",
    "    for ckey in new:\n",
    "        recurse_grammar(grammar, ckey, order, canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.724725Z",
     "start_time": "2019-11-10T17:57:46.717202Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_grammar(grammar, start_symbol='<START>', canonical=True):\n",
    "    order = [start_symbol]\n",
    "    recurse_grammar(grammar, start_symbol, order, canonical)\n",
    "    if len(order) != len(grammar.keys()):\n",
    "        assert len(order) < len(grammar.keys())\n",
    "    return {k: sorted(grammar[k]) for k in order}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees to grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.756022Z",
     "start_time": "2019-11-10T17:57:46.734004Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_grammar(tree, grammar):\n",
    "    node, children, _, _ = tree\n",
    "    if not children: return grammar\n",
    "    tokens = []\n",
    "    if node not in grammar:\n",
    "        grammar[node] = list()\n",
    "    for c in children:\n",
    "        tokens.append(c[0])\n",
    "        to_grammar(c, grammar)\n",
    "    grammar[node].append(tuple(tokens))\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.769563Z",
     "start_time": "2019-11-10T17:57:46.760070Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_grammar(g1, g2):\n",
    "    all_keys = set(list(g1.keys()) + list(g2.keys()))\n",
    "    merged = {}\n",
    "    for k in all_keys:\n",
    "        alts = set(g1.get(k, []) + g2.get(k, []))\n",
    "        merged[k] = alts\n",
    "    return {k:[l for l in merged[k]] for k in merged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.817096Z",
     "start_time": "2019-11-10T17:57:46.785403Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_grammar(my_trees):\n",
    "    grammar = {}\n",
    "    ret = []\n",
    "    for my_tree in my_trees:\n",
    "        tree = my_tree['tree']\n",
    "        start = tree[0]\n",
    "        src_file = my_tree['original']\n",
    "        arg_file = my_tree['arg']\n",
    "        ret.append((start, src_file, arg_file))\n",
    "        g = to_grammar(tree, grammar)\n",
    "        grammar = merge_grammar(grammar, g)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.847482Z",
     "start_time": "2019-11-10T17:57:46.825591Z"
    }
   },
   "outputs": [],
   "source": [
    "%top calc_grammar = convert_to_grammar(generalized_calc_trees)\n",
    "%top show_grammar(calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:46.868656Z",
     "start_time": "2019-11-10T17:57:46.854612Z"
    }
   },
   "outputs": [],
   "source": [
    "%top mathexpr_grammar = convert_to_grammar(generalized_mathexpr_trees)\n",
    "%top show_grammar(mathexpr_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.119084Z",
     "start_time": "2019-11-10T17:57:47.061424Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "gf = LimitFuzzer(mathexpr_grammar)\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Empty Alternatives for IF and Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to insert empty rules for those loops and conditionals that can be skipped. For loops, the entire sequence has to contain the empty marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.137813Z",
     "start_time": "2019-11-10T17:57:47.125917Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_empty_rules(grammar):\n",
    "    new_grammar = {}\n",
    "    for k in grammar:\n",
    "        if k in ':if_':\n",
    "            name, marker = k.split('#')\n",
    "            if name.endswith(' *'):\n",
    "                new_grammar[k] = grammar[k].add(('',))\n",
    "            else:\n",
    "                new_grammar[k] = grammar[k]\n",
    "        elif k in ':while_':\n",
    "            # TODO -- we have to check the rules for sequences of whiles.\n",
    "            # for now, ignore.\n",
    "            new_grammar[k] = grammar[k]\n",
    "        else:\n",
    "            new_grammar[k] = grammar[k]\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.157739Z",
     "start_time": "2019-11-10T17:57:47.140489Z"
    }
   },
   "outputs": [],
   "source": [
    "%top ne_calc_grammar = check_empty_rules(calc_grammar)\n",
    "%top show_grammar(ne_calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.247155Z",
     "start_time": "2019-11-10T17:57:47.167237Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "gf = LimitFuzzer(ne_calc_grammar)\n",
    "for i in range(10):\n",
    "    print(repr(gf.fuzz(key='<START>')))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.267363Z",
     "start_time": "2019-11-10T17:57:47.252023Z"
    }
   },
   "outputs": [],
   "source": [
    "%top ne_mathexpr_grammar = check_empty_rules(mathexpr_grammar)\n",
    "%top show_grammar(ne_mathexpr_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.321182Z",
     "start_time": "2019-11-10T17:57:47.269905Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "gf = LimitFuzzer(ne_mathexpr_grammar)\n",
    "for i in range(10):\n",
    "    print(repr(gf.fuzz(key='<START>')))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to generalize the loops. The idea is to look for patterns exclusively in the similarly named while loops using any of the regular expression learners. For the prototype, we replaced the modified Sequitur with the modified Fernau which gave us better regular expressions than before. The main constraint we have is that we want to avoid repeated execution of program if possible. Fernau algorithm can recover a reasonably approximate regular exression based only on positive data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The modified Fernau algorithm\n",
    "\n",
    "The Fernau algorithm is from _Algorithms for learning regular expressions from positive data_ by _HenningFernau_. Our algorithm uses a modified form of the Prefix-Tree-Acceptor from Fernau. First we define an LRF buffer of a given size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.333819Z",
     "start_time": "2019-11-10T17:57:47.324230Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buf:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.items = [None] * self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add1()` takes in an array, and transfers the first element of the array into the end of current buffer, and simultaneously drops the first element of the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.346280Z",
     "start_time": "2019-11-10T17:57:47.336828Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buf(Buf):\n",
    "    def add1(self, items):\n",
    "        self.items.append(items.pop(0))\n",
    "        return self.items.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For equality between the buffer and an array, we only compare when both the array and the items are actually elements and not chunked arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.358036Z",
     "start_time": "2019-11-10T17:57:47.349290Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buf(Buf):\n",
    "    def __eq__(self, items):\n",
    "        if any(isinstance(i, dict) for i in self.items): return False\n",
    "        if any(isinstance(i, dict) for i in items): return False\n",
    "        return items == self.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detect_chunks()` detects any repeating portions of a list of `n` size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.371220Z",
     "start_time": "2019-11-10T17:57:47.362193Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_chunks(n, lst_):\n",
    "    lst = list(lst_)\n",
    "    chunks = set()\n",
    "    last = Buf(n)\n",
    "    # check if the next_n elements are repeated.\n",
    "    for _ in range(len(lst) - n):\n",
    "        lnext_n = lst[0:n]\n",
    "        if last == lnext_n:\n",
    "            # found a repetition.\n",
    "            chunks.add(tuple(last.items))\n",
    "        else:\n",
    "            pass\n",
    "        last.add1(lst)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have detected plausible repeating sequences, we gather all similar sequences into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.385090Z",
     "start_time": "2019-11-10T17:57:47.374234Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunkify(lst_,n , chunks):\n",
    "    lst = list(lst_)\n",
    "    chunked_lst = []\n",
    "    while len(lst) >= n:\n",
    "        lnext_n = lst[0:n]\n",
    "        if (not any(isinstance(i, dict) for i in lnext_n)) and tuple(lnext_n) in chunks:\n",
    "            chunked_lst.append({'_':lnext_n})\n",
    "            lst = lst[n:]\n",
    "        else:\n",
    "            chunked_lst.append(lst.pop(0))\n",
    "    chunked_lst.extend(lst)\n",
    "    return chunked_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `identify_chunks()` simply calls the `detect_chunks()` on all given lists, and then converts all chunks identified into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.405031Z",
     "start_time": "2019-11-10T17:57:47.396242Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_chunks(my_lsts):\n",
    "    # initialize\n",
    "    all_chunks = {}\n",
    "    maximum = max(len(lst) for lst in my_lsts)\n",
    "    for i in range(1, maximum//2+1):\n",
    "        all_chunks[i] = set()\n",
    "\n",
    "    # First, identify chunks in each list.\n",
    "    for lst in my_lsts:\n",
    "        for i in range(1,maximum//2+1):\n",
    "            chunks = detect_chunks(i, lst)\n",
    "            all_chunks[i] |= chunks\n",
    "\n",
    "    # Then, chunkify\n",
    "    new_lsts = []\n",
    "    for lst in my_lsts:\n",
    "        for i in range(1,maximum//2+1):\n",
    "            chunks = all_chunks[i]\n",
    "            lst = chunkify(lst, i, chunks)\n",
    "        new_lsts.append(lst)\n",
    "    return new_lsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prefix tree acceptor\n",
    "\n",
    "The prefix tree acceptor is a way to represent positive data. The `Node` class holds a single node in the prefix tree acceptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.433268Z",
     "start_time": "2019-11-10T17:57:47.407536Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # Each tree node gets its unique id.\n",
    "    _uid = 0\n",
    "    def __init__(self, item):\n",
    "        # self.repeats = False\n",
    "        self.count = 1 # how many repetitions.\n",
    "        self.counters = set()\n",
    "        self.last = False\n",
    "        self.children = []\n",
    "        self.item = item\n",
    "        self.uid = Node._uid\n",
    "        Node._uid += 1\n",
    "\n",
    "    def update_counters(self):\n",
    "        self.counters.add(self.count)\n",
    "        self.count = 0\n",
    "        for c in self.children:\n",
    "            c.update_counters()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(\"(%s, [%s])\", (self.item, ' '.join([str(i) for i in self.children])))\n",
    "\n",
    "    def to_json(self):\n",
    "        s = (\"(%s)\" % ' '.join(self.item['_'])) if isinstance(self.item, dict) else str(self.item)\n",
    "        return (s, tuple(self.counters), [i.to_json() for i in self.children])\n",
    "\n",
    "    def inc_count(self):\n",
    "        self.count += 1\n",
    "\n",
    "    def add_ref(self):\n",
    "        self.count = 1\n",
    "\n",
    "    def get_child(self, c):\n",
    "        for i in self.children:\n",
    "            if i.item == c: return i\n",
    "        return None\n",
    "\n",
    "    def add_child(self, c):\n",
    "        # first check if it is the current node. If it is, increment\n",
    "        # count, and return ourselves.\n",
    "        if c == self.item:\n",
    "            self.inc_count()\n",
    "            return self\n",
    "        else:\n",
    "            # check if it is one of the children. If it is a child, then\n",
    "            # preserve its original count.\n",
    "            nc = self.get_child(c)\n",
    "            if nc is None:\n",
    "                nc = Node(c)\n",
    "                self.children.append(nc)\n",
    "            else:\n",
    "                nc.add_ref()\n",
    "            return nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update_tree()` essentially transforms a list of nodes to a chain of nodes starting at `root` if the `root` is an empty tree. If the `root` already contains a tree, the `update_tree()` traverses the path represented by `lst_` and makes a new child branch where the path specified doesn't exist in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.466927Z",
     "start_time": "2019-11-10T17:57:47.438577Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_tree(lst_, root):\n",
    "    lst = list(lst_)\n",
    "    branch = root\n",
    "    while lst:\n",
    "        first, *lst = lst\n",
    "        branch = branch.add_child(first)\n",
    "    branch.last = True\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a number of lists, the `create_tree_with_lists()` creates an actual tree out of these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.466927Z",
     "start_time": "2019-11-10T17:57:47.438577Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_tree_with_lsts(lsts):\n",
    "    Node._uid = 0\n",
    "    root =  Node(None)\n",
    "    for lst in lsts:\n",
    "        root.count = 1 # there is at least one element.\n",
    "        update_tree(lst, root)\n",
    "        root.update_counters()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a node, and a key, return the key and alts as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.466927Z",
     "start_time": "2019-11-10T17:57:47.438577Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_star(node, key):\n",
    "    if node.item is None:\n",
    "        return [], {}\n",
    "    if isinstance(node.item, dict):\n",
    "        # take care of counters\n",
    "        elements = node.item['_']\n",
    "        my_key = \"<%s-%d-s>\" % (key, node.uid)\n",
    "        alts = [elements]\n",
    "        if len(node.counters) > 1: # repetition\n",
    "            alts.append(elements + [my_key])\n",
    "        return [my_key], {my_key:alts}\n",
    "    else:\n",
    "        return [str(node.item)], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.466927Z",
     "start_time": "2019-11-10T17:57:47.438577Z"
    }
   },
   "outputs": [],
   "source": [
    "def node_to_grammar(node, grammar, key):\n",
    "    rule = []\n",
    "    alts = [rule]\n",
    "    if node.uid == 0:\n",
    "        my_key = \"<%s>\" % key\n",
    "    else:\n",
    "        my_key = \"<%s-%d>\" % (key, node.uid)\n",
    "    grammar[my_key] = alts\n",
    "    if node.item is not None:\n",
    "        mk, g = get_star(node, key)\n",
    "        rule.extend(mk)\n",
    "        grammar.update(g)\n",
    "    # is the node last?\n",
    "    if node.last:\n",
    "        assert node.item is not None\n",
    "        # add a duplicate rule that ends here.\n",
    "        ending_rule = list(rule)\n",
    "        # if there are no children, the current rule is\n",
    "        # any way ending.\n",
    "        if node.children:\n",
    "            alts.append(ending_rule)\n",
    "\n",
    "    if node.children:\n",
    "        if len(node.children) > 1:\n",
    "            my_ckey = \"<%s-%d-c>\" % (key, node.uid)\n",
    "            rule.append(my_ckey)\n",
    "            grammar[my_ckey] = [ [\"<%s-%d>\" % (key, c.uid)] for c in node.children]\n",
    "        else:\n",
    "            my_ckey = \"<%s-%d>\" % (key, node.children[0].uid)\n",
    "            rule.append(my_ckey)\n",
    "    else:\n",
    "        pass\n",
    "    for c in node.children:\n",
    "        node_to_grammar(c, grammar, key)\n",
    "    return grammar\n",
    "\n",
    "def generate_grammar(lists, key):\n",
    "    lsts = identify_chunks(lists)\n",
    "    tree = create_tree_with_lsts(lsts)\n",
    "    grammar = {}\n",
    "    node_to_grammar(tree, grammar, key)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a rule, determine the abstraction for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.476159Z",
     "start_time": "2019-11-10T17:57:47.470016Z"
    }
   },
   "outputs": [],
   "source": [
    "def collapse_alts(rules, k):\n",
    "    ss = [[str(r) for r in rule] for rule in rules]\n",
    "    x = generate_grammar(ss, k[1:-1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.491239Z",
     "start_time": "2019-11-10T17:57:47.483133Z"
    }
   },
   "outputs": [],
   "source": [
    "def collapse_rules(grammar):\n",
    "    r_grammar = {}\n",
    "    for k in grammar:\n",
    "        new_grammar = collapse_alts(grammar[k], k)\n",
    "        # merge the new_grammar with r_grammar\n",
    "        # we know none of the keys exist in r_grammar because\n",
    "        # new keys are k prefixed.\n",
    "        for k_ in new_grammar:\n",
    "            r_grammar[k_] = new_grammar[k_]\n",
    "    return r_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.509482Z",
     "start_time": "2019-11-10T17:57:47.493276Z"
    }
   },
   "outputs": [],
   "source": [
    "%top collapsed_calc_grammar = collapse_rules(ne_calc_grammar)\n",
    "%top show_grammar(collapsed_calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.578044Z",
     "start_time": "2019-11-10T17:57:47.516225Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "gf = LimitFuzzer(ne_mathexpr_grammar)\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.596605Z",
     "start_time": "2019-11-10T17:57:47.581506Z"
    }
   },
   "outputs": [],
   "source": [
    "%top collapsed_mathexpr_grammar = collapse_rules(ne_mathexpr_grammar)\n",
    "%top show_grammar(collapsed_mathexpr_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:47.886545Z",
     "start_time": "2019-11-10T17:57:47.599063Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "gf = LimitFuzzer(collapsed_mathexpr_grammar)\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:49.870934Z",
     "start_time": "2019-11-10T17:57:47.889744Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "gf = LimitFuzzer(collapsed_calc_grammar)\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:49.882123Z",
     "start_time": "2019-11-10T17:57:49.874258Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_spaces_in_keys(grammar):\n",
    "    keys = {key: key.replace(' ', '_') for key in grammar}\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        new_alt = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = []\n",
    "            for t in rule:\n",
    "                for k in keys:\n",
    "                    t = t.replace(k, keys[k])\n",
    "                new_rule.append(t)\n",
    "            new_alt.append(new_rule)\n",
    "        new_grammar[keys[key]] = new_alt\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:49.895947Z",
     "start_time": "2019-11-10T17:57:49.884497Z"
    }
   },
   "outputs": [],
   "source": [
    "%top calc_grammar = convert_spaces_in_keys(collapsed_calc_grammar)\n",
    "%top show_grammar(calc_grammar, canonical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:50.451614Z",
     "start_time": "2019-11-10T17:57:49.899852Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook import GrammarMiner, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:50.461682Z",
     "start_time": "2019-11-10T17:57:50.455278Z"
    }
   },
   "outputs": [],
   "source": [
    "%top gf = LimitFuzzer(calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.580116Z",
     "start_time": "2019-11-10T17:57:50.464688Z"
    }
   },
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%top mathexpr_grammar = convert_spaces_in_keys(collapsed_mathexpr_grammar)\n",
    "%top show_grammar(mathexpr_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:50.461682Z",
     "start_time": "2019-11-10T17:57:50.455278Z"
    }
   },
   "outputs": [],
   "source": [
    "%top gf = LimitFuzzer(mathexpr_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate and redundant entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** we indicate things that operate on canonical by _c, and those that operate on fuzzable by _f, and both by _cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.622029Z",
     "start_time": "2019-11-10T17:57:52.585760Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_in_chain(token, chain):\n",
    "    while True:\n",
    "        if token in chain:\n",
    "            token = chain[token]\n",
    "            assert isinstance(token, str)\n",
    "        else:\n",
    "            break\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new symbol for `grammar` based on `symbol_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.642997Z",
     "start_time": "2019-11-10T17:57:52.629721Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_symbol(grammar, symbol_name=\"<symbol>\"):\n",
    "    if symbol_name not in grammar:\n",
    "        return symbol_name\n",
    "\n",
    "    count = 1\n",
    "    while True:\n",
    "        tentative_symbol_name = symbol_name[:-1] + \"-\" + repr(count) + \">\"\n",
    "        if tentative_symbol_name not in grammar:\n",
    "            return tentative_symbol_name\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace keys that have a single token definition with the token in the defition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_candidate_chains(grammar, ignores):\n",
    "    to_replace = {}\n",
    "    for k in grammar:\n",
    "        if k in ignores: continue\n",
    "        if len(grammar[k]) != 1: continue\n",
    "        rule = grammar[k][0]\n",
    "        if len(rule) != 1: continue\n",
    "        if is_nt(rule[0]):\n",
    "            to_replace[k] = rule[0]\n",
    "        else:\n",
    "            pass\n",
    "    return to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_by_new_key(grammar, keys_to_replace):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = [keys_to_replace.get(token, token)\n",
    "                        for token in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[keys_to_replace.get(key, key)] = new_rules\n",
    "    assert len(grammar) == len(new_grammar)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_by_key(grammar, keys_to_replace):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        if key in keys_to_replace: continue\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            for t in rule:\n",
    "                assert isinstance(t, str)\n",
    "            new_rule = [first_in_chain(token, keys_to_replace) for token in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[key] = new_rules\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.740200Z",
     "start_time": "2019-11-10T17:57:52.725551Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_single_entries(grammar):\n",
    "    keys_to_replace = replacement_candidate_chains(grammar, {start_symbol, '<main>'})\n",
    "    return replace_key_by_key(grammar, keys_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove keys that have similar rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_duplicate_rule_keys(grammar):\n",
    "    collect = {}\n",
    "    for k in grammar:\n",
    "        salt = str(sorted(grammar[k]))\n",
    "        if salt not in collect:\n",
    "            collect[salt] = (k, set())\n",
    "        else:\n",
    "            collect[salt][1].add(k)\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.787273Z",
     "start_time": "2019-11-10T17:57:52.775510Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_rule_keys(grammar):\n",
    "    g = grammar\n",
    "    while True:\n",
    "        collect = collect_duplicate_rule_keys(g)\n",
    "        keys_to_replace = {}\n",
    "        for salt in collect:\n",
    "            k, st = collect[salt]\n",
    "            for s in st:\n",
    "                keys_to_replace[s] = k\n",
    "        if not keys_to_replace:\n",
    "            break\n",
    "        g = replace_key_by_key(g, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the control flow vestiges from names, and simply name them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.806238Z",
     "start_time": "2019-11-10T17:57:52.792077Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_replacement_keys(grammar):\n",
    "    g = deep_copy(grammar)\n",
    "    to_replace = {}\n",
    "    for k in grammar:\n",
    "        if ':' in k:\n",
    "            first, rest = k.split(':')\n",
    "            sym = new_symbol(g, symbol_name=first + '>')\n",
    "            assert sym not in g\n",
    "            g[sym] = None\n",
    "            to_replace[k] = sym\n",
    "        else:\n",
    "            continue\n",
    "    return to_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove keys that are referred to only from a single rule, and which have a single alternative.\n",
    "Import. This can't work on canonical representation. First, given a key, we figure out its distance to `<START>`.\n",
    "\n",
    "This is different from `remove_single_entries()` in that, there we do not care if the key is being used multiple times. Here, we only replace keys that are referred to only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.875404Z",
     "start_time": "2019-11-10T17:57:52.868777Z"
    }
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.887855Z",
     "start_time": "2019-11-10T17:57:52.878866Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_to_start(item, parents, start_symbol, seen=None):\n",
    "    if seen is None: seen = set()\n",
    "    if item in seen: return math.inf\n",
    "    seen.add(item)\n",
    "    if item == start_symbol: return 0\n",
    "    else: return 1 + min(len_to_start(p, parents, start_symbol, seen)\n",
    "                         for p in parents[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.897527Z",
     "start_time": "2019-11-10T17:57:52.892359Z"
    }
   },
   "outputs": [],
   "source": [
    "def order_by_length_to_start(items, parent_map, start_symbol):\n",
    "    return sorted(items, key=lambda i: len_to_start(i, parent_map, start_symbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a map of `child -> [parents]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents_of_tokens(grammar, key, seen=None, parents=None):\n",
    "    if parents is None: parents, seen = {}, set()\n",
    "    if key in seen: return parents\n",
    "    seen.add(key)\n",
    "    for res in grammar[key]:\n",
    "        for token in res:\n",
    "            if not is_nt(token): continue\n",
    "            parents.setdefault(token, []).append(key)\n",
    "    for ckey in {i for i in  grammar if i not in seen}:\n",
    "        get_parents_of_tokens(grammar, ckey, seen, parents)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(keys_to_replace):\n",
    "    to_process = list(keys_to_replace.keys())\n",
    "    updated_dict = {}\n",
    "    references = {}\n",
    "    order = []\n",
    "    while to_process:\n",
    "        key, *to_process = to_process\n",
    "        rule = keys_to_replace[key]\n",
    "        new_rule = []\n",
    "        skip = False\n",
    "        for token in rule:\n",
    "            if token not in updated_dict:\n",
    "                if token in to_process:\n",
    "                    # so this token will get defined later. We simply postpone\n",
    "                    # the processing of this key until that key is defined.\n",
    "                    # TODO: check for cycles.\n",
    "                    to_process.append(key)\n",
    "                    references.setdefault(token, set()).add(key)\n",
    "                    skip = True\n",
    "                    break\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "            else:\n",
    "                new_rule.extend(updated_dict[token])\n",
    "        if not skip:\n",
    "            order.append(key)\n",
    "            updated_dict[key] = new_rule\n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keys_by_rule(grammar, keys_to_replace):\n",
    "    # we now need to verify that none of the keys are part of the sequences.\n",
    "    keys_to_replace = remove_references(keys_to_replace)\n",
    "\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        if key in keys_to_replace: continue\n",
    "\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = []\n",
    "            for token in rule:\n",
    "                if token in keys_to_replace:\n",
    "                    new_rule.extend(keys_to_replace[token])\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[key] = new_rules\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.933626Z",
     "start_time": "2019-11-10T17:57:52.923401Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_single_alts(grammar, start_symbol):\n",
    "    single_alts = {p for p in grammar if len(grammar[p]) == 1 and p != start_symbol}\n",
    "\n",
    "    child_parent_map = get_parents_of_tokens(grammar, start_symbol)\n",
    "    assert len(child_parent_map) < len(grammar)\n",
    "\n",
    "    single_refs = {p:child_parent_map[p] for p in single_alts if len(child_parent_map[p]) <= 1}\n",
    "\n",
    "    ordered = order_by_length_to_start(single_refs, child_parent_map, start_symbol)\n",
    "\n",
    "    for p in ordered:\n",
    "        assert len(grammar[p]) == 1\n",
    "        if not isinstance(grammar[p][0], str):\n",
    "            print(p, grammar[p][0])\n",
    "\n",
    "    keys_to_replace = {p:grammar[p][0] for p in ordered}\n",
    "    g =  replace_keys_by_rule(grammar, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove similar rules from under a single key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_rule(r): return len(r)\n",
    "def len_definition(d): return sum([len_rule(r) for r in d])\n",
    "def len_grammar(g): return sum([len_definition(g[k]) for k in g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rules_in_a_key(g):\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        s = {str(r):r for r in g[k]}\n",
    "        g_[k] = list(sorted(list(s.values())))\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_gc(grammar, start_symbol):\n",
    "    def strip_key(grammar, key, order):\n",
    "        rules = sorted(grammar[key])\n",
    "        old_len = len(order)\n",
    "        for rule in rules:\n",
    "            for token in rule:\n",
    "                if is_nt(token):\n",
    "                    if token not in order:\n",
    "                        order.append(token)\n",
    "        new = order[old_len:]\n",
    "        for ckey in new:\n",
    "            strip_key(grammar, ckey, order)\n",
    "\n",
    "    order = [start_symbol]\n",
    "    strip_key(grammar, start_symbol, order)\n",
    "    assert len(order) == len(grammar.keys())\n",
    "    g = {k: sorted(grammar[k]) for k in order}\n",
    "    for k in g:\n",
    "        for r in g[k]:\n",
    "            for t in r:\n",
    "                assert isinstance(t, str)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_grammar(g, start_symbol):\n",
    "    g = grammar_gc(g, start_symbol)\n",
    "    g1 = check_empty_rules(g) # add optional rules\n",
    "    g1 = grammar_gc(g1, start_symbol)\n",
    " \n",
    "    g2 = collapse_rules(g1) # learn regex\n",
    "    g2 = grammar_gc(g2, start_symbol)\n",
    "\n",
    "    g3 = convert_spaces_in_keys(g2) # fuzzable grammar\n",
    "    g3 = grammar_gc(g3, start_symbol)\n",
    "    return g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_entry_chains(grammar, start_symbol):\n",
    "    keys_to_replace = replacement_candidate_chains(grammar, {start_symbol, '<main>'})\n",
    "    return replace_key_by_key(grammar, keys_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_token_names(grammar):\n",
    "    keys_to_replace = collect_replacement_keys(grammar)\n",
    "    g = replace_key_by_new_key(grammar, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_definitions(g):\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        rs_ = []\n",
    "        for r in g[k]:\n",
    "            assert not isinstance(r, str)\n",
    "            if len(r) == 1 and r[0] == k: continue\n",
    "            rs_.append(r)\n",
    "        g_[k] = rs_\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_grammar(e, start_symbol):\n",
    "    assert start_symbol in e\n",
    "    l = len_grammar(e)\n",
    "    diff = 1\n",
    "    while diff > 0:\n",
    "        assert start_symbol in e\n",
    "        e = remove_single_entry_chains(e, start_symbol)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_duplicate_rule_keys(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = cleanup_token_names(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_single_alts(e, start_symbol)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_duplicate_rules_in_a_key(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_self_definitions(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        l_ = len_grammar(e)\n",
    "        diff = l - l_\n",
    "        l = l_\n",
    "    e = grammar_gc(e, start_symbol)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accio Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.943772Z",
     "start_time": "2019-11-10T17:57:52.939025Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Parser import non_canonical, canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:52.978492Z",
     "start_time": "2019-11-10T17:57:52.948383Z"
    }
   },
   "outputs": [],
   "source": [
    "def accio_grammar(fname, src, samples, cache=True):\n",
    "    hash_id = hashlib.md5(json.dumps(samples).encode()).hexdigest()\n",
    "    cache_file = \"build/%s_%s_generalized_tree.json\" % (fname, hash_id)\n",
    "    if os.path.exists(cache_file) and cache:\n",
    "        with open(cache_file) as f:\n",
    "            generalized_tree = json.load(f)\n",
    "    else:\n",
    "    # regenerate the program\n",
    "        program_src[fname] = src\n",
    "        with open('subjects/%s' % fname, 'w+') as f:\n",
    "            print(src, file=f) \n",
    "        resrc = rewrite(src, fname)\n",
    "        with open('build/%s' % fname, 'w+') as f:\n",
    "            print(resrc, file=f)\n",
    "        os.makedirs('samples/%s' % fname, exist_ok=True)\n",
    "        sample_files = {(\"samples/%s/%d.csv\"%(fname,i)):s for i,s in enumerate(samples)}\n",
    "        for k in sample_files:\n",
    "            with open(k, 'w+') as f:\n",
    "                print(sample_files[k], file=f)\n",
    "\n",
    "        call_trace = []\n",
    "        for i in sample_files:\n",
    "            thash_id = hashlib.md5(json.dumps(sample_files[i]).encode()).hexdigest()\n",
    "            trace_cache_file = \"build/%s_%s_trace.json\" % (fname, thash_id)\n",
    "            if os.path.exists(trace_cache_file) and cache:\n",
    "                with open(trace_cache_file) as f:\n",
    "                    my_tree = f.read()\n",
    "            else:\n",
    "                my_tree = do([\"python3\", \"./build/%s\" % fname, i]).stdout\n",
    "                with open(trace_cache_file, 'w+') as f:\n",
    "                    print(my_tree, file=f)\n",
    "            call_trace.append(json.loads(my_tree)[0])\n",
    "\n",
    "        mined_tree = miner(call_trace)\n",
    "        reset_generalizer()\n",
    "        generalized_m_tree = generalize_method_trees(mined_tree)\n",
    "        reset_generalizer()\n",
    "        generalized_tree = generalize_loop_trees(generalized_m_tree)\n",
    "        reset_generalizer()\n",
    "        # costly data structure.\n",
    "        with open(cache_file, 'w+') as f:\n",
    "            json.dump(generalized_tree, f)\n",
    "    g0 = convert_to_grammar(generalized_tree)\n",
    "    with open('build/%s_grammar_0.json' % fname, 'w+') as f:\n",
    "        json.dump(g0, f)\n",
    "    assert '<START>' in g0\n",
    "    g = cleanup_grammar(g0, start_symbol='<START>')\n",
    "    with open('build/%s_grammar_first.json' % fname, 'w+') as f:\n",
    "        json.dump(g, f)\n",
    "    g = compact_grammar(g, start_symbol='<START>')\n",
    "    with open('build/%s_grammar.json' % fname, 'w+') as f:\n",
    "        json.dump(g, f)\n",
    "    return show_grammar(non_canonical(g), canonical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_grammar_cf(g):\n",
    "    for k in sorted(g):\n",
    "        print(k, \"\\n\\t\", list(sorted(g[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:54.478334Z",
     "start_time": "2019-11-10T17:57:52.983131Z"
    }
   },
   "outputs": [],
   "source": [
    "%top  calc_grammar = accio_grammar('calculator.py', VARS['calc_src'], ['(1+2)-2', '11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:54.508763Z",
     "start_time": "2019-11-10T17:57:54.486781Z"
    }
   },
   "outputs": [],
   "source": [
    "%top display_grammar_cf(calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T17:57:54.524542Z",
     "start_time": "2019-11-10T17:57:54.512016Z"
    }
   },
   "outputs": [],
   "source": [
    "%top gf = LimitFuzzer(canonical(calc_grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%top\n",
    "# [(\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a few instrumented supporting libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIO replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": [
     "#myio_src"
    ]
   },
   "outputs": [],
   "source": [
    "%%var myio_src\n",
    "# [(\n",
    "r\"\"\"File-like objects that read from or write to a string buffer.\n",
    "\n",
    "This implements (nearly) all stdio methods.\n",
    "\n",
    "f = StringIO()      # ready for writing\n",
    "f = StringIO(buf)   # ready for reading\n",
    "f.close()           # explicitly release resources held\n",
    "flag = f.isatty()   # always false\n",
    "pos = f.tell()      # get current position\n",
    "f.seek(pos)         # set current position\n",
    "f.seek(pos, mode)   # mode 0: absolute; 1: relative; 2: relative to EOF\n",
    "buf = f.read()      # read until EOF\n",
    "buf = f.read(n)     # read up to n bytes\n",
    "buf = f.readline()  # read until end of line ('\\n') or EOF\n",
    "list = f.readlines()# list of f.readline() results until EOF\n",
    "f.truncate([size])  # truncate file at to at most size (default: current pos)\n",
    "f.write(buf)        # write at current position\n",
    "f.writelines(list)  # for line in list: f.write(line)\n",
    "f.getvalue()        # return whole file's contents as a string\n",
    "\n",
    "Notes:\n",
    "- Using a real file is often faster (but less convenient).\n",
    "- There's also a much faster implementation in C, called cStringIO, but\n",
    "  it's not subclassable.\n",
    "- fileno() is left unimplemented so that code which uses it triggers\n",
    "  an exception early.\n",
    "- Seeking far beyond EOF and then writing will insert real null\n",
    "  bytes that occupy space in the buffer.\n",
    "- There's a simple test set (see end of this file).\n",
    "\"\"\"\n",
    "try:\n",
    "    from errno import EINVAL\n",
    "except ImportError:\n",
    "    EINVAL = 22\n",
    "\n",
    "__all__ = [\"StringIO\"]\n",
    "\n",
    "def _complain_ifclosed(closed):\n",
    "    if closed:\n",
    "        raise ValueError(\"I/O operation on closed file\")\n",
    "\n",
    "class StringIO:\n",
    "    \"\"\"class StringIO([buffer])\n",
    "\n",
    "    When a StringIO object is created, it can be initialized to an existing\n",
    "    string by passing the string to the constructor. If no string is given,\n",
    "    the StringIO will start empty.\n",
    "\n",
    "    The StringIO object can accept either Unicode or 8-bit strings, but\n",
    "    mixing the two may take some care. If both are used, 8-bit strings that\n",
    "    cannot be interpreted as 7-bit ASCII (that use the 8th bit) will cause\n",
    "    a UnicodeError to be raised when getvalue() is called.\n",
    "    \"\"\"\n",
    "    def __init__(self, buf = ''):\n",
    "        # Force self.buf to be a string or unicode\n",
    "        if not isinstance(buf, str):\n",
    "            buf = str(buf)\n",
    "        self.buf = buf\n",
    "        self.len = len(buf)\n",
    "        self.buflist = []\n",
    "        self.pos = 0\n",
    "        self.closed = False\n",
    "        self.softspace = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"A file object is its own iterator, for example iter(f) returns f\n",
    "        (unless f is closed). When a file is used as an iterator, typically\n",
    "        in a for loop (for example, for line in f: print line), the next()\n",
    "        method is called repeatedly. This method returns the next input line,\n",
    "        or raises StopIteration when EOF is hit.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        r = self.readline()\n",
    "        if not r:\n",
    "            raise StopIteration\n",
    "        return r\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Free the memory buffer.\n",
    "        \"\"\"\n",
    "        if not self.closed:\n",
    "            self.closed = True\n",
    "            del self.buf, self.pos\n",
    "\n",
    "    def isatty(self):\n",
    "        \"\"\"Returns False because StringIO objects are not connected to a\n",
    "        tty-like device.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        return False\n",
    "\n",
    "    def seek(self, pos, mode = 0):\n",
    "        \"\"\"Set the file's current position.\n",
    "\n",
    "        The mode argument is optional and defaults to 0 (absolute file\n",
    "        positioning); other values are 1 (seek relative to the current\n",
    "        position) and 2 (seek relative to the file's end).\n",
    "\n",
    "        There is no return value.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        if self.buflist:\n",
    "            self.buf += ''.join(self.buflist)\n",
    "            self.buflist = []\n",
    "        if mode == 1:\n",
    "            pos += self.pos\n",
    "        elif mode == 2:\n",
    "            pos += self.len\n",
    "        self.pos = max(0, pos)\n",
    "\n",
    "    def tell(self):\n",
    "        \"\"\"Return the file's current position.\"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        return self.pos\n",
    "\n",
    "    def read(self, n = -1):\n",
    "        \"\"\"Read at most size bytes from the file\n",
    "        (less if the read hits EOF before obtaining size bytes).\n",
    "\n",
    "        If the size argument is negative or omitted, read all data until EOF\n",
    "        is reached. The bytes are returned as a string object. An empty\n",
    "        string is returned when EOF is encountered immediately.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        if self.buflist:\n",
    "            self.buf += ''.join(self.buflist)\n",
    "            self.buflist = []\n",
    "        if n is None or n < 0:\n",
    "            newpos = self.len\n",
    "        else:\n",
    "            newpos = min(self.pos+n, self.len)\n",
    "        r = self.buf[self.pos:newpos]\n",
    "        self.pos = newpos\n",
    "        return r\n",
    "\n",
    "    def readline(self, length=None):\n",
    "        r\"\"\"Read one entire line from the file.\n",
    "\n",
    "        A trailing newline character is kept in the string (but may be absent\n",
    "        when a file ends with an incomplete line). If the size argument is\n",
    "        present and non-negative, it is a maximum byte count (including the\n",
    "        trailing newline) and an incomplete line may be returned.\n",
    "\n",
    "        An empty string is returned only when EOF is encountered immediately.\n",
    "\n",
    "        Note: Unlike stdio's fgets(), the returned string contains null\n",
    "        characters ('\\0') if they occurred in the input.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        if self.buflist:\n",
    "            self.buf += ''.join(self.buflist)\n",
    "            self.buflist = []\n",
    "        i = self.buf.find('\\n', self.pos)\n",
    "        if i < 0:\n",
    "            newpos = self.len\n",
    "        else:\n",
    "            newpos = i+1\n",
    "        if length is not None and length > 0:\n",
    "            if self.pos + length < newpos:\n",
    "                newpos = self.pos + length\n",
    "        r = self.buf[self.pos:newpos]\n",
    "        self.pos = newpos\n",
    "        return r\n",
    "\n",
    "    def readlines(self, sizehint = 0):\n",
    "        \"\"\"Read until EOF using readline() and return a list containing the\n",
    "        lines thus read.\n",
    "\n",
    "        If the optional sizehint argument is present, instead of reading up\n",
    "        to EOF, whole lines totalling approximately sizehint bytes (or more\n",
    "        to accommodate a final whole line).\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        lines = []\n",
    "        line = self.readline()\n",
    "        while line:\n",
    "            lines.append(line)\n",
    "            total += len(line)\n",
    "            if 0 < sizehint <= total:\n",
    "                break\n",
    "            line = self.readline()\n",
    "        return lines\n",
    "\n",
    "    def truncate(self, size=None):\n",
    "        \"\"\"Truncate the file's size.\n",
    "\n",
    "        If the optional size argument is present, the file is truncated to\n",
    "        (at most) that size. The size defaults to the current position.\n",
    "        The current file position is not changed unless the position\n",
    "        is beyond the new file size.\n",
    "\n",
    "        If the specified size exceeds the file's current size, the\n",
    "        file remains unchanged.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        if size is None:\n",
    "            size = self.pos\n",
    "        elif size < 0:\n",
    "            raise IOError(EINVAL, \"Negative size not allowed\")\n",
    "        elif size < self.pos:\n",
    "            self.pos = size\n",
    "        self.buf = self.getvalue()[:size]\n",
    "        self.len = size\n",
    "\n",
    "    def write(self, s):\n",
    "        \"\"\"Write a string to the file.\n",
    "\n",
    "        There is no return value.\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "        if not s: return\n",
    "        # Force s to be a string or unicode\n",
    "        if not isinstance(s, str):\n",
    "            s = str(s)\n",
    "        spos = self.pos\n",
    "        slen = self.len\n",
    "        if spos == slen:\n",
    "            self.buflist.append(s)\n",
    "            self.len = self.pos = spos + len(s)\n",
    "            return\n",
    "        if spos > slen:\n",
    "            self.buflist.append('\\0'*(spos - slen))\n",
    "            slen = spos\n",
    "        newpos = spos + len(s)\n",
    "        if spos < slen:\n",
    "            if self.buflist:\n",
    "                self.buf += ''.join(self.buflist)\n",
    "            self.buflist = [self.buf[:spos], s, self.buf[newpos:]]\n",
    "            self.buf = ''\n",
    "            if newpos > slen:\n",
    "                slen = newpos\n",
    "        else:\n",
    "            self.buflist.append(s)\n",
    "            slen = newpos\n",
    "        self.len = slen\n",
    "        self.pos = newpos\n",
    "\n",
    "    def writelines(self, iterable):\n",
    "        \"\"\"Write a sequence of strings to the file. The sequence can be any\n",
    "        iterable object producing strings, typically a list of strings. There\n",
    "        is no return value.\n",
    "\n",
    "        (The name is intended to match readlines(); writelines() does not add\n",
    "        line separators.)\n",
    "        \"\"\"\n",
    "        write = self.write\n",
    "        for line in iterable:\n",
    "            write(line)\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Flush the internal buffer\n",
    "        \"\"\"\n",
    "        _complain_ifclosed(self.closed)\n",
    "\n",
    "    def getvalue(self):\n",
    "        \"\"\"\n",
    "        Retrieve the entire contents of the \"file\" at any time before\n",
    "        the StringIO object's close() method is called.\n",
    "\n",
    "        The StringIO object can accept either Unicode or 8-bit strings,\n",
    "        but mixing the two may take some care. If both are used, 8-bit\n",
    "        strings that cannot be interpreted as 7-bit ASCII (that use the\n",
    "        8th bit) will cause a UnicodeError to be raised when getvalue()\n",
    "        is called.\n",
    "        \"\"\"\n",
    "        if self.buflist:\n",
    "            self.buf += ''.join(self.buflist)\n",
    "            self.buflist = []\n",
    "        return self.buf\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": [
     "#myio_src"
    ]
   },
   "outputs": [],
   "source": [
    "# [(\n",
    "with open('build/myio.py', 'w+') as f:\n",
    "    print(VARS['myio_src'], file=f)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShLex Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%var mylex_src\n",
    "# [(\n",
    "\"\"\"A lexical analyzer class for simple shell-like syntaxes.\"\"\"\n",
    "\n",
    "# Module and documentation by Eric S. Raymond, 21 Dec 1998\n",
    "# Input stacking and error message cleanup added by ESR, March 2000\n",
    "# push_source() and pop_source() made explicit by ESR, January 2001.\n",
    "# Posix compliance, split(), string arguments, and\n",
    "# iterator interface by Gustavo Niemeyer, April 2003.\n",
    "# changes to tokenize more like Posix shells by Vinay Sajip, July 2016.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "from myio import StringIO\n",
    "\n",
    "__all__ = [\"shlex\", \"split\", \"quote\"]\n",
    "\n",
    "class shlex:\n",
    "    \"A lexical analyzer class for simple shell-like syntaxes.\"\n",
    "    def __init__(self, instream=None, infile=None, posix=False,\n",
    "                 punctuation_chars=False):\n",
    "        if isinstance(instream, str):\n",
    "            instream = StringIO(instream)\n",
    "        if instream is not None:\n",
    "            self.instream = instream\n",
    "            self.infile = infile\n",
    "        else:\n",
    "            self.instream = sys.stdin\n",
    "            self.infile = None\n",
    "        self.posix = posix\n",
    "        if posix:\n",
    "            self.eof = None\n",
    "        else:\n",
    "            self.eof = ''\n",
    "        self.commenters = '#'\n",
    "        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\n",
    "                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\n",
    "        if self.posix:\n",
    "            self.wordchars += ('ßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿ'\n",
    "                               'ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝÞ')\n",
    "        self.whitespace = ' \\t\\r\\n'\n",
    "        self.whitespace_split = False\n",
    "        self.quotes = '\\'\"'\n",
    "        self.escape = '\\\\'\n",
    "        self.escapedquotes = '\"'\n",
    "        self.state = ' '\n",
    "        self.pushback = deque()\n",
    "        self.lineno = 1\n",
    "        self.debug = 0\n",
    "        self.token = ''\n",
    "        self.filestack = deque()\n",
    "        self.source = None\n",
    "        if not punctuation_chars:\n",
    "            punctuation_chars = ''\n",
    "        elif punctuation_chars is True:\n",
    "            punctuation_chars = '();<>|&'\n",
    "        self.punctuation_chars = punctuation_chars\n",
    "        if punctuation_chars:\n",
    "            # _pushback_chars is a push back queue used by lookahead logic\n",
    "            self._pushback_chars = deque()\n",
    "            # these chars added because allowed in file names, args, wildcards\n",
    "            self.wordchars += '~-./*?='\n",
    "            #remove any punctuation chars from wordchars\n",
    "            t = self.wordchars.maketrans(dict.fromkeys(punctuation_chars))\n",
    "            self.wordchars = self.wordchars.translate(t)\n",
    "\n",
    "    def push_token(self, tok):\n",
    "        \"Push a token onto the stack popped by the get_token method\"\n",
    "        if self.debug >= 1:\n",
    "            print(\"shlex: pushing token \" + repr(tok))\n",
    "        self.pushback.appendleft(tok)\n",
    "\n",
    "    def push_source(self, newstream, newfile=None):\n",
    "        \"Push an input source onto the lexer's input source stack.\"\n",
    "        if isinstance(newstream, str):\n",
    "            newstream = StringIO(newstream)\n",
    "        self.filestack.appendleft((self.infile, self.instream, self.lineno))\n",
    "        self.infile = newfile\n",
    "        self.instream = newstream\n",
    "        self.lineno = 1\n",
    "        if self.debug:\n",
    "            if newfile is not None:\n",
    "                print('shlex: pushing to file %s' % (self.infile,))\n",
    "            else:\n",
    "                print('shlex: pushing to stream %s' % (self.instream,))\n",
    "\n",
    "    def pop_source(self):\n",
    "        \"Pop the input source stack.\"\n",
    "        self.instream.close()\n",
    "        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n",
    "        if self.debug:\n",
    "            print('shlex: popping to %s, line %d' \\\n",
    "                  % (self.instream, self.lineno))\n",
    "        self.state = ' '\n",
    "\n",
    "    def get_token(self):\n",
    "        \"Get a token from the input stream (or from stack if it's nonempty)\"\n",
    "        if self.pushback:\n",
    "            tok = self.pushback.popleft()\n",
    "            if self.debug >= 1:\n",
    "                print(\"shlex: popping token \" + repr(tok))\n",
    "            return tok\n",
    "        # No pushback.  Get a token.\n",
    "        raw = self.read_token()\n",
    "        # Handle inclusions\n",
    "        if self.source is not None:\n",
    "            while raw == self.source:\n",
    "                spec = self.sourcehook(self.read_token())\n",
    "                if spec:\n",
    "                    (newfile, newstream) = spec\n",
    "                    self.push_source(newstream, newfile)\n",
    "                raw = self.get_token()\n",
    "        # Maybe we got EOF instead?\n",
    "        while raw == self.eof:\n",
    "            if not self.filestack:\n",
    "                return self.eof\n",
    "            else:\n",
    "                self.pop_source()\n",
    "                raw = self.get_token()\n",
    "        # Neither inclusion nor EOF\n",
    "        if self.debug >= 1:\n",
    "            if raw != self.eof:\n",
    "                print(\"shlex: token=\" + repr(raw))\n",
    "            else:\n",
    "                print(\"shlex: token=EOF\")\n",
    "        return raw\n",
    "\n",
    "    def read_token(self):\n",
    "        quoted = False\n",
    "        escapedstate = ' '\n",
    "        while True:\n",
    "            if self.punctuation_chars and self._pushback_chars:\n",
    "                nextchar = self._pushback_chars.pop()\n",
    "            else:\n",
    "                nextchar = self.instream.read(1)\n",
    "            if nextchar == '\\n':\n",
    "                self.lineno += 1\n",
    "            if self.debug >= 3:\n",
    "                print(\"shlex: in state %r I see character: %r\" % (self.state,\n",
    "                                                                  nextchar))\n",
    "            if self.state is None:\n",
    "                self.token = ''        # past end of file\n",
    "                break\n",
    "            elif self.state == ' ':\n",
    "                if not nextchar:\n",
    "                    self.state = None  # end of file\n",
    "                    break\n",
    "                elif nextchar in self.whitespace:\n",
    "                    if self.debug >= 2:\n",
    "                        print(\"shlex: I see whitespace in whitespace state\")\n",
    "                    if self.token or (self.posix and quoted):\n",
    "                        break   # emit current token\n",
    "                    else:\n",
    "                        continue\n",
    "                elif nextchar in self.commenters:\n",
    "                    self.instream.readline()\n",
    "                    self.lineno += 1\n",
    "                elif self.posix and nextchar in self.escape:\n",
    "                    escapedstate = 'a'\n",
    "                    self.state = nextchar\n",
    "                elif nextchar in self.wordchars:\n",
    "                    self.token = nextchar\n",
    "                    self.state = 'a'\n",
    "                elif nextchar in self.punctuation_chars:\n",
    "                    self.token = nextchar\n",
    "                    self.state = 'c'\n",
    "                elif nextchar in self.quotes:\n",
    "                    if not self.posix:\n",
    "                        self.token = nextchar\n",
    "                    self.state = nextchar\n",
    "                elif self.whitespace_split:\n",
    "                    self.token = nextchar\n",
    "                    self.state = 'a'\n",
    "                else:\n",
    "                    self.token = nextchar\n",
    "                    if self.token or (self.posix and quoted):\n",
    "                        break   # emit current token\n",
    "                    else:\n",
    "                        continue\n",
    "            elif self.state in self.quotes:\n",
    "                quoted = True\n",
    "                if not nextchar:      # end of file\n",
    "                    if self.debug >= 2:\n",
    "                        print(\"shlex: I see EOF in quotes state\")\n",
    "                    # XXX what error should be raised here?\n",
    "                    raise ValueError(\"No closing quotation\")\n",
    "                if nextchar == self.state:\n",
    "                    if not self.posix:\n",
    "                        self.token += nextchar\n",
    "                        self.state = ' '\n",
    "                        break\n",
    "                    else:\n",
    "                        self.state = 'a'\n",
    "                elif (self.posix and nextchar in self.escape and self.state\n",
    "                      in self.escapedquotes):\n",
    "                    escapedstate = self.state\n",
    "                    self.state = nextchar\n",
    "                else:\n",
    "                    self.token += nextchar\n",
    "            elif self.state in self.escape:\n",
    "                if not nextchar:      # end of file\n",
    "                    if self.debug >= 2:\n",
    "                        print(\"shlex: I see EOF in escape state\")\n",
    "                    # XXX what error should be raised here?\n",
    "                    raise ValueError(\"No escaped character\")\n",
    "                # In posix shells, only the quote itself or the escape\n",
    "                # character may be escaped within quotes.\n",
    "                if (escapedstate in self.quotes and\n",
    "                        nextchar != self.state and nextchar != escapedstate):\n",
    "                    self.token += self.state\n",
    "                self.token += nextchar\n",
    "                self.state = escapedstate\n",
    "            elif self.state in ('a', 'c'):\n",
    "                if not nextchar:\n",
    "                    self.state = None   # end of file\n",
    "                    break\n",
    "                elif nextchar in self.whitespace:\n",
    "                    if self.debug >= 2:\n",
    "                        print(\"shlex: I see whitespace in word state\")\n",
    "                    self.state = ' '\n",
    "                    if self.token or (self.posix and quoted):\n",
    "                        break   # emit current token\n",
    "                    else:\n",
    "                        continue\n",
    "                elif nextchar in self.commenters:\n",
    "                    self.instream.readline()\n",
    "                    self.lineno += 1\n",
    "                    if self.posix:\n",
    "                        self.state = ' '\n",
    "                        if self.token or (self.posix and quoted):\n",
    "                            break   # emit current token\n",
    "                        else:\n",
    "                            continue\n",
    "                elif self.state == 'c':\n",
    "                    if nextchar in self.punctuation_chars:\n",
    "                        self.token += nextchar\n",
    "                    else:\n",
    "                        if nextchar not in self.whitespace:\n",
    "                            self._pushback_chars.append(nextchar)\n",
    "                        self.state = ' '\n",
    "                        break\n",
    "                elif self.posix and nextchar in self.quotes:\n",
    "                    self.state = nextchar\n",
    "                elif self.posix and nextchar in self.escape:\n",
    "                    escapedstate = 'a'\n",
    "                    self.state = nextchar\n",
    "                elif (nextchar in self.wordchars or nextchar in self.quotes\n",
    "                      or self.whitespace_split):\n",
    "                    self.token += nextchar\n",
    "                else:\n",
    "                    if self.punctuation_chars:\n",
    "                        self._pushback_chars.append(nextchar)\n",
    "                    else:\n",
    "                        self.pushback.appendleft(nextchar)\n",
    "                    if self.debug >= 2:\n",
    "                        print(\"shlex: I see punctuation in word state\")\n",
    "                    self.state = ' '\n",
    "                    if self.token or (self.posix and quoted):\n",
    "                        break   # emit current token\n",
    "                    else:\n",
    "                        continue\n",
    "        result = self.token\n",
    "        self.token = ''\n",
    "        if self.posix and not quoted and result == '':\n",
    "            result = None\n",
    "        if self.debug > 1:\n",
    "            if result:\n",
    "                print(\"shlex: raw token=\" + repr(result))\n",
    "            else:\n",
    "                print(\"shlex: raw token=EOF\")\n",
    "        return result\n",
    "\n",
    "    def sourcehook(self, newfile):\n",
    "        \"Hook called on a filename to be sourced.\"\n",
    "        if newfile[0] == '\"':\n",
    "            newfile = newfile[1:-1]\n",
    "        # This implements cpp-like semantics for relative-path inclusion.\n",
    "        if isinstance(self.infile, str) and not os.path.isabs(newfile):\n",
    "            newfile = os.path.join(os.path.dirname(self.infile), newfile)\n",
    "        return (newfile, open(newfile, \"r\"))\n",
    "\n",
    "    def error_leader(self, infile=None, lineno=None):\n",
    "        \"Emit a C-compiler-like, Emacs-friendly error-message leader.\"\n",
    "        if infile is None:\n",
    "            infile = self.infile\n",
    "        if lineno is None:\n",
    "            lineno = self.lineno\n",
    "        return \"\\\"%s\\\", line %d: \" % (infile, lineno)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        token = self.get_token()\n",
    "        if token == self.eof:\n",
    "            raise StopIteration\n",
    "        return token\n",
    "\n",
    "def split(s, comments=False, posix=True):\n",
    "    lex = shlex(s, posix=posix)\n",
    "    lex.whitespace_split = True\n",
    "    if not comments:\n",
    "        lex.commenters = ''\n",
    "    return list(lex)\n",
    "\n",
    "\n",
    "_find_unsafe = re.compile(r'[^\\w@%+=:,./-]', re.ASCII).search\n",
    "\n",
    "def quote(s):\n",
    "    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"\n",
    "    if not s:\n",
    "        return \"''\"\n",
    "    if _find_unsafe(s) is None:\n",
    "        return s\n",
    "\n",
    "    # use single quotes, and put single quotes into double quotes\n",
    "    # the string $'b is then quoted as '$'\"'\"'b'\n",
    "    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"\n",
    "\n",
    "\n",
    "def _print_tokens(lexer):\n",
    "    while 1:\n",
    "        tt = lexer.get_token()\n",
    "        if not tt:\n",
    "            break\n",
    "        print(\"Token: \" + repr(tt))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# [(\n",
    "with open('build/mylex.py', 'w+') as f:\n",
    "    print(VARS['mylex_src'], file=f)\n",
    "# )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile('json.tar.gz') # for microjson validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Precision = 1000\n",
    "Max_Recall = 1000\n",
    "Autogram = {}\n",
    "AutogramFuzz = {}\n",
    "AutogramGrammar = {}\n",
    "Mimid = {}\n",
    "MimidFuzz = {}\n",
    "MimidGrammar = {}\n",
    "MaxTimeout = 60*60*24 # 2 day\n",
    "MaxParseTimeout = 60*5\n",
    "CHECK = {'cgidecode','calculator', 'mathexpr', 'urlparse', 'microjson', 'sexpr'}\n",
    "reset_generalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar_with_taints(name, src, samples):\n",
    "    header = '''\n",
    "import fuzzingbook.GrammarMiner\n",
    "from fuzzingbook.GrammarMiner import Tracer\n",
    "from fuzzingbook.InformationFlow import ostr\n",
    "from fuzzingbook.GrammarMiner import TaintedScopedGrammarMiner as TSGM\n",
    "from fuzzingbook.GrammarMiner import readable\n",
    "\n",
    "import subjects.autogram_%s\n",
    "import fuzzingbook\n",
    "\n",
    "class ostr_new(ostr):\n",
    "    def __new__(cls, value, *args, **kw):\n",
    "        return str.__new__(cls, value)\n",
    "\n",
    "    def __init__(self, value, taint=None, origin=None, **kwargs):\n",
    "        self.taint = taint\n",
    "\n",
    "        if origin is None:\n",
    "            origin = ostr.DEFAULT_ORIGIN\n",
    "        if isinstance(origin, int):\n",
    "            self.origin = list(range(origin, origin + len(self)))\n",
    "        else:\n",
    "            self.origin = origin\n",
    "        #assert len(self.origin) == len(self) <-- bug fix here.\n",
    "\n",
    "class ostr_new(ostr_new):\n",
    "    def create(self, res, origin=None):\n",
    "        return ostr_new(res, taint=self.taint, origin=origin)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # bugfix here.\n",
    "        return str.__repr__(self)\n",
    " \n",
    "def recover_grammar_with_taints(fn, inputs, **kwargs):\n",
    "    miner = TSGM()\n",
    "    for inputstr in inputs:\n",
    "        with Tracer(ostr_new(inputstr), **kwargs) as tracer:\n",
    "            fn(tracer.my_input)\n",
    "        for k in tracer.trace:\n",
    "            k[2].method = k[2].method.replace('<lambda>', 'lambda')\n",
    "        miner.update_grammar(tracer.my_input, tracer.trace)\n",
    "    return readable(miner.clean_grammar())\n",
    "\n",
    "def replaceAngular(grammar):\n",
    "    # special handling for Autogram because it does not look for <> and </>\n",
    "    # in rules, which messes up with parsing.\n",
    "    new_g = {}\n",
    "    replaced = False\n",
    "    for k in grammar:\n",
    "        new_rules = []\n",
    "        for rule in grammar[k]:\n",
    "            new_rule = rule.replace('<>', '<openA><closeA>').replace('</>', '<openA>/<closeA>').replace('<lambda>','<openA>lambda<closeA>')\n",
    "            if rule != new_rule:\n",
    "                replaced = True\n",
    "            new_rules.append(new_rule)\n",
    "        new_g[k] = new_rules\n",
    "    if replaced:\n",
    "        new_g['<openA>'] = ['<']\n",
    "        new_g['<closeA>'] = ['<']\n",
    "    return new_g\n",
    "def replace_start(grammar):\n",
    "    assert '<start>' in grammar\n",
    "    start = grammar['<start>']\n",
    "    del grammar['<start>']\n",
    "    grammar['<START>'] = start\n",
    "    return replaceAngular(grammar)\n",
    "    \n",
    "samples = [i.strip() for i in [\n",
    "%s\n",
    "] if i.strip()]\n",
    "import json\n",
    "autogram_grammar_t = recover_grammar_with_taints(subjects.autogram_%s.main, samples)\n",
    "print(json.dumps(replace_start(autogram_grammar_t)))\n",
    "'''\n",
    "    mod_name = name.replace('.py','')\n",
    "    with open('./subjects/autogram_%s' % name, 'w+') as f:\n",
    "        print(src, file=f)\n",
    " \n",
    "    with open('./build/autogram_%s' % name, 'w+') as f:\n",
    "        print(header % (mod_name, ',\\n'.join([repr(i) for i in samples]), mod_name), file=f)\n",
    "  \n",
    "    with ExpectTimeout(MaxTimeout):\n",
    "        result = do([\"python3\",\"./build/autogram_%s\" % name], env={'PYTHONPATH':'.'}, log=True)\n",
    "        if result.stderr.strip():\n",
    "            print(result.stderr, file=sys.stderr)\n",
    "        assert isinstance(result.stdout, str)\n",
    "        print(result.stdout)\n",
    "        return show_grammar(json.loads(result.stdout), canonical=False)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Parser import IterativeEarleyParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Recall\n",
    "\n",
    "How many of the *valid* inputs from the golden grammar can be recognized by a parser using our grammar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recall(golden_grammar, my_grammar, validator, maximum=Max_Recall, start_symbol='<START>', log=False):\n",
    "    my_count = maximum\n",
    "    ie = IterativeEarleyParser(my_grammar, start_symbol=start_symbol)\n",
    "    golden = LimitFuzzer(canonical(golden_grammar))\n",
    "    success = 0\n",
    "    while my_count != 0:\n",
    "        src = golden.fuzz(key=start_symbol)\n",
    "        try:\n",
    "            validator(src)\n",
    "            my_count -= 1\n",
    "            try:\n",
    "                # print('?', repr(src), file=sys.stderr)\n",
    "                for tree in ie.parse(src):\n",
    "                    success += 1\n",
    "                    break\n",
    "                if log: print(maximum - my_count, '+', repr(src), success, file=sys.stderr)\n",
    "            except:\n",
    "                #print(\"Error:\", sys.exc_info()[0], file=sys.stderr)\n",
    "                if log: print(maximum - my_count, '-', repr(src), file=sys.stderr)\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    return (success, maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Precision\n",
    "How many of the inputs produced using our grammar are valid? (Accepted by the program)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_it(s, module):\n",
    "    if s in EXEC_MAP: return EXEC_MAP[s]\n",
    "    result = do([\"python3\", \"./build/check.py\", \"subjects/%s\" % module, s], shell=False)\n",
    "    with open('build/%s.log' % module, 'a+') as f:\n",
    "        print('------------------', file=f)\n",
    "        print(' '.join([\"python3\",\"./build/check.py\", \"subjects/%s\" % module, s]), file=f)\n",
    "        print(\":=\", result.returncode, file=f)\n",
    "        print(\"\\n\", file=f)\n",
    "        f.flush()\n",
    "    v = (result.returncode == 0)\n",
    "    EXEC_MAP[s] = v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision(name, grammar, maximum=Max_Precision, start_symbol='<START>', log=False):\n",
    "    success = 0\n",
    "    with ExpectError():\n",
    "        fuzzer = LimitFuzzer(canonical(grammar))\n",
    "        for i in range(maximum):\n",
    "            v = fuzzer.fuzz(key=start_symbol)\n",
    "            c = check_it(v, name)\n",
    "            success += (1 if c else 0)\n",
    "            if log: print(i, repr(v), c)\n",
    "    return (success, maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class timeit():\n",
    "    def __enter__(self):\n",
    "        self.tic = datetime.now()\n",
    "        return self\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.delta = datetime.now() - self.tic\n",
    "        self.runtime = (self.delta.microseconds, self.delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.ExpectError import ExpectError, ExpectTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(s):\n",
    "    # see the rewrite fn. We remove newlines from grammar training to make it easier to visualize\n",
    "    return s.strip().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grammar(grammar, tool, program):\n",
    "    with open(\"build/%s-%s.grammar.json\" % (tool, program), 'w+') as f:\n",
    "        json.dump(grammar, f)\n",
    "    return {k:sorted(grammar[k]) for k in grammar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mimid_p = {}\n",
    "Mimid_r = {}\n",
    "Autogram_p = {}\n",
    "Autogram_r = {}\n",
    "\n",
    "Mimid_t ={}\n",
    "Autogram_t ={}\n",
    "\n",
    "for k in program_src:\n",
    "    Mimid_p[k] = None\n",
    "    Mimid_r[k] = None\n",
    "    Mimid_t[k] = None\n",
    "    Autogram_p[k] = None\n",
    "    Autogram_r[k] = None\n",
    "    Autogram_t[k] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLParse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlparse_golden = {\n",
    "  \"<START>\": [\n",
    "    \"<url>\"\n",
    "  ],\n",
    "  \"<url>\": [\n",
    "    \"<scheme>://<authority><path><query>\"\n",
    "  ],\n",
    "  \"<scheme>\": [\n",
    "    \"http\",\n",
    "    \"https\",\n",
    "    \"ftp\",\n",
    "    \"ftps\"\n",
    "  ],\n",
    "  \"<authority>\": [\n",
    "    \"<host>\",\n",
    "#    \"<host>:<port>\",\n",
    "#    \"<userinfo>@<host>\",\n",
    "#    \"<userinfo>@<host>:<port>\"\n",
    "  ],\n",
    "#  \"<user>\": [\n",
    "#    \"user1\",\n",
    "#    \"user2\",\n",
    "#    \"user3\",\n",
    "#    \"user4\",\n",
    "#    \"user5\"\n",
    "#  ],\n",
    "#  \"<pass>\": [\n",
    "#    \"pass1\",\n",
    "#    \"pass2\",\n",
    "#    \"pass3\",\n",
    "#    \"pass4\",\n",
    "#    \"pass5\"\n",
    "#  ],\n",
    "\n",
    "  \"<host>\": [\n",
    "    \"host1\",\n",
    "    \"host2\",\n",
    "    \"host3\",\n",
    "    \"host4\",\n",
    "    \"host5\"\n",
    "  ],\n",
    "# \"<port>\": [\n",
    "#   \"<nat>\"\n",
    "# ],\n",
    "# \"<nat>\": [\n",
    "#   \"10\",\n",
    "#   \"20\",\n",
    "#   \"30\",\n",
    "#   \"40\",\n",
    "#   \"50\"\n",
    "# ],\n",
    "# \"<userinfo>\": [\n",
    "#   \"<user>:<pass>\"\n",
    "# ],\n",
    "  \"<path>\": [\n",
    "    \"\",\n",
    "    \"/\",\n",
    "    \"/<id>\",\n",
    "    \"/<id><path>\"\n",
    "  ],\n",
    "  \"<id>\": [\n",
    "    \"folder\"\n",
    "  ],\n",
    "  \"<query>\": [\n",
    "    \"\",\n",
    "    \"?<params>\"\n",
    "  ],\n",
    "  \"<params>\": [\n",
    "    \"<param>\",\n",
    "    \"<param>&<params>\"\n",
    "  ],\n",
    "  \"<param>\": [\n",
    "    \"<key>=<value>\"\n",
    "  ],\n",
    "  \"<key>\": [\n",
    "    \"key1\",\n",
    "    \"key2\",\n",
    "    \"key3\",\n",
    "    \"key4\"\n",
    "  ],\n",
    "  \"<value>\": [\n",
    "    \"value1\",\n",
    "    \"value2\",\n",
    "    \"value3\",\n",
    "    \"value4\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as we detail in the paper, both the miners are unable to generalize well with the kind of inputs above. The problem is the lack of generalization of string tokens. Hence we use the ones below, which are generated using the _golden grammar_. This is the output of simply using the golden grammar to fuzz and generate 100 inputs. Captured here for deterministic reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = LimitFuzzer(canonical(urlparse_golden))\n",
    "urlparse_samples = [gf.fuzz(key='<START>') for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in urlparse_samples:\n",
    "    r = do(['python3', 'build/check.py','subjects/urlparse.py', sample]).returncode\n",
    "    assert r == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mimid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    urlparse_grammar = accio_grammar('urlparse.py', VARS['urlparse_src'], urlparse_samples)\n",
    "Mimid_t['urlparse.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_grammar(urlparse_grammar, 'mimid', 'urlparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'urlparse' in CHECK:\n",
    "    result = check_precision('urlparse.py', urlparse_grammar)\n",
    "    Mimid_p['urlparse.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subjects.urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'urlparse' in CHECK:\n",
    "    result = check_recall(urlparse_golden, urlparse_grammar, subjects.urlparse.main)\n",
    "    Mimid_r['urlparse.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    autogram_urlparse_grammar_t = recover_grammar_with_taints('urlparse.py', VARS['urlparse_src'], urlparse_samples)\n",
    "Autogram_t['urlparse.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(autogram_urlparse_grammar_t, 'autogram_t', 'urlparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'urlparse' in CHECK:\n",
    "    result = check_precision('urlparse.py', autogram_urlparse_grammar_t)\n",
    "    Autogram_p['urlparse.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'urlparse' in CHECK:\n",
    "    result = check_recall(urlparse_golden, autogram_urlparse_grammar_t, subjects.urlparse.main)\n",
    "    Autogram_r['urlparse.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGIDecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgidecode_golden = {\n",
    "  \"<START>\": [\n",
    "    \"<cgidecode-s>\"\n",
    "  ],\n",
    "  \"<cgidecode-s>\": [\n",
    "      '<cgidecode>',\n",
    "      '<cgidecode><cgidecode-s>'],\n",
    "  \"<cgidecode>\": [\n",
    "    \"<single_char>\",\n",
    "    \"<percentage_char>\"\n",
    "  ],\n",
    "  \"<single_char>\": list(string.ascii_lowercase + string.ascii_uppercase + string.digits + \"-./_~\"),\n",
    "  \"<percentage_char>\": [urllib.parse.quote(i) for i in string.punctuation if i not in  \"-./_~\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "cgidecode_samples = [\n",
    "    \"Hello%2c+world%21\",\n",
    "    'Send+mail+to+me%40fuzzingbook.org',\n",
    "    'name=Fred&class=101&math=2+%2B+2+%3D+4&',\n",
    "    'name=Fred&status=good&status=happy&',\n",
    "    'http://target/getdata.php?data=%3cscript%20src=%22http%3a%2f%2f',\n",
    "    'www.badplace.com%2fnasty.js%22%3e%3c%2fscript%3e',\n",
    "    'http://target/login.asp?userid=bob%27%3b%20update%20logintable%20set%20passwd%3d%270wn3d%27%3b--%00',\n",
    "    'Colon%20%3A%20Hash%20%23%20Percent%20%25',\n",
    "    'O%21nP%22BG%23JI%24Tw%25mJ%26bB%27xX%28zy%29Aj%2aZ',\n",
    "    'E%2bNp%2cRP%2dVN%2eyV%2ftW%2AIJ%2BAe%2CkM%2DKf%2EB',\n",
    "    'W%2FAo%3azF%3blw%3ctY%3dqy%3eLm%3fCS%3AyB%3BHq%3Ck',\n",
    "    'y%3DZM%3EVH%3FRx%40gG%5bhh%5cjn%5dOD%5eDR%5fcu%5Bm',\n",
    "    'b%5CJm%5Drl%5Ezn%5FKe%60hQ%7bBj%7chf%7dmB%7eyc%7Bp',\n",
    "    'w%7CWd%7DCG%7Ec',\n",
    "    'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n",
    "    '1234567890',\n",
    "    '-./_~'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    cgidecode_grammar = accio_grammar('cgidecode.py', VARS['cgidecode_src'], cgidecode_samples)\n",
    "Mimid_t['cgidecode.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mimid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(cgidecode_grammar, 'mimid', 'cgidecode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cgidecode' in CHECK:\n",
    "    result = check_precision('cgidecode.py', cgidecode_grammar)\n",
    "    Mimid_p['cgidecode.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subjects.cgidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cgidecode' in CHECK:\n",
    "    result = check_recall(cgidecode_golden, cgidecode_grammar, subjects.cgidecode.main)\n",
    "    Mimid_r['cgidecode.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    autogram_cgidecode_grammar_t = recover_grammar_with_taints('cgidecode.py', VARS['cgidecode_src'], cgidecode_samples)\n",
    "Autogram_t['cgidecode.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(autogram_cgidecode_grammar_t, 'autogram_t', 'cgidecode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cgidecode' in CHECK:\n",
    "    result = check_precision('cgidecode.py', autogram_cgidecode_grammar_t)\n",
    "    Autogram_p['cgidecode.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cgidecode' in CHECK:\n",
    "    result = check_recall(cgidecode_golden, autogram_cgidecode_grammar_t, subjects.cgidecode.main)\n",
    "    Autogram_r['cgidecode.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_golden = {\n",
    "  \"<START>\": [\n",
    "    \"<expr>\"\n",
    "  ],\n",
    "  \"<expr>\": [\n",
    "    \"<term>+<expr>\",\n",
    "    \"<term>-<expr>\",\n",
    "    \"<term>\"\n",
    "  ],\n",
    "  \"<term>\": [\n",
    "    \"<factor>*<term>\",\n",
    "    \"<factor>/<term>\",\n",
    "    \"<factor>\"\n",
    "  ],\n",
    "  \"<factor>\": [\n",
    "    \"(<expr>)\",\n",
    "    \"<number>\"\n",
    "  ],\n",
    "  \"<number>\": [\n",
    "    \"<integer>.<integer>\",\n",
    "    \"<integer>\"\n",
    "  ],\n",
    "  \"<integer>\": [\n",
    "    \"<digit><integer>\",\n",
    "    \"<digit>\"\n",
    "  ],\n",
    "  \"<digit>\": [ \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\" ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_samples=[i.strip() for i in '''\\\n",
    "(1+2)*3/(423-334+9983)-5-((6)-(701))\n",
    "(123+133*(12-3)/9+8)+33\n",
    "(100)\n",
    "21*3\n",
    "33/44+2\n",
    "100\n",
    "23*234*22*4\n",
    "(123+133*(12-3)/9+8)+33\n",
    "1+2\n",
    "31/20-2\n",
    "555+(234-445)\n",
    "1-(41/2)\n",
    "443-334+33-222\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    calc_grammar = accio_grammar('calculator.py', VARS['calc_src'], calc_samples)\n",
    "Mimid_t['calculator.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mimid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(calc_grammar, 'mimid', 'calculator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'calculator' in CHECK:\n",
    "    result = check_precision('calculator.py', calc_grammar)\n",
    "    Mimid_p['calculator.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subjects.calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'calculator' in CHECK:\n",
    "    result = check_recall(calc_golden, calc_grammar, subjects.calculator.main)\n",
    "    Mimid_r['calculator.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    autogram_calc_grammar_t = recover_grammar_with_taints('calculator.py', VARS['calc_src'], calc_samples)\n",
    "Autogram_t['calculator.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(autogram_calc_grammar_t, 'autogram_t', 'calculator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'calculator' in CHECK:\n",
    "    result = check_precision('calculator.py', autogram_calc_grammar_t)\n",
    "    Autogram_p['calculator.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'calculator' in CHECK:\n",
    "    result = check_recall(calc_golden, autogram_calc_grammar_t, subjects.calculator.main)\n",
    "    Autogram_r['calculator.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MathExpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathexpr_golden = {\n",
    "  \"<START>\": [\n",
    "    \"<expr>\"\n",
    "  ],\n",
    "  \"<word>\": [\n",
    "    \"pi\",\n",
    "    \"e\",\n",
    "    \"phi\",\n",
    "    \"abs\",\n",
    "    \"acos\",\n",
    "    \"asin\",\n",
    "    \"atan\",\n",
    "    \"atan2\",\n",
    "    \"ceil\",\n",
    "    \"cos\",\n",
    "    \"cosh\",\n",
    "    \"degrees\",\n",
    "    \"exp\",\n",
    "    \"fabs\",\n",
    "    \"floor\",\n",
    "    \"fmod\",\n",
    "    \"frexp\",\n",
    "    \"hypot\",\n",
    "    \"ldexp\",\n",
    "    \"log\",\n",
    "    \"log10\",\n",
    "    \"modf\",\n",
    "    \"pow\",\n",
    "    \"radians\",\n",
    "    \"sin\",\n",
    "    \"sinh\",\n",
    "    \"sqrt\",\n",
    "    \"tan\",\n",
    "    \"tanh\",\n",
    "    \"<alpha>\"\n",
    "  ],\n",
    "  \"<alpha>\": [ \"a\", \"b\", \"c\", \"d\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"],\n",
    "  \"<expr>\": [\n",
    "    \"<term>+<expr>\",\n",
    "    \"<term>-<expr>\",\n",
    "    \"<term>\"\n",
    "  ],\n",
    "  \"<term>\": [\n",
    "    \"<factor>*<term>\",\n",
    "    \"<factor>/<term>\",\n",
    "    \"<factor>\"\n",
    "  ],\n",
    "  \"<factor>\": [\n",
    "    \"+<factor>\",\n",
    "    \"-<factor>\",\n",
    "    \"(<expr>)\",\n",
    "    \"<word>(<expr>,<expr>)\",\n",
    "    \"<word>\",\n",
    "    \"<number>\"\n",
    "  ],\n",
    "  \"<number>\": [\n",
    "    \"<integer>.<integer>\",\n",
    "    \"<integer>\"\n",
    "  ],\n",
    "  \"<integer>\": [\n",
    "    \"<digit><integer>\",\n",
    "    \"<digit>\"\n",
    "  ],\n",
    "  \"<digit>\": [ \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\" ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathexpr_samples=[i.strip() for i in '''\n",
    "(pi*e+2)*3/(423-334+9983)-5-((6)-(701-x))\n",
    "(123+133*(12-3)/9+8)+33\n",
    "(100)\n",
    "pi * e\n",
    "(1 - 1 + -1) * pi\n",
    "1.0 / 3 * 6\n",
    "(x + e * 10) / 10\n",
    "(a + b) / c\n",
    "1 + pi / 4\n",
    "(1-2)/3.0 + 0.0000\n",
    "-(1 + 2) * 3\n",
    "(1 + 2) * 3\n",
    "100\n",
    "1 + 2 * 3\n",
    "23*234*22*4\n",
    "(123+133*(12-3)/9+8)+33\n",
    "1+2\n",
    "31/20-2\n",
    "555+(234-445)\n",
    "1-(41/2)\n",
    "443-334+33-222\n",
    "cos(x+4*3) + 2 * 3\n",
    "exp(0)\n",
    "-(1 + 2) * 3\n",
    "(1-2)/3.0 + 0.0000\n",
    "abs(-2) + pi / 4\n",
    "(pi + e * 10) / 10\n",
    "1.0 / 3 * 6\n",
    "cos(pi) * 1\n",
    "atan2(2, 1)\n",
    "hypot(5, 12)\n",
    "pow(3, 5)\n",
    "'''.strip().split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mimid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    mathexpr_grammar = accio_grammar('mathexpr.py', VARS['mathexpr_src'], mathexpr_samples, cache=False)\n",
    "Mimid_t['mathexpr.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(mathexpr_grammar, 'mimid', 'mathexpr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mathexpr' in CHECK:\n",
    "    result = check_precision('mathexpr.py', mathexpr_grammar)\n",
    "    Mimid_p['mathexpr.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subjects.mathexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mathexpr' in CHECK:\n",
    "    result = check_recall(mathexpr_golden, mathexpr_grammar, subjects.mathexpr.main)\n",
    "    Mimid_r['mathexpr.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    autogram_mathexpr_grammar_t = recover_grammar_with_taints('mathexpr.py',  VARS['mathexpr_src'], mathexpr_samples)\n",
    "Autogram_t['mathexpr.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(autogram_mathexpr_grammar_t, 'autogram_t', 'mathexpr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mathexpr' in CHECK:\n",
    "    result = check_precision('mathexpr.py', autogram_mathexpr_grammar_t)\n",
    "    Autogram_p['mathexpr.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mathexpr' in CHECK:\n",
    "    result = check_recall(mathexpr_golden, autogram_mathexpr_grammar_t, subjects.mathexpr.main)\n",
    "    Autogram_r['mathexpr.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_golden = {\n",
    "    \"<START>\": [ [\"<json>\"] ],\n",
    "    \"<json>\": [ [\"<element>\"] ],\n",
    "    \"<value>\": [\n",
    "        [\"<object>\"],\n",
    "        [\"<array>\"],\n",
    "        [\"<string>\"],\n",
    "        [\"<number>\"],\n",
    "        [\"true\"],\n",
    "        [\"false\"],\n",
    "        [\"null\"],\n",
    "    ],\n",
    "    \"<object>\": [ [\"{\", \"<ws>\", \"<members>\", \"<ws>\", \"}\"], ],\n",
    "    \"<members>\": [\n",
    "        [\"<member>\", \",\", \"<members>\"],\n",
    "        [\"<member>\"]\n",
    "    ],\n",
    "    \"<member>\": [ [\"<ws>\", \"<string>\", \"<ws>\", \":\", \"<element>\"] ],\n",
    "    \"<array>\": [\n",
    "        [\"[\", \"<ws>\", \"]\"],\n",
    "        [\"[\", \"<elements>\", \"]\"],\n",
    "    ],\n",
    "    \"<elements>\": [ [\"<element>\", \",\", \"<elements>\"], [\"<element>\"], ],\n",
    "    \"<element>\": [ [\"<ws>\", \"<value>\", \"<ws>\"], ],\n",
    "    \"<string>\": [ [\"\\\"\",\"<characters>\",\"\\\"\"] ],\n",
    "    \"<characters>\": [ [\"<character>\", \"<characters>\"], [] ],\n",
    "    \"<character>\":  [[s] for s in string.printable if s not in {\"\\\"\", \"\\\\\"}] +\n",
    "    [[\"\\\\\", \"<escape>\"]],\n",
    "    \"<escape>\": [[c] for c in '\"\\\\/bfnrt\"'] + [[\"u\", \"<hex>\", \"<hex>\", \"<hex>\", \"<hex>\"]],\n",
    "    \"<hex>\": [\n",
    "        [\"<digit>\" ],\n",
    "        [\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [\"f\"],\n",
    "        [\"A\"], [\"B\"], [\"C\"], [\"D\"], [\"E\"], [\"F\"]\n",
    "    ],\n",
    "    \"<number>\": [ [\"<integer>\", \"<fraction>\", \"<exponent>\"] ],\n",
    "    \"<integer>\": [\n",
    "        [\"<onenine>\",\"<digits>\"],\n",
    "        [\"<digit>\"],\n",
    "        [\"-\",\"<digit>\"],\n",
    "        [\"-\", \"<onenine>\",\"<digits>\"],\n",
    "    ],\n",
    "    \"<digits>\": [ [\"<digit>\", \"<digits>\"], [\"<digit>\"], ],\n",
    "    \"<digit>\": [ [\"0\"], [\"<onenine>\"], ],\n",
    "    \"<onenine>\": [ [\"1\"],  [\"2\"],  [\"3\"],  [\"4\"],  [\"5\"], [\"6\"],  [\"7\"],  [\"8\"],  [\"9\"] ],\n",
    "    \"<fraction>\": [ [\".\", \"<digits>\"], [] ],\n",
    "    \"<exponent>\" :[ [\"E\", \"<sign>\", \"<digits>\"], [\"e\", \"<sign>\", \"<digits>\"], [] ],\n",
    "    \"<sign>\": [ [\"+\"], [\"-\"], [] ],\n",
    "    \"<ws>\": [ [\" \", \"<ws>\"], [] ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = LimitFuzzer(json_golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(repr(gf.fuzz(key='<START>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Microjson Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done through `json.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# json samples\n",
    "json_samples = [i.strip().replace('\\n', ' ') for i in ['''\n",
    "{\"abcd\":[],\n",
    "  \"efgh\":{\"y\":[],\n",
    "    \"pqrstuv\":  null,\n",
    "    \"p\":  \"\",\n",
    "    \"q\":\"\" ,\n",
    "    \"r\": \"\" ,\n",
    "    \"float1\": 1.0,\n",
    "    \"float2\":1.0,\n",
    "    \"float3\":1.0 ,\n",
    "    \"float4\": 1.0 ,\n",
    "     \"_124\": {\"wx\" :  null,\n",
    "     \"zzyym!!2@@39\": [1.1, 2452, 398, {\"x\":[[4,53,6,[7  ,8,90 ],10]]}]} }\n",
    " }\n",
    "''',\n",
    "'''\n",
    "{\"mykey1\": [1, 2, 3], \"mykey2\": null, \"mykey\":\"'`:{}<>&%[]\\\\\\\\^~|$'\"}\n",
    "''','''\n",
    "{\"emptya\": [], \"emptyh\": {}, \"emptystr\":\"\", \"null\":null}\n",
    "''', '''\n",
    "[\n",
    "    \"JSON Test Pattern pass1\",\n",
    "    {\"object with 1 member\":[\"array with 1 element\"]},\n",
    "    {},\n",
    "    [],\n",
    "    -42,\n",
    "    true,\n",
    "    false,\n",
    "    null,\n",
    "    {\n",
    "        \"integer\": 1234567890,\n",
    "        \"real\": -9876.543210,\n",
    "        \"e\": 0.123456789e-12,\n",
    "        \"E\": 1.234567890E+34,\n",
    "        \"\":  23456789012E66,\n",
    "        \"zero\": 0,\n",
    "        \"one\": 1,\n",
    "        \"space\": \" \",\n",
    "        \"quote\": \"\\\\\"\",\n",
    "        \"backslash\": \"\\\\\\\\\",\n",
    "        \"controls\": \"\\\\b\\\\f\\\\n\\\\r\\\\t\",\n",
    "        \"slash\": \"/ & \\\\/\",\n",
    "        \"alpha\": \"abcdefghijklmnopqrstuvwyz\",\n",
    "        \"ALPHA\": \"ABCDEFGHIJKLMNOPQRSTUVWYZ\",\n",
    "        \"digit\": \"0123456789\",\n",
    "        \"0123456789\": \"digit\",\n",
    "        \"special\": \"`1~!@#$%^&*()_+-={':[,]}|;.</>?\",\n",
    "        \"true\": true,\n",
    "        \"false\": false,\n",
    "        \"null\": null,\n",
    "        \"array\":[  ],\n",
    "        \"object\":{  },\n",
    "        \"address\": \"50 St. James Street\",\n",
    "        \"url\": \"http://www.JSON.org/\",\n",
    "        \"comment\": \"// /* <!-- --\",\n",
    "        \"# -- --> */\": \" \",\n",
    "        \" s p a c e d \" :[1,2 , 3\n",
    "\n",
    ",\n",
    "\n",
    "4 , 5        ,          6           ,7        ],\"compact\":[1,2,3,4,5,6,7],\n",
    "        \"jsontext\": \"{\\\\\"object with 1 member\\\\\":[\\\\\"array with 1 element\\\\\"]}\",\n",
    "        \"quotes\": \"&#34; %22 0x22 034 &#x22;\",\n",
    "        \"\\\\/\\\\\\\\\\\\\"\\\\b\\\\f\\\\n\\\\r\\\\t`1~!@#$%^&*()_+-=[]{}|;:',./<>?\"\n",
    ": \"A key can be any string\"\n",
    "    },\n",
    "    0.5 ,98.6\n",
    ",\n",
    "99.44\n",
    ",\n",
    "\n",
    "1066,\n",
    "1e1,\n",
    "0.1e1,\n",
    "1e-1,\n",
    "1e00,2e+00,2e-00\n",
    ",\"rosebud\"]\n",
    "''', '''\n",
    "{\"menu\":\n",
    "  {\n",
    "    \"id\": \"file\",\n",
    "      \"value\": \"File\",\n",
    "      \"popup\": {\n",
    "        \"menuitem\": [\n",
    "        {\"value\": \"New\", \"onclick\": \"CreateNewDoc()\"},\n",
    "        {\"value\": \"Open\", \"onclick\": \"OpenDoc()\"},\n",
    "        {\"value\": \"Close\", \"onclick\": \"CloseDoc()\"}\n",
    "        ]\n",
    "      }\n",
    "  }\n",
    "}\n",
    "''', '''\n",
    "{\n",
    "  \"XMDEwOlJlcG9zaXRvcnkxODQ2MjA4ODQ=\": \"-----BEGIN PGP SIGNATURE-----\\n\\niQIzBAABAQAdFiEESn/54jMNIrGSE6Tp6cQjvhfv7nAFAlnT71cACgkQ6cQjvhfv\\n7nCWwA//XVqBKWO0zF+                             bZl6pggvky3Oc2j1pNFuRWZ29LXpNuD5WUGXGG209B0hI\\nDkmcGk19ZKUTnEUJV2Xd0R7AW01S/YSub7OYcgBkI7qUE13FVHN5ln1KvH2all2n\\n2+JCV1HcJLEoTjqIFZSSu/sMdhkLQ9/NsmMAzpf/           iIM0nQOyU4YRex9eD1bYj6nA\\nOQPIDdAuaTQj1gFPHYLzM4zJnCqGdRlg0sOM/zC5apBNzIwlgREatOYQSCfCKV7k\\nnrU34X8b9BzQaUx48Qa+Dmfn5KQ8dl27RNeWAqlkuWyv3pUauH9UeYW+KyuJeMkU\\n+     NyHgAsWFaCFl23kCHThbLStMZOYEnGagrd0hnm1TPS4GJkV4wfYMwnI4KuSlHKB\\njHl3Js9vNzEUQipQJbgCgTiWvRJoK3ENwBTMVkKHaqT4x9U4Jk/                                                XZB6Q8MA09ezJ\\n3QgiTjTAGcum9E9QiJqMYdWQPWkaBIRRz5cET6HPB48YNXAAUsfmuYsGrnVLYbG+                                                                                     \\nUpC6I97VybYHTy2O9XSGoaLeMI9CsFn38ycAxxbWagk5mhclNTP5mezIq6wKSwmr\\nX11FW3n1J23fWZn5HJMBsRnUCgzqzX3871IqLYHqRJ/bpZ4h20RhTyPj5c/z7QXp\\neSakNQMfbbMcljkha+            ZMuVQX1K9aRlVqbmv3ZMWh+OijLYVU2bc=\\n=5Io4\\n-----END PGP SIGNATURE-----\\n\"\n",
    "}\n",
    "''', '''\n",
    "{\"widget\":\n",
    "  {\n",
    "    \"debug\": \"on\",\n",
    "      \"window\": {\n",
    "        \"title\": \"Sample Konfabulator Widget\",\n",
    "        \"name\": \"main_window\",\n",
    "        \"width\": 500,\n",
    "        \"height\": 500\n",
    "      },\n",
    "      \"image\": {\n",
    "        \"src\": \"Images/Sun.png\",\n",
    "        \"name\": \"sun1\",\n",
    "        \"hOffset\": 250,\n",
    "        \"vOffset\": 250,\n",
    "        \"alignment\": \"center\"\n",
    "      },\n",
    "      \"text\": {\n",
    "        \"data\": \"Click Here\",\n",
    "        \"size\": 36,\n",
    "        \"style\": \"bold\",\n",
    "        \"name\": \"text1\",\n",
    "        \"hOffset\": 250,\n",
    "        \"vOffset\": 100,\n",
    "        \"alignment\": \"center\",\n",
    "        \"onMouseUp\": \"sun1.opacity = (sun1.opacity / 100) * 90;\"\n",
    "      }\n",
    "  }\n",
    "}\n",
    "''',\n",
    "'''\n",
    "{\n",
    "    \"fruit\": \"Apple\",\n",
    "    \"size\": \"Large\",\n",
    "    \"color\": \"Red\",\n",
    "    \"product\": \"Jam\"\n",
    "}\n",
    "''',\n",
    "'''\n",
    "{\"menu\":\n",
    "  {\n",
    "    \"header\": \"SVG Viewer\",\n",
    "      \"items\": [\n",
    "      {\"id\": \"Open\"},\n",
    "      {\"id\": \"OpenNew\", \"label\": \"Open New\"},\n",
    "      null,\n",
    "      {\"id\": \"ZoomIn\", \"label\": \"Zoom In\"},\n",
    "      {\"id\": \"ZoomOut\", \"label\": \"Zoom Out\"},\n",
    "      {\"id\": \"OriginalView\", \"label\": \"Original View\"},\n",
    "      null,\n",
    "      {\"id\": \"Quality\"},\n",
    "      {\"id\": \"Pause\"},\n",
    "      {\"id\": \"Mute\"},\n",
    "      null,\n",
    "      {\"id\": \"Find\", \"label\": \"Find...\"},\n",
    "      {\"id\": \"FindAgain\", \"label\": \"Find Again\"},\n",
    "      {\"id\": \"Copy\"},\n",
    "      {\"id\": \"CopyAgain\", \"label\": \"Copy Again\"},\n",
    "      {\"id\": \"CopySVG\", \"label\": \"Copy SVG\"},\n",
    "      {\"id\": \"ViewSVG\", \"label\": \"View SVG\"},\n",
    "      {\"id\": \"ViewSource\", \"label\": \"View Source\"},\n",
    "      {\"id\": \"SaveAs\", \"label\": \"Save As\"},\n",
    "      null,\n",
    "      {\"id\": \"Help\"},\n",
    "      {\"id\": \"About\", \"label\": \"About Adobe CVG Viewer...\"}\n",
    "    ]\n",
    "  }}\n",
    "''',\n",
    "'''\n",
    "{\n",
    "    \"quiz\": {\n",
    "        \"sport\": {\n",
    "            \"q1\": {\n",
    "                \"question\": \"Which one is correct team name in NBA?\",\n",
    "                \"options\": [\n",
    "                    \"New York Bulls\",\n",
    "                    \"Los Angeles Kings\",\n",
    "                    \"Golden State Warriros\",\n",
    "                    \"Huston Rocket\"\n",
    "                ],\n",
    "                \"answer\": \"Huston Rocket\"\n",
    "            }\n",
    "        },\n",
    "        \"maths\": {\n",
    "            \"q1\": {\n",
    "                \"question\": \"5 + 7 = ?\",\n",
    "                \"options\": [\n",
    "                    \"10\",\n",
    "                    \"11\",\n",
    "                    \"12\",\n",
    "                    \"13\"\n",
    "                ],\n",
    "                \"answer\": \"12\"\n",
    "            },\n",
    "            \"q2\": {\n",
    "                \"question\": \"12 - 8 = ?\",\n",
    "                \"options\": [\n",
    "                    \"1\",\n",
    "                    \"2\",\n",
    "                    \"3\",\n",
    "                    \"4\"\n",
    "                ],\n",
    "                \"answer\": \"4\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "''',\n",
    "'''\n",
    "{\n",
    "  \"colors\":\n",
    "  [\n",
    "    {\n",
    "      \"color\": \"black\",\n",
    "      \"category\": \"hue\",\n",
    "      \"type\": \"primary\",\n",
    "      \"code\": {\n",
    "        \"rgba\": [255,255,255,1],\n",
    "        \"hex\": \"#000\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"color\": \"white\",\n",
    "      \"category\": \"value\",\n",
    "      \"code\": {\n",
    "        \"rgba\": [0,0,0,1],\n",
    "        \"hex\": \"#FFF\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"color\": \"red\",\n",
    "      \"category\": \"hue\",\n",
    "      \"type\": \"primary\",\n",
    "      \"code\": {\n",
    "        \"rgba\": [255,0,0,1],\n",
    "        \"hex\": \"#FF0\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"color\": \"blue\",\n",
    "      \"category\": \"hue\",\n",
    "      \"type\": \"primary\",\n",
    "      \"code\": {\n",
    "        \"rgba\": [0,0,255,1],\n",
    "        \"hex\": \"#00F\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"color\": \"yellow\",\n",
    "      \"category\": \"hue\",\n",
    "      \"type\": \"primary\",\n",
    "      \"code\": {\n",
    "        \"rgba\": [255,255,0,1],\n",
    "        \"hex\": \"#FF0\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"color\": \"green\",\n",
    "      \"category\": \"hue\",\n",
    "      \"type\": \"secondary\",\n",
    "      \"code\": {\n",
    "        \"rgba\": [0,255,0,1],\n",
    "        \"hex\": \"#0F0\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "''',\n",
    "'''\n",
    "{\n",
    "  \"aliceblue\": \"#f0f8ff\",\n",
    "  \"antiquewhite\": \"#faebd7\",\n",
    "  \"aqua\": \"#00ffff\",\n",
    "  \"aquamarine\": \"#7fffd4\",\n",
    "  \"azure\": \"#f0ffff\",\n",
    "  \"beige\": \"#f5f5dc\",\n",
    "  \"bisque\": \"#ffe4c4\",\n",
    "  \"black\": \"#000000\",\n",
    "  \"blanchedalmond\": \"#ffebcd\",\n",
    "  \"blue\": \"#0000ff\",\n",
    "  \"blueviolet\": \"#8a2be2\",\n",
    "  \"brown\": \"#a52a2a\",\n",
    "  \"majenta\": \"#ff0ff\"\n",
    "}\n",
    "''']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mimid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    microjson_grammar = accio_grammar('microjson.py', VARS['microjson_src'], json_samples)\n",
    "Mimid_t['microjson.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(microjson_grammar, 'mimid', 'microjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'microjson' in CHECK:\n",
    "    result = check_precision('microjson.py', microjson_grammar)\n",
    "    Mimid_p['microjson.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subjects.microjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slurp(fn):\n",
    "    with open(fn) as f:\n",
    "        s = f.read()\n",
    "        return s.replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shutil.which('gzcat'):\n",
    "    !gzcat json.tar.gz | tar -xpf -\n",
    "elif shutil.which('zcat'):\n",
    "    !zcat json.tar.gz | tar -xpf -\n",
    "else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = pathlib.Path('recall/json')\n",
    "json_files =  [i.as_posix() for i in json_path.glob('**/*.json')]\n",
    "json_samples_2 = [slurp(i) for i in json_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recall_samples(samples, my_grammar, validator, log=False):\n",
    "    n_max = len(samples)\n",
    "    ie = IterativeEarleyParser(my_grammar, start_symbol='<START>')\n",
    "    my_samples = list(samples)\n",
    "    count = 0\n",
    "    while my_samples:\n",
    "        src, *my_samples = my_samples\n",
    "        try:\n",
    "            validator(src)\n",
    "            try:\n",
    "                # JSON files are much larger because they are from real world\n",
    "                for tree in ie.parse(src):\n",
    "                    count += 1\n",
    "                    break\n",
    "                if log: print('+', repr(src), count, file=sys.stderr)\n",
    "            except:\n",
    "                if log: print('-', repr(src), file=sys.stderr)\n",
    "        except:\n",
    "            pass\n",
    "    return (count, n_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'microjson' in CHECK:\n",
    "    result = check_recall_samples(json_samples_2, microjson_grammar, subjects.microjson.main)\n",
    "    Mimid_r['microjson.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    autogram_microjson_grammar_t = recover_grammar_with_taints('microjson.py', VARS['microjson_src'], json_samples)\n",
    "Autogram_t['microjson.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(autogram_microjson_grammar_t, 'autogram_t', 'microjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'microjson' in CHECK:\n",
    "    result = check_precision('microjson.py', autogram_microjson_grammar_t)\n",
    "    Autogram_p['microjson.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'microjson' in CHECK:\n",
    "    result = check_recall_samples(json_samples_2, autogram_microjson_grammar_t, subjects.microjson.main)\n",
    "    Autogram_r['microjson.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we found and fixed a bug in the Information flow chapter of the fuzzingbook, which was causing Autogram to fail (See `flatten` and `ostr_new` in `recover_grammar_with_taints`). This caused the precision numbers of Autogram to improve. However, please see the grammars generated. They are still enumerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_table(keys, autogram, mimid, title):\n",
    "    keys = [k for k in keys if k in autogram and k in mimid and autogram[k] and mimid[k]]\n",
    "    tbl = ['<tr>%s</tr>' % ''.join([\"<th>%s</th>\" % k for k in ['<b>%s</b>' % title,'Autogram', 'Mimid']])]\n",
    "    for k in keys:\n",
    "        h_c = \"<td>%s</td>\" % k\n",
    "        a_c = \"<td>%s</td>\" % autogram.get(k,('',0))[0]\n",
    "        m_c = \"<td>%s</td>\" % mimid.get(k,('',0))[0]\n",
    "        tbl.append('<tr>%s</tr>' % ''.join([h_c, a_c, m_c]))\n",
    "    return display(HTML('<table>%s</table>' % '\\n'.join(tbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sec(hm):\n",
    "    return {k:((hm[k][1]).seconds, ' ') for k in hm if hm[k]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table II (Time in Seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autogram_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mimid_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_table(Autogram_t.keys(), to_sec(Autogram_t), to_sec(Mimid_t), 'Timing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table III (Precision)\n",
    "How many inputs we generate using our inferred grammar is valid? (accepted by the subject program?)\n",
    "\n",
    "Note that the paper reports precision per 100 inputs. We have increased the count to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autogram_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mimid_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_table(Autogram_p.keys(), Autogram_p, Mimid_p, 'Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table IV (Recall)\n",
    "\n",
    "How many *valid* inputs generated by the golden grammar or collected externally are parsable by a parser using our grammar?\n",
    "\n",
    "Note that the recall is reported per 100 inputs in paper. We have increased the count to 1000. For Microjson, the recall numbers are based on 100 realworld documents. These are available in json.tar.gz that is bundled along with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autogram_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mimid_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_table(Autogram_p.keys(), Autogram_r, Mimid_r, 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Recognizer (not a Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var calc_rec_src\n",
    "import string\n",
    "def is_digit(i):\n",
    "    return i in list(string.digits)\n",
    "\n",
    "def parse_num(s,i):\n",
    "    while s[i:] and is_digit(s[i]):\n",
    "        i = i +1\n",
    "    return i\n",
    "\n",
    "def parse_paren(s, i):\n",
    "    assert s[i] == '('\n",
    "    i = parse_expr(s, i+1)\n",
    "    if s[i:] == '':\n",
    "        raise Exception(s, i)\n",
    "    assert s[i] == ')'\n",
    "    return i+1\n",
    "\n",
    "\n",
    "def parse_expr(s, i = 0):\n",
    "    expr = []\n",
    "    is_op = True\n",
    "    while s[i:] != '':\n",
    "        c = s[i]\n",
    "        if c in list(string.digits):\n",
    "            if not is_op: raise Exception(s,i)\n",
    "            i = parse_num(s,i)\n",
    "            is_op = False\n",
    "        elif c in ['+', '-', '*', '/']:\n",
    "            if is_op: raise Exception(s,i)\n",
    "            is_op = True\n",
    "            i = i + 1\n",
    "        elif c == '(':\n",
    "            if not is_op: raise Exception(s,i)\n",
    "            i = parse_paren(s, i)\n",
    "            is_op = False\n",
    "        elif c == ')':\n",
    "            break\n",
    "        else:\n",
    "            raise Exception(s,i)\n",
    "    if is_op:\n",
    "        raise Exception(s,i)\n",
    "    return i\n",
    "\n",
    "def main(arg):\n",
    "    parse_expr(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rec_grammar = accio_grammar('cal.py', VARS['calc_rec_src'], calc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rec_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing with Parser Combinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var config_src\n",
    "# [(\n",
    "import urllib.parse\n",
    "# This is useful for parsers  such as parser combinators\n",
    "# we use a `tag` to incorporate variable name info in the method name.\n",
    "ENCODE_ARGS = True\n",
    "def encode_method_name(name, my_args):\n",
    "    if not ENCODE_ARGS: return name\n",
    "    if not my_args: return name\n",
    "    # trick to extract variable name\n",
    "    if hasattr(my_args[0], 'tag'):\n",
    "        return \"%s_%s\" % (my_args[0].tag, name)\n",
    "    return name\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('build/config.py', 'w+') as f:\n",
    "    print(VARS['config_src'], file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var myparsec_src\n",
    "# From https://github.com/xmonader/pyparsec\n",
    "from functools import reduce\n",
    "import string \n",
    "flatten = lambda l: [item for sublist in l for item in (sublist if isinstance(sublist, list) else [sublist] )]\n",
    "\n",
    "class Maybe:\n",
    "    pass\n",
    "\n",
    "class Just(Maybe):\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Just %s>\"%str(self.val)\n",
    "    \n",
    "class Nothing(Maybe):\n",
    "    _instance = None\n",
    "    def __new__(class_, *args, **kwargs):\n",
    "        if not isinstance(class_._instance, class_):\n",
    "            class_._instance = object.__new__(class_, *args, **kwargs)\n",
    "        return class_._instance\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Nothing>\"\n",
    "\n",
    "class Either:\n",
    "    pass\n",
    "\n",
    "class Left:\n",
    "    def __init__(self, errmsg):\n",
    "        self.errmsg = errmsg\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"(Left %s)\"%self.errmsg\n",
    "\n",
    "    __repr__ = __str__\n",
    "    def map(self, f):\n",
    "        return self \n",
    "\n",
    "class Right:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def unwrap(self):\n",
    "        return self.val\n",
    "\n",
    "    @property\n",
    "    def val0(self):\n",
    "        if isinstance(self.val[0], list):\n",
    "            return flatten(self.val[0])\n",
    "        else:\n",
    "            return [self.val[0]]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"(Right %s)\"% str(self.val)\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def map(self, f):\n",
    "        return Right( (f(self.val0), self.val[1])) \n",
    "\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, f, tag=''):\n",
    "        self.f = f\n",
    "        self._suppressed = False\n",
    "        self.tag = tag\n",
    "\n",
    "    def parse(self, *args, **kwargs):\n",
    "        return self.f(*args, **kwargs)\n",
    "\n",
    "    __call__ = parse\n",
    "    \n",
    "    def __rshift__(self, rparser):\n",
    "        return and_then(self, rparser)\n",
    "\n",
    "    def __lshift__(self, rparser):\n",
    "        return and_then(self, rparser)\n",
    "    \n",
    "    def __or__(self, rparser):\n",
    "        return or_else(self, rparser)\n",
    "\n",
    "    def map(self, transformer):\n",
    "        return Parser(lambda *args, **kwargs: self.f(*args, **kwargs).map(transformer), self.tag)\n",
    "\n",
    "    def __mul__(self, times):\n",
    "        return n(self, times) \n",
    "\n",
    "    set_action = map\n",
    "\n",
    "    def suppress(self):\n",
    "        self._suppressed = True \n",
    "        return self\n",
    "\n",
    "def pure(x):\n",
    "    def curried(s):\n",
    "        return Right((x, s))\n",
    "    return Parser(curried, 'pure')\n",
    "\n",
    "def ap(p1, p2):\n",
    "    def curried(s):\n",
    "        res = p2(s)\n",
    "        return p1(*res.val[0])\n",
    "    return curried\n",
    "\n",
    "def compose(p1, p2):\n",
    "    def newf(*args, **kwargs):\n",
    "        return p2(p1(*args, **kwargs))\n",
    "    return newf\n",
    "\n",
    "def run_parser(p, inp):\n",
    "    return p(inp)\n",
    "\n",
    "def _isokval(v):\n",
    "    if isinstance(v, str) and not v.strip():\n",
    "        return False\n",
    "    if isinstance(v, list) and v and v[0] == \"\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def and_then(p1, p2):\n",
    "    def curried(s):\n",
    "        res1 = p1(s)\n",
    "        if isinstance(res1, Left):\n",
    "            return res1\n",
    "        else:\n",
    "            res2 = p2(res1.val[1]) # parse remaining chars.\n",
    "            if isinstance(res2, Right):\n",
    "                v1 = res1.val0\n",
    "                v2 = res2.val0\n",
    "                vs = []\n",
    "                if not p1._suppressed and _isokval(v1):\n",
    "                    vs += v1 \n",
    "                if not p2._suppressed and _isokval(v2):\n",
    "                    vs += v2\n",
    "\n",
    "                return Right( (vs, res2.val[1])) \n",
    "            return res2\n",
    "    return Parser(curried, 'and_then')\n",
    "\n",
    "def n(parser, count):\n",
    "    def curried(s):\n",
    "        fullparsed = \"\"\n",
    "        for i in range(count):\n",
    "            res = parser(s)\n",
    "            if isinstance(res, Left):\n",
    "                return res\n",
    "            else:\n",
    "                parsed, remaining = res.unwrap()\n",
    "                s = remaining\n",
    "                fullparsed += parsed\n",
    "        return Right((fullparsed, s))\n",
    "    return Parser(curried, 'n')\n",
    "\n",
    "def or_else(p1, p2):\n",
    "    def curried(s):\n",
    "        res = p1(s)\n",
    "        if isinstance(res, Right):\n",
    "            return res\n",
    "        else:\n",
    "            res = p2(s)\n",
    "            if isinstance(res, Right):\n",
    "                return res\n",
    "            else:\n",
    "                return Left(\"Failed at both\") \n",
    "    return Parser(curried, 'or_else')\n",
    "\n",
    "def char(c):\n",
    "    def curried(s):\n",
    "        if not s:\n",
    "            msg = \"S is empty\"\n",
    "            return Left(msg)\n",
    "        else:\n",
    "            if s[0] == c:\n",
    "                return Right((c, s[1:]) )\n",
    "            else:\n",
    "                return Left(\"Expecting '%s' and found '%s'\"%(c, s[0]))\n",
    "    return Parser(curried, 'char')\n",
    "\n",
    "foldl = reduce\n",
    "def choice(parsers):\n",
    "    return foldl(or_else, parsers)\n",
    "\n",
    "def any_of(chars):\n",
    "    return choice(list(map(char, chars)))\n",
    "\n",
    "def parse_string(s):\n",
    "    return foldl(and_then, list(map(char, list(s)))).map(lambda l: \"\".join(l))\n",
    "\n",
    "def until_seq(seq):\n",
    "    def curried(s):\n",
    "        if not s:\n",
    "            msg = \"S is empty\"\n",
    "            return Left(msg)\n",
    "        else:\n",
    "            if seq == s[:len(seq)]:\n",
    "                return Right((\"\", s))\n",
    "            else:\n",
    "                return Left(\"Expecting '%s' and found '%s'\"%(seq, s[:len(seq)]))\n",
    "    return Parser(curried, 'until_seq')\n",
    "\n",
    "def until(p):\n",
    "    def curried(s):\n",
    "        res = p(s)\n",
    "        if isinstance(res, Left):\n",
    "            return res\n",
    "        else:\n",
    "            return Right((\"\", s))\n",
    "    return Parser(curried, 'until')\n",
    "\n",
    "chars = parse_string\n",
    "\n",
    "def parse_zero_or_more(parser, inp): #zero or more\n",
    "    res = parser(inp)\n",
    "    if isinstance(res, Left):\n",
    "        return \"\", inp\n",
    "    else:\n",
    "        firstval, restinpafterfirst = res.val\n",
    "        subseqvals, remaining = parse_zero_or_more(parser, restinpafterfirst)\n",
    "        values = firstval\n",
    "        if subseqvals:\n",
    "            if isinstance(firstval, str):\n",
    "                values = firstval+subseqvals\n",
    "            elif isinstance(firstval, list):\n",
    "                values = firstval+ ([subseqvals] if isinstance(subseqvals, str) else subseqvals)\n",
    "        return values, remaining\n",
    "\n",
    "def many(parser):\n",
    "    def curried(s):\n",
    "        return Right(parse_zero_or_more(parser,s))\n",
    "    return Parser(curried, 'many')\n",
    "\n",
    "\n",
    "def many1(parser):\n",
    "    def curried(s):\n",
    "        res = run_parser(parser, s)\n",
    "        if isinstance(res, Left):\n",
    "            return res\n",
    "        else:\n",
    "            return run_parser(many(parser), s)\n",
    "    return Parser(curried, 'many1')\n",
    "\n",
    "\n",
    "def optionally(parser):\n",
    "    noneparser = Parser(lambda x: Right( (Nothing(), \"\")))\n",
    "    return or_else(parser, noneparser)\n",
    "\n",
    "def sep_by1(sep, parser):\n",
    "    sep_then_parser = sep >> parser\n",
    "    return parser >> many(sep_then_parser)\n",
    "\n",
    "def sep_by(sep, parser):\n",
    "    return (sep_by1(sep, parser) | Parser(lambda x: Right( ([], \"\")), 'sep_by'))\n",
    "\n",
    "def forward(parsergeneratorfn):\n",
    "    def curried(s):\n",
    "        return parsergeneratorfn()(s)\n",
    "    return curried\n",
    "\n",
    "letter = any_of(string.ascii_letters)\n",
    "letter.tag = 'letter'\n",
    "lletter = any_of(string.ascii_lowercase)\n",
    "lletter.tag = 'lletter'\n",
    "uletter = any_of(string.ascii_uppercase)\n",
    "uletter.tag = 'uletter'\n",
    "digit = any_of(string.digits)\n",
    "digit.tag = 'digit'\n",
    "digits = many1(digit)\n",
    "digits.tag = 'digits'\n",
    "whitespace = any_of(string.whitespace)\n",
    "whitespace.tag = 'whitespace'\n",
    "ws = whitespace.suppress()\n",
    "ws.tag = 'ws'\n",
    "letters = many1(letter)\n",
    "letters.tag = 'letters'\n",
    "word = letters\n",
    "word.tag = 'word'\n",
    "alphanumword = many(letter >> (letters|digits))\n",
    "alphanumword.tag = 'alphanumword'\n",
    "num_as_int = digits.map(lambda l: int(\"\".join(l)))\n",
    "num_as_int.tag = 'num_as_int'\n",
    "between = lambda p1, p2 , p3 : p1 >> p2 >> p3\n",
    "surrounded_by = lambda surparser, contentparser: surparser >> contentparser >> surparser\n",
    "quotedword = surrounded_by( (char('\"')|char(\"'\")).suppress() , word)\n",
    "quotedword.tag = 'quotedword'\n",
    "option = optionally\n",
    "option.tag = 'optionally'\n",
    "\n",
    "# commasepareted_p = sep_by(char(\",\").suppress(), many1(word) | many1(digit) | many1(quotedword))\n",
    "commaseparated_of = lambda p: sep_by(char(\",\").suppress(), many(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('build/myparsec.py', 'w+') as f:\n",
    "    src = rewrite(VARS['myparsec_src'], original='myparsec.py')\n",
    "    print(src, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myparsec.py', 'w+') as f:\n",
    "    src = VARS['myparsec_src']\n",
    "    print(src, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject - assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var parsec_src\n",
    "import string\n",
    "import json\n",
    "import sys\n",
    "import myparsec as pyparsec\n",
    "\n",
    "alphap = pyparsec.char('a')\n",
    "alphap.tag = 'alphap'\n",
    "eqp = pyparsec.char('=')\n",
    "eqp.tag = 'eqp'\n",
    "digitp = pyparsec.digits\n",
    "digitp.tag = 'digitp'\n",
    "abcparser = pyparsec.word >> eqp >> digitp\n",
    "abcparser.tag = 'abcparser'\n",
    "\n",
    "def main(arg):\n",
    "    v = abcparser.parse(arg)\n",
    "    if isinstance(v, pyparsec.Left):\n",
    "        raise Exception('parse failed')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsec_samples =  [\n",
    "    'a=0',\n",
    "    'ab=0',\n",
    "    'b=2',\n",
    "    'bbc=4491123',\n",
    "    'zuui=2',\n",
    "    'c=9',\n",
    "    'd=012',\n",
    "    'e=79',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering the parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accio_tree(fname, src, samples, restrict=True):\n",
    "    program_src[fname] = src\n",
    "    with open('subjects/%s' % fname, 'w+') as f:\n",
    "        print(src, file=f)\n",
    "    resrc = rewrite(src, fname)\n",
    "    if restrict:\n",
    "        resrc = resrc.replace('restrict = {\\'files\\': [sys.argv[0]]}', 'restrict = {}')\n",
    "    with open('build/%s' % fname, 'w+') as f:\n",
    "        print(resrc, file=f)\n",
    "    os.makedirs('samples/%s' % fname, exist_ok=True)\n",
    "    sample_files = {(\"samples/%s/%d.csv\"%(fname,i)):s for i,s in enumerate(samples)}\n",
    "    for k in sample_files:\n",
    "        with open(k, 'w+') as f:\n",
    "            print(sample_files[k], file=f)\n",
    "\n",
    "    call_trace = []\n",
    "    for i in sample_files:\n",
    "        my_tree = do([\"python3\", \"./build/%s\" % fname, i]).stdout\n",
    "        call_trace.append(json.loads(my_tree)[0])\n",
    "    mined_tree = miner(call_trace)\n",
    "    reset_generalizer()\n",
    "    generalized_m_tree = generalize_method_trees(mined_tree)\n",
    "    reset_generalizer()\n",
    "    generalized_tree = generalize_loop_trees(generalized_m_tree)\n",
    "    reset_generalizer()\n",
    "    return generalized_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsec_trees = accio_tree('parsec.py', VARS['parsec_src'], parsec_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zoom(display_tree(parsec_trees[0]['tree'], extract_node=extract_node_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsec_g = compact_grammar(cleanup_grammar(convert_to_grammar(parsec_trees), start_symbol='<START>'), start_symbol='<START>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_grammar_cf(parsec_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = LimitFuzzer(parsec_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    v = gf.fuzz(key='<START>')\n",
    "    print(repr(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Lisp Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var sexpr_src\n",
    "import myparsec as pyparsec\n",
    "import sys\n",
    "import json\n",
    "\n",
    "alphaP = pyparsec.letters\n",
    "alphaP.tag = 'alphaP'\n",
    "\n",
    "digitP = pyparsec.digits\n",
    "digitP.tag = 'digitP'\n",
    "\n",
    "quoteP = pyparsec.char('\"')\n",
    "quoteP.tag = 'quoteP'\n",
    "stringP = quoteP >> pyparsec.many(alphaP | digitP | pyparsec.whitespace) >> quoteP\n",
    "stringP.tag = 'stringP'\n",
    "\n",
    "idP = pyparsec.letter >> pyparsec.many(alphaP|digitP)\n",
    "idP.tag = 'idP'\n",
    "\n",
    "atomP = idP | digitP | stringP\n",
    "atomP.tag = 'atomP'\n",
    "\n",
    "openP = pyparsec.char('(')\n",
    "closeP = pyparsec.char(')')\n",
    "listP = pyparsec.forward(lambda:  openP >> pyparsec.sep_by(pyparsec.whitespace, pyparsec.many(sexprP)) >> closeP)\n",
    "listP.tag = 'listP'\n",
    "sexprP = (atomP |  listP)\n",
    "sexprP.tag = 'sexprP'\n",
    "\n",
    "\n",
    "def main(arg):\n",
    "    v = sexprP.parse(arg)\n",
    "    if isinstance(v, pyparsec.Left):\n",
    "        raise Exception('parse failed')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Lisp golden grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexpr_golden = {\n",
    "    '<START>': [ '<sexpr>' ],\n",
    "    '<sexpr>': [ '<atom>', '<list>' ],\n",
    "    '<atom>': ['<id>', '<digits>', '<string>' ],\n",
    "    '<list>': [ '(<whitespace><sexprs><whitespace>)'],\n",
    "    '<sexprs>': [ '', '<sexpr><space><whitespace><sexprs>'],\n",
    "    '<id>': ['<letter><alphanums>'],\n",
    "    '<alphanums>': [ '', '<alphanum><alphanums>'],\n",
    "    '<alphanum>' : ['<letter>', '<digit>'],\n",
    "    '<digit>': [i for i in string.digits],\n",
    "    '<space>': [' '],\n",
    "    '<whitespace>' : ['', '<space><whitespace>'],\n",
    "    '<letter>' : [i for i in string.ascii_letters],\n",
    "    '<digits>': ['<digit>', '<digit>''<digits>'],\n",
    "    '<sletter>': ['<space>', '<digit>', '<letter>'],\n",
    "    '<sletters>': ['', '<sletter><sletters>'],\n",
    "    '<string>': ['\"<sletters>\"']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = LimitFuzzer(canonical(sexpr_golden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexpr_samples = list(set([f.fuzz(key='<START>') for i in range(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeit() as t:\n",
    "    sexpr_grammar = accio_grammar('sexpr.py', VARS['sexpr_src'], sexpr_samples)\n",
    "Mimid_t['sexpr.py'] = t.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grammar(sexpr_grammar, 'mimid', 'sexpr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if 'sexpr' in CHECK:\n",
    "result = check_precision('sexpr.py', sexpr_grammar)\n",
    "Mimid_p['sexpr.py'] = result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subjects.sexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if 'sexpr' in CHECK:\n",
    "result = check_recall(sexpr_golden, sexpr_grammar, subjects.sexpr.main)\n",
    "Mimid_r['sexpr.py'] = result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing with PEG Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var peg_src\n",
    "import re\n",
    "RE_NONTERMINAL = re.compile(r'(<[^<> ]*>)')\n",
    "\n",
    "def canonical(grammar, letters=False):\n",
    "    def split(expansion):\n",
    "        if isinstance(expansion, tuple): expansion = expansion[0]\n",
    "        return [token for token in re.split(RE_NONTERMINAL, expansion) if token]\n",
    "    def tokenize(word): return list(word) if letters else [word]\n",
    "    def canonical_expr(expression):\n",
    "        return [token for word in split(expression)\n",
    "            for token in ([word] if word in grammar else tokenize(word))]\n",
    "    return {k: [canonical_expr(expression) for expression in alternatives]\n",
    "        for k, alternatives in grammar.items()}\n",
    "\n",
    "def crange(character_start, character_end):\n",
    "    return [chr(i) for i in range(ord(character_start), ord(character_end) + 1)]\n",
    "\n",
    "def unify_key(grammar, key, text, at=0):\n",
    "    if key not in grammar:\n",
    "        if text[at:].startswith(key):\n",
    "            return at + len(key), (key, [])\n",
    "        else:\n",
    "            return at, None\n",
    "    for rule in grammar[key]:\n",
    "        to, res = unify_rule(grammar, rule, text, at)\n",
    "        if res:\n",
    "            return (to, (key, res))\n",
    "    return 0, None\n",
    "\n",
    "def unify_rule(grammar, rule, text, at):\n",
    "    results = []\n",
    "    for token in rule:\n",
    "        at, res = unify_key(grammar, token, text, at)\n",
    "        if res is None:\n",
    "            return at, None\n",
    "        results.append(res)\n",
    "    return at, results\n",
    "\n",
    "import string\n",
    "VAR_GRAMMAR = {\n",
    "    '<start>': ['<assignment>'],\n",
    "    '<assignment>': ['<identifier>=<expr>'],\n",
    "    '<identifier>': ['<word>'],\n",
    "    '<word>': ['<alpha><word>', '<alpha>'],\n",
    "    '<alpha>': list(string.ascii_letters),\n",
    "    '<expr>': ['<term>+<expr>', '<term>-<expr>', '<term>'],\n",
    "    '<term>': ['<factor>*<term>', '<factor>/<term>', '<factor>'],\n",
    "    '<factor>': ['+<factor>', '-<factor>', '(<expr>)', '<identifier>', '<number>'],\n",
    "    '<number>': ['<integer>.<integer>', '<integer>'],\n",
    "    '<integer>': ['<digit><integer>', '<digit>'],\n",
    "    '<digit>': crange('0', '9')\n",
    "}\n",
    "def main(arg):\n",
    "    C_VG = canonical(VAR_GRAMMAR)\n",
    "    at, result = unify_key(C_VG, '<start>', arg)\n",
    "    if result is None:\n",
    "        raise Exception('Not parsed')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%var config_src\n",
    "# [(\n",
    "import urllib.parse\n",
    "# This is useful for parsers such as PEG where the argument\n",
    "# is important as the name of the non-terminal.\n",
    "ENCODE_ARGS = True\n",
    "def encode_method_name(name, my_args):\n",
    "    if not ENCODE_ARGS: return name\n",
    "    if not my_args: return name\n",
    "    if len(my_args) == 1: return name\n",
    "    #return \"%s_%s_\" % (name, urllib.parse.quote('_'.join([i if isinstance(i, str) else str(i) for i in my_args])))\n",
    "    return \"%s_%s_\" % (name, urllib.parse.quote(str(my_args[1])))\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('build/config.py', 'w+') as f:\n",
    "    print(VARS['config_src'], file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEG samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg_samples = [\n",
    "    'a=0',\n",
    "    'ab=0',\n",
    "    'bbc=4491123',\n",
    "    'b=2',\n",
    "    'zuui=2',\n",
    "    'c=9',\n",
    "    'd=012',\n",
    "    'e=79',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg_trees = accio_tree('peg.py', VARS['peg_src'], peg_samples, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node_unquote(node, id):\n",
    "    symbol, children, *annotation = node\n",
    "    if (symbol[0], symbol[-1]) == ('<', '>'):\n",
    "        symbol = \"<%s>\" % urllib.parse.unquote(symbol[1:-2])\n",
    "    return symbol, children, ''.join(str(a) for a in annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(peg_trees[0]['tree'], extract_node=extract_node_unquote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_peg(g):\n",
    "    def rep(r):\n",
    "         return r.replace('<unify_key_%3C', '<').replace('%3E_',':').replace('<unify_rule_%3C', '<')\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        rs = []\n",
    "        for r in g[k]:\n",
    "            r_ = []\n",
    "            for t in r:\n",
    "                t_ = rep(t)\n",
    "                r_.append(t_)\n",
    "            rs.append(r_)\n",
    "        g_[rep(k)] = rs\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg_g1 = clean_peg(compact_grammar(cleanup_grammar(convert_to_grammar(peg_trees), start_symbol='<START>'), start_symbol='<START>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_grammar_cf(peg_g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = LimitFuzzer(peg_g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    v = gf.fuzz(key='<START>')\n",
    "    print(repr(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitation due to context sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_msamples=[i.strip() for i in '''\n",
    "x\n",
    "exp(0)\n",
    "e\n",
    "'''.strip().split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(fail_msamples):\n",
    "    print(j, repr(i), do(['python3', 'build/check.py','subjects/mathexpr.py', i]).returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_msample_files = {(\"samples/%s/%d_fail.csv\"%('mathexpr.py',i)):s for i,s in enumerate(fail_msamples)}\n",
    "for k in fail_msample_files:\n",
    "    with open(k, 'w+') as f:\n",
    "        print(fail_msample_files[k], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_mcall_trace = []\n",
    "for i in fail_msample_files:\n",
    "    my_tree = do([\"python3\", \"./build/%s\" % 'mathexpr.py', i]).stdout\n",
    "    fail_mcall_trace.append(json.loads(my_tree)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_mined_tree = miner(fail_mcall_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_generalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_gmtree = generalize_method_trees(fail_mined_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_generalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_gltree = generalize_loop_trees(fail_gmtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzingbook.Parser import highlight_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail_hl(dot, nid, symbol, ann):\n",
    "    return symbol == '<parseVariable.0:while_1,0 =#[\"loop_1\"]>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_fail_node = highlight_node(fail_hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(fail_gltree[0]['tree'], node_attr=highlight_fail_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(fail_gltree[1]['tree'], node_attr=highlight_fail_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(fail_gltree[2]['tree'], node_attr=highlight_fail_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_generalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_g = convert_to_grammar(fail_gltree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(fail_g.keys()):\n",
    "    print(k)\n",
    "    for r in sorted(fail_g[k]):\n",
    "        print('\\t',repr(''.join(r)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = LimitFuzzer(fail_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(repr(gf.fuzz(key='<START>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit due to re-parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_usamples=[i.strip() for i in '''\n",
    "https://host/folder\n",
    "https://host/folder?s=q\n",
    "https://host/folder#f\n",
    "'''.strip().split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(fail_usamples):\n",
    "    print(j, repr(i), do(['python3', 'build/check.py','subjects/urlparse.py', i]).returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_usample_files = {(\"samples/%s/%d_fail.csv\"%('urlparse.py',i)):s for i,s in enumerate(fail_usamples)}\n",
    "for k in fail_usample_files:\n",
    "    with open(k, 'w+') as f:\n",
    "        print(fail_usample_files[k], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_ucall_trace = []\n",
    "for i in fail_usample_files:\n",
    "    my_tree = do([\"python3\", \"./build/%s\" % 'urlparse.py', i]).stdout\n",
    "    fail_ucall_trace.append(json.loads(my_tree)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_mined_tree = miner(fail_ucall_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(fail_mined_tree[0]['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(fail_mined_tree[1]['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(fail_mined_tree[2]['tree'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "394.15px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
